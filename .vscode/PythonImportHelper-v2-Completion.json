[
    {
        "label": "ALIKED",
        "importPath": "lightglue",
        "description": "lightglue",
        "isExtraImport": true,
        "detail": "lightglue",
        "documentation": {}
    },
    {
        "label": "LightGlue",
        "importPath": "lightglue",
        "description": "lightglue",
        "isExtraImport": true,
        "detail": "lightglue",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "D2Net",
        "importPath": "lib.model_test",
        "description": "lib.model_test",
        "isExtraImport": true,
        "detail": "lib.model_test",
        "documentation": {}
    },
    {
        "label": "D2Net",
        "importPath": "lib.model_test",
        "description": "lib.model_test",
        "isExtraImport": true,
        "detail": "lib.model_test",
        "documentation": {}
    },
    {
        "label": "process_multiscale",
        "importPath": "lib.pyramid",
        "description": "lib.pyramid",
        "isExtraImport": true,
        "detail": "lib.pyramid",
        "documentation": {}
    },
    {
        "label": "process_multiscale",
        "importPath": "lib.pyramid",
        "description": "lib.pyramid",
        "isExtraImport": true,
        "detail": "lib.pyramid",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "ZipFile",
        "importPath": "zipfile",
        "description": "zipfile",
        "isExtraImport": true,
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "gdown",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gdown",
        "description": "gdown",
        "detail": "gdown",
        "documentation": {}
    },
    {
        "label": "sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn",
        "description": "sklearn",
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "dirtorch.extract_features",
        "description": "dirtorch.extract_features",
        "isExtraImport": true,
        "detail": "dirtorch.extract_features",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "dirtorch.extract_features",
        "description": "dirtorch.extract_features",
        "isExtraImport": true,
        "detail": "dirtorch.extract_features",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "dirtorch.utils",
        "description": "dirtorch.utils",
        "isExtraImport": true,
        "detail": "dirtorch.utils",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "dirtorch.utils",
        "description": "dirtorch.utils",
        "isExtraImport": true,
        "detail": "dirtorch.utils",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "dirtorch.utils",
        "description": "dirtorch.utils",
        "isExtraImport": true,
        "detail": "dirtorch.utils",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "dirtorch.utils",
        "description": "dirtorch.utils",
        "isExtraImport": true,
        "detail": "dirtorch.utils",
        "documentation": {}
    },
    {
        "label": "kornia",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "kornia",
        "description": "kornia",
        "detail": "kornia",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pycolmap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycolmap",
        "description": "pycolmap",
        "detail": "pycolmap",
        "documentation": {}
    },
    {
        "label": "extract_patches_from_pyramid",
        "importPath": "kornia.feature.laf",
        "description": "kornia.feature.laf",
        "isExtraImport": true,
        "detail": "kornia.feature.laf",
        "documentation": {}
    },
    {
        "label": "laf_from_center_scale_ori",
        "importPath": "kornia.feature.laf",
        "description": "kornia.feature.laf",
        "isExtraImport": true,
        "detail": "kornia.feature.laf",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "torchvision.models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "scipy.io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.io",
        "description": "scipy.io",
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "loadmat",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "loadmat",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "NonMaxSuppression",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "extract_multiscale",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "load_network",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "load_network",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "NonMaxSuppression",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "extract_multiscale",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "NonMaxSuppression",
        "importPath": "extract",
        "description": "extract",
        "isExtraImport": true,
        "detail": "extract",
        "documentation": {}
    },
    {
        "label": "superpoint",
        "importPath": "SuperGluePretrainedNetwork.models",
        "description": "SuperGluePretrainedNetwork.models",
        "isExtraImport": true,
        "detail": "SuperGluePretrainedNetwork.models",
        "documentation": {}
    },
    {
        "label": "AdalamFilter",
        "importPath": "kornia.feature.adalam",
        "description": "kornia.feature.adalam",
        "isExtraImport": true,
        "detail": "kornia.feature.adalam",
        "documentation": {}
    },
    {
        "label": "get_cuda_device_if_available",
        "importPath": "kornia.utils.helpers",
        "description": "kornia.utils.helpers",
        "isExtraImport": true,
        "detail": "kornia.utils.helpers",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "LoFTR",
        "importPath": "kornia.feature",
        "description": "kornia.feature",
        "isExtraImport": true,
        "detail": "kornia.feature",
        "documentation": {}
    },
    {
        "label": "default_cfg",
        "importPath": "kornia.feature.loftr.loftr",
        "description": "kornia.feature.loftr.loftr",
        "isExtraImport": true,
        "detail": "kornia.feature.loftr.loftr",
        "documentation": {}
    },
    {
        "label": "SuperGlue",
        "importPath": "SuperGluePretrainedNetwork.models.superglue",
        "description": "SuperGluePretrainedNetwork.models.superglue",
        "isExtraImport": true,
        "detail": "SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "PIL.Image",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.Image",
        "description": "PIL.Image",
        "detail": "PIL.Image",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "read_model",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_model",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "qvec2rotmat",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_cameras_binary",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_cameras_text",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_images_binary",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_images_text",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_model",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_model",
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "isExtraImport": true,
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pformat",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pformat",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "pformat",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "h5py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "h5py",
        "description": "h5py",
        "detail": "h5py",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "cm",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.patheffects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.patheffects",
        "description": "matplotlib.patheffects",
        "detail": "matplotlib.patheffects",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "plotly.graph_objects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.graph_objects",
        "description": "plotly.graph_objects",
        "detail": "plotly.graph_objects",
        "documentation": {}
    },
    {
        "label": "collections.abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections.abc",
        "description": "collections.abc",
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "SimpleNamespace",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "SimpleNamespace",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "scipy.spatial",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "KDTree",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "match_features",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "reconstruction",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "pairs_from_retrieval",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "match_features",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "reconstruction",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "pairs_from_retrieval",
        "importPath": "hloc",
        "description": "hloc",
        "isExtraImport": true,
        "detail": "hloc",
        "documentation": {}
    },
    {
        "label": "QueryLocalizer",
        "importPath": "hloc.localize_sfm",
        "description": "hloc.localize_sfm",
        "isExtraImport": true,
        "detail": "hloc.localize_sfm",
        "documentation": {}
    },
    {
        "label": "pose_from_cluster",
        "importPath": "hloc.localize_sfm",
        "description": "hloc.localize_sfm",
        "isExtraImport": true,
        "detail": "hloc.localize_sfm",
        "documentation": {}
    },
    {
        "label": "matplotlib.cm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.cm",
        "description": "matplotlib.cm",
        "detail": "matplotlib.cm",
        "documentation": {}
    },
    {
        "label": "Matching",
        "importPath": "models.matching",
        "description": "models.matching",
        "isExtraImport": true,
        "detail": "models.matching",
        "documentation": {}
    },
    {
        "label": "Matching",
        "importPath": "models.matching",
        "description": "models.matching",
        "isExtraImport": true,
        "detail": "models.matching",
        "documentation": {}
    },
    {
        "label": "AverageTimer",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "VideoStreamer",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "make_matching_plot_fast",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "frame2tensor",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "compute_pose_error",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "compute_epipolar_error",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "estimate_pose",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "make_matching_plot",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "error_colormap",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "AverageTimer",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "pose_auc",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "read_image",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "rotate_intrinsics",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "rotate_pose_inplane",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "scale_intrinsics",
        "importPath": "models.utils",
        "description": "models.utils",
        "isExtraImport": true,
        "detail": "models.utils",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageEnhance",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageEnhance",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "grid_positions",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "upscale_positions",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "downscale_positions",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "savefig",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "imshow_image",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "interpolate_dense_features",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "upscale_positions",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "importPath": "lib.utils",
        "description": "lib.utils",
        "isExtraImport": true,
        "detail": "lib.utils",
        "documentation": {}
    },
    {
        "label": "NoGradientError",
        "importPath": "lib.exceptions",
        "description": "lib.exceptions",
        "isExtraImport": true,
        "detail": "lib.exceptions",
        "documentation": {}
    },
    {
        "label": "EmptyTensorError",
        "importPath": "lib.exceptions",
        "description": "lib.exceptions",
        "isExtraImport": true,
        "detail": "lib.exceptions",
        "documentation": {}
    },
    {
        "label": "EmptyTensorError",
        "importPath": "lib.exceptions",
        "description": "lib.exceptions",
        "isExtraImport": true,
        "detail": "lib.exceptions",
        "documentation": {}
    },
    {
        "label": "EmptyTensorError",
        "importPath": "lib.exceptions",
        "description": "lib.exceptions",
        "isExtraImport": true,
        "detail": "lib.exceptions",
        "documentation": {}
    },
    {
        "label": "NoGradientError",
        "importPath": "lib.exceptions",
        "description": "lib.exceptions",
        "isExtraImport": true,
        "detail": "lib.exceptions",
        "documentation": {}
    },
    {
        "label": "imagesize",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imagesize",
        "description": "imagesize",
        "detail": "imagesize",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "scipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy",
        "description": "scipy",
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "scipy.misc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.misc",
        "description": "scipy.misc",
        "detail": "scipy.misc",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "MegaDepthDataset",
        "importPath": "lib.dataset",
        "description": "lib.dataset",
        "isExtraImport": true,
        "detail": "lib.dataset",
        "documentation": {}
    },
    {
        "label": "loss_function",
        "importPath": "lib.loss",
        "description": "lib.loss",
        "isExtraImport": true,
        "detail": "lib.loss",
        "documentation": {}
    },
    {
        "label": "D2Net",
        "importPath": "lib.model",
        "description": "lib.model",
        "isExtraImport": true,
        "detail": "lib.model",
        "documentation": {}
    },
    {
        "label": "pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdb",
        "description": "pdb",
        "detail": "pdb",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "ceil",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "ceil",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn.modules",
        "description": "torch.nn.modules",
        "isExtraImport": true,
        "detail": "torch.nn.modules",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn.parameter",
        "description": "torch.nn.parameter",
        "isExtraImport": true,
        "detail": "torch.nn.parameter",
        "documentation": {}
    },
    {
        "label": "sklearn.decomposition",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "multiprocessing.dummy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing.dummy",
        "description": "multiprocessing.dummy",
        "detail": "multiprocessing.dummy",
        "documentation": {}
    },
    {
        "label": "mkdir",
        "importPath": "dirtorch.utils.convenient",
        "description": "dirtorch.utils.convenient",
        "isExtraImport": true,
        "detail": "dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "mkdir",
        "importPath": "dirtorch.utils.convenient",
        "description": "dirtorch.utils.convenient",
        "isExtraImport": true,
        "detail": "dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "tonumpy",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "matmul",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "pool",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "tonumpy",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "pool",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "tonumpy",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "matmul",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "pool",
        "importPath": "dirtorch.utils.common",
        "description": "dirtorch.utils.common",
        "isExtraImport": true,
        "detail": "dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "get_loader",
        "importPath": "dirtorch.utils.pytorch_loader",
        "description": "dirtorch.utils.pytorch_loader",
        "isExtraImport": true,
        "detail": "dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "get_loader",
        "importPath": "dirtorch.utils.pytorch_loader",
        "description": "dirtorch.utils.pytorch_loader",
        "isExtraImport": true,
        "detail": "dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "dirtorch.test_dir",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dirtorch.test_dir",
        "description": "dirtorch.test_dir",
        "detail": "dirtorch.test_dir",
        "documentation": {}
    },
    {
        "label": "extract_image_features",
        "importPath": "dirtorch.test_dir",
        "description": "dirtorch.test_dir",
        "isExtraImport": true,
        "detail": "dirtorch.test_dir",
        "documentation": {}
    },
    {
        "label": "dirtorch.nets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dirtorch.nets",
        "description": "dirtorch.nets",
        "detail": "dirtorch.nets",
        "documentation": {}
    },
    {
        "label": "dirtorch.datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dirtorch.datasets",
        "description": "dirtorch.datasets",
        "detail": "dirtorch.datasets",
        "documentation": {}
    },
    {
        "label": "dirtorch.datasets.downloader",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dirtorch.datasets.downloader",
        "description": "dirtorch.datasets.downloader",
        "detail": "dirtorch.datasets.downloader",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "ImageList",
        "importPath": "dirtorch.datasets.generic",
        "description": "dirtorch.datasets.generic",
        "isExtraImport": true,
        "detail": "dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "kapture",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "kapture",
        "description": "kapture",
        "detail": "kapture",
        "documentation": {}
    },
    {
        "label": "kapture_from_dir",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "get_csv_fullpath",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "global_features_to_file",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "kapture_from_dir",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "get_csv_fullpath",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "keypoints_to_file",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "descriptors_to_file",
        "importPath": "kapture.io.csv",
        "description": "kapture.io.csv",
        "isExtraImport": true,
        "detail": "kapture.io.csv",
        "documentation": {}
    },
    {
        "label": "get_image_fullpath",
        "importPath": "kapture.io.records",
        "description": "kapture.io.records",
        "isExtraImport": true,
        "detail": "kapture.io.records",
        "documentation": {}
    },
    {
        "label": "get_image_fullpath",
        "importPath": "kapture.io.records",
        "description": "kapture.io.records",
        "isExtraImport": true,
        "detail": "kapture.io.records",
        "documentation": {}
    },
    {
        "label": "get_global_features_fullpath",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "image_global_features_to_file",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "global_features_check_dir",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "get_keypoints_fullpath",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "keypoints_check_dir",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "image_keypoints_to_file",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "get_descriptors_fullpath",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "descriptors_check_dir",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "image_descriptors_to_file",
        "importPath": "kapture.io.features",
        "description": "kapture.io.features",
        "isExtraImport": true,
        "detail": "kapture.io.features",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "instanciate_transformation",
        "importPath": "tools.transforms",
        "description": "tools.transforms",
        "isExtraImport": true,
        "detail": "tools.transforms",
        "documentation": {}
    },
    {
        "label": "instanciate_transformation",
        "importPath": "tools.transforms",
        "description": "tools.transforms",
        "isExtraImport": true,
        "detail": "tools.transforms",
        "documentation": {}
    },
    {
        "label": "persp_apply",
        "importPath": "tools.transforms_tools",
        "description": "tools.transforms_tools",
        "isExtraImport": true,
        "detail": "tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "persp_apply",
        "importPath": "tools.transforms_tools",
        "description": "tools.transforms_tools",
        "isExtraImport": true,
        "detail": "tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.sampler",
        "description": "nets.sampler",
        "isExtraImport": true,
        "detail": "nets.sampler",
        "documentation": {}
    },
    {
        "label": "FullSampler",
        "importPath": "nets.sampler",
        "description": "nets.sampler",
        "isExtraImport": true,
        "detail": "nets.sampler",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.repeatability_loss",
        "description": "nets.repeatability_loss",
        "isExtraImport": true,
        "detail": "nets.repeatability_loss",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.reliability_loss",
        "description": "nets.reliability_loss",
        "isExtraImport": true,
        "detail": "nets.reliability_loss",
        "documentation": {}
    },
    {
        "label": "APLoss",
        "importPath": "nets.ap_loss",
        "description": "nets.ap_loss",
        "isExtraImport": true,
        "detail": "nets.ap_loss",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "tools",
        "description": "tools",
        "isExtraImport": true,
        "detail": "tools",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "tools",
        "description": "tools",
        "isExtraImport": true,
        "detail": "tools",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "tools",
        "description": "tools",
        "isExtraImport": true,
        "detail": "tools",
        "documentation": {}
    },
    {
        "label": "trainer",
        "importPath": "tools",
        "description": "tools",
        "isExtraImport": true,
        "detail": "tools",
        "documentation": {}
    },
    {
        "label": "common",
        "importPath": "tools",
        "description": "tools",
        "isExtraImport": true,
        "detail": "tools",
        "documentation": {}
    },
    {
        "label": "norm_RGB",
        "importPath": "tools.dataloader",
        "description": "tools.dataloader",
        "isExtraImport": true,
        "detail": "tools.dataloader",
        "documentation": {}
    },
    {
        "label": "norm_RGB",
        "importPath": "tools.dataloader",
        "description": "tools.dataloader",
        "isExtraImport": true,
        "detail": "tools.dataloader",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "tools.dataloader",
        "description": "tools.dataloader",
        "isExtraImport": true,
        "detail": "tools.dataloader",
        "documentation": {}
    },
    {
        "label": "norm_RGB",
        "importPath": "tools.dataloader",
        "description": "tools.dataloader",
        "isExtraImport": true,
        "detail": "tools.dataloader",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.patchnet",
        "description": "nets.patchnet",
        "isExtraImport": true,
        "detail": "nets.patchnet",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.patchnet",
        "description": "nets.patchnet",
        "isExtraImport": true,
        "detail": "nets.patchnet",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.patchnet",
        "description": "nets.patchnet",
        "isExtraImport": true,
        "detail": "nets.patchnet",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.patchnet",
        "description": "nets.patchnet",
        "isExtraImport": true,
        "detail": "nets.patchnet",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nets.losses",
        "description": "nets.losses",
        "isExtraImport": true,
        "detail": "nets.losses",
        "documentation": {}
    },
    {
        "label": "uniform_filter",
        "importPath": "scipy.ndimage",
        "description": "scipy.ndimage",
        "isExtraImport": true,
        "detail": "scipy.ndimage",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "ALIKED",
        "kind": 6,
        "importPath": "hloc.extractors.aliked",
        "description": "hloc.extractors.aliked",
        "peekOfCode": "class ALIKED(BaseModel):\n    default_conf = {\n        \"model_name\": \"aliked-n16\",\n        \"max_num_keypoints\": -1,\n        \"detection_threshold\": 0.2,\n        \"nms_radius\": 2,\n    }\n    required_inputs = [\"image\"]\n    def _init(self, conf):\n        conf.pop(\"name\")",
        "detail": "hloc.extractors.aliked",
        "documentation": {}
    },
    {
        "label": "D2Net",
        "kind": 6,
        "importPath": "hloc.extractors.d2net",
        "description": "hloc.extractors.d2net",
        "peekOfCode": "class D2Net(BaseModel):\n    default_conf = {\n        \"model_name\": \"d2_tf.pth\",\n        \"checkpoint_dir\": d2net_path / \"models\",\n        \"use_relu\": True,\n        \"multiscale\": False,\n    }\n    required_inputs = [\"image\"]\n    def _init(self, conf):\n        model_file = conf[\"checkpoint_dir\"] / conf[\"model_name\"]",
        "detail": "hloc.extractors.d2net",
        "documentation": {}
    },
    {
        "label": "d2net_path",
        "kind": 5,
        "importPath": "hloc.extractors.d2net",
        "description": "hloc.extractors.d2net",
        "peekOfCode": "d2net_path = Path(__file__).parent / \"../../third_party/d2net\"\nsys.path.append(str(d2net_path))\nfrom lib.model_test import D2Net as _D2Net  # noqa: E402\nfrom lib.pyramid import process_multiscale  # noqa: E402\nclass D2Net(BaseModel):\n    default_conf = {\n        \"model_name\": \"d2_tf.pth\",\n        \"checkpoint_dir\": d2net_path / \"models\",\n        \"use_relu\": True,\n        \"multiscale\": False,",
        "detail": "hloc.extractors.d2net",
        "documentation": {}
    },
    {
        "label": "DIR",
        "kind": 6,
        "importPath": "hloc.extractors.dir",
        "description": "hloc.extractors.dir",
        "peekOfCode": "class DIR(BaseModel):\n    default_conf = {\n        \"model_name\": \"Resnet-101-AP-GeM\",\n        \"whiten_name\": \"Landmarks_clean\",\n        \"whiten_params\": {\n            \"whitenp\": 0.25,\n            \"whitenv\": None,\n            \"whitenm\": 1.0,\n        },\n        \"pooling\": \"gem\",",
        "detail": "hloc.extractors.dir",
        "documentation": {}
    },
    {
        "label": "os.environ[\"DB_ROOT\"]",
        "kind": 5,
        "importPath": "hloc.extractors.dir",
        "description": "hloc.extractors.dir",
        "peekOfCode": "os.environ[\"DB_ROOT\"] = \"\"  # required by dirtorch\nfrom dirtorch.extract_features import load_model  # noqa: E402\nfrom dirtorch.utils import common  # noqa: E402\n# The DIR model checkpoints (pickle files) include sklearn.decomposition.pca,\n# which has been deprecated in sklearn v0.24\n# and must be explicitly imported with `from sklearn.decomposition import PCA`.\n# This is a hacky workaround to maintain forward compatibility.\nsys.modules[\"sklearn.decomposition.pca\"] = sklearn.decomposition._pca\nclass DIR(BaseModel):\n    default_conf = {",
        "detail": "hloc.extractors.dir",
        "documentation": {}
    },
    {
        "label": "sys.modules[\"sklearn.decomposition.pca\"]",
        "kind": 5,
        "importPath": "hloc.extractors.dir",
        "description": "hloc.extractors.dir",
        "peekOfCode": "sys.modules[\"sklearn.decomposition.pca\"] = sklearn.decomposition._pca\nclass DIR(BaseModel):\n    default_conf = {\n        \"model_name\": \"Resnet-101-AP-GeM\",\n        \"whiten_name\": \"Landmarks_clean\",\n        \"whiten_params\": {\n            \"whitenp\": 0.25,\n            \"whitenv\": None,\n            \"whitenm\": 1.0,\n        },",
        "detail": "hloc.extractors.dir",
        "documentation": {}
    },
    {
        "label": "DISK",
        "kind": 6,
        "importPath": "hloc.extractors.disk",
        "description": "hloc.extractors.disk",
        "peekOfCode": "class DISK(BaseModel):\n    default_conf = {\n        \"weights\": \"depth\",\n        \"max_keypoints\": None,\n        \"nms_window_size\": 5,\n        \"detection_threshold\": 0.0,\n        \"pad_if_not_divisible\": True,\n    }\n    required_inputs = [\"image\"]\n    def _init(self, conf):",
        "detail": "hloc.extractors.disk",
        "documentation": {}
    },
    {
        "label": "DoG",
        "kind": 6,
        "importPath": "hloc.extractors.dog",
        "description": "hloc.extractors.dog",
        "peekOfCode": "class DoG(BaseModel):\n    default_conf = {\n        \"options\": {\n            \"first_octave\": 0,\n            \"peak_threshold\": 0.01,\n        },\n        \"descriptor\": \"rootsift\",\n        \"max_keypoints\": -1,\n        \"patch_size\": 32,\n        \"mr_size\": 12,",
        "detail": "hloc.extractors.dog",
        "documentation": {}
    },
    {
        "label": "sift_to_rootsift",
        "kind": 2,
        "importPath": "hloc.extractors.dog",
        "description": "hloc.extractors.dog",
        "peekOfCode": "def sift_to_rootsift(x):\n    x = x / (np.linalg.norm(x, ord=1, axis=-1, keepdims=True) + EPS)\n    x = np.sqrt(x.clip(min=EPS))\n    x = x / (np.linalg.norm(x, axis=-1, keepdims=True) + EPS)\n    return x\nclass DoG(BaseModel):\n    default_conf = {\n        \"options\": {\n            \"first_octave\": 0,\n            \"peak_threshold\": 0.01,",
        "detail": "hloc.extractors.dog",
        "documentation": {}
    },
    {
        "label": "EPS",
        "kind": 5,
        "importPath": "hloc.extractors.dog",
        "description": "hloc.extractors.dog",
        "peekOfCode": "EPS = 1e-6\ndef sift_to_rootsift(x):\n    x = x / (np.linalg.norm(x, ord=1, axis=-1, keepdims=True) + EPS)\n    x = np.sqrt(x.clip(min=EPS))\n    x = x / (np.linalg.norm(x, axis=-1, keepdims=True) + EPS)\n    return x\nclass DoG(BaseModel):\n    default_conf = {\n        \"options\": {\n            \"first_octave\": 0,",
        "detail": "hloc.extractors.dog",
        "documentation": {}
    },
    {
        "label": "MegaPlaces",
        "kind": 6,
        "importPath": "hloc.extractors.megaloc",
        "description": "hloc.extractors.megaloc",
        "peekOfCode": "class MegaPlaces(BaseModel):\n    required_inputs = [\"image\"]\n    def _init(self, conf):\n        self.net = torch.hub.load(\"gmberton/MegaLoc\", \"get_trained_model\").eval()\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        self.norm_rgb = tvf.Normalize(mean=mean, std=std)\n    def _forward(self, data):\n        image = self.norm_rgb(data[\"image\"])\n        desc = self.net(image)",
        "detail": "hloc.extractors.megaloc",
        "documentation": {}
    },
    {
        "label": "NetVLADLayer",
        "kind": 6,
        "importPath": "hloc.extractors.netvlad",
        "description": "hloc.extractors.netvlad",
        "peekOfCode": "class NetVLADLayer(nn.Module):\n    def __init__(self, input_dim=512, K=64, score_bias=False, intranorm=True):\n        super().__init__()\n        self.score_proj = nn.Conv1d(input_dim, K, kernel_size=1, bias=score_bias)\n        centers = nn.parameter.Parameter(torch.empty([input_dim, K]))\n        nn.init.xavier_uniform_(centers)\n        self.register_parameter(\"centers\", centers)\n        self.intranorm = intranorm\n        self.output_dim = input_dim * K\n    def forward(self, x):",
        "detail": "hloc.extractors.netvlad",
        "documentation": {}
    },
    {
        "label": "NetVLAD",
        "kind": 6,
        "importPath": "hloc.extractors.netvlad",
        "description": "hloc.extractors.netvlad",
        "peekOfCode": "class NetVLAD(BaseModel):\n    default_conf = {\"model_name\": \"VGG16-NetVLAD-Pitts30K\", \"whiten\": True}\n    required_inputs = [\"image\"]\n    # Models exported using\n    # https://github.com/uzh-rpg/netvlad_tf_open/blob/master/matlab/net_class2struct.m.\n    checkpoint_urls = {\n        \"VGG16-NetVLAD-Pitts30K\": \"https://cvg-data.inf.ethz.ch/hloc/netvlad/Pitts30K_struct.mat\",  # noqa: E501\n        \"VGG16-NetVLAD-TokyoTM\": \"https://cvg-data.inf.ethz.ch/hloc/netvlad/TokyoTM_struct.mat\",  # noqa: E501\n    }\n    def _init(self, conf):",
        "detail": "hloc.extractors.netvlad",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.extractors.netvlad",
        "description": "hloc.extractors.netvlad",
        "peekOfCode": "logger = logging.getLogger(__name__)\nEPS = 1e-6\nclass NetVLADLayer(nn.Module):\n    def __init__(self, input_dim=512, K=64, score_bias=False, intranorm=True):\n        super().__init__()\n        self.score_proj = nn.Conv1d(input_dim, K, kernel_size=1, bias=score_bias)\n        centers = nn.parameter.Parameter(torch.empty([input_dim, K]))\n        nn.init.xavier_uniform_(centers)\n        self.register_parameter(\"centers\", centers)\n        self.intranorm = intranorm",
        "detail": "hloc.extractors.netvlad",
        "documentation": {}
    },
    {
        "label": "EPS",
        "kind": 5,
        "importPath": "hloc.extractors.netvlad",
        "description": "hloc.extractors.netvlad",
        "peekOfCode": "EPS = 1e-6\nclass NetVLADLayer(nn.Module):\n    def __init__(self, input_dim=512, K=64, score_bias=False, intranorm=True):\n        super().__init__()\n        self.score_proj = nn.Conv1d(input_dim, K, kernel_size=1, bias=score_bias)\n        centers = nn.parameter.Parameter(torch.empty([input_dim, K]))\n        nn.init.xavier_uniform_(centers)\n        self.register_parameter(\"centers\", centers)\n        self.intranorm = intranorm\n        self.output_dim = input_dim * K",
        "detail": "hloc.extractors.netvlad",
        "documentation": {}
    },
    {
        "label": "OpenIBL",
        "kind": 6,
        "importPath": "hloc.extractors.openibl",
        "description": "hloc.extractors.openibl",
        "peekOfCode": "class OpenIBL(BaseModel):\n    default_conf = {\n        \"model_name\": \"vgg16_netvlad\",\n    }\n    required_inputs = [\"image\"]\n    def _init(self, conf):\n        self.net = torch.hub.load(\n            \"yxgeee/OpenIBL\", conf[\"model_name\"], pretrained=True\n        ).eval()\n        mean = [0.48501960784313836, 0.4579568627450961, 0.4076039215686255]",
        "detail": "hloc.extractors.openibl",
        "documentation": {}
    },
    {
        "label": "R2D2",
        "kind": 6,
        "importPath": "hloc.extractors.r2d2",
        "description": "hloc.extractors.r2d2",
        "peekOfCode": "class R2D2(BaseModel):\n    default_conf = {\n        \"model_name\": \"r2d2_WASF_N16.pt\",\n        \"max_keypoints\": 5000,\n        \"scale_factor\": 2**0.25,\n        \"min_size\": 256,\n        \"max_size\": 1024,\n        \"min_scale\": 0,\n        \"max_scale\": 1,\n        \"reliability_threshold\": 0.7,",
        "detail": "hloc.extractors.r2d2",
        "documentation": {}
    },
    {
        "label": "r2d2_path",
        "kind": 5,
        "importPath": "hloc.extractors.r2d2",
        "description": "hloc.extractors.r2d2",
        "peekOfCode": "r2d2_path = Path(__file__).parent / \"../../third_party/r2d2\"\nsys.path.append(str(r2d2_path))\nfrom extract import NonMaxSuppression, extract_multiscale, load_network  # noqa: E402\nclass R2D2(BaseModel):\n    default_conf = {\n        \"model_name\": \"r2d2_WASF_N16.pt\",\n        \"max_keypoints\": 5000,\n        \"scale_factor\": 2**0.25,\n        \"min_size\": 256,\n        \"max_size\": 1024,",
        "detail": "hloc.extractors.r2d2",
        "documentation": {}
    },
    {
        "label": "SuperPoint",
        "kind": 6,
        "importPath": "hloc.extractors.superpoint",
        "description": "hloc.extractors.superpoint",
        "peekOfCode": "class SuperPoint(BaseModel):\n    default_conf = {\n        \"nms_radius\": 4,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": -1,\n        \"remove_borders\": 4,\n        \"fix_sampling\": False,\n    }\n    required_inputs = [\"image\"]\n    detection_noise = 2.0",
        "detail": "hloc.extractors.superpoint",
        "documentation": {}
    },
    {
        "label": "sample_descriptors_fix_sampling",
        "kind": 2,
        "importPath": "hloc.extractors.superpoint",
        "description": "hloc.extractors.superpoint",
        "peekOfCode": "def sample_descriptors_fix_sampling(keypoints, descriptors, s: int = 8):\n    \"\"\"Interpolate descriptors at keypoint locations\"\"\"\n    b, c, h, w = descriptors.shape\n    keypoints = (keypoints + 0.5) / (keypoints.new_tensor([w, h]) * s)\n    keypoints = keypoints * 2 - 1  # normalize to (-1, 1)\n    descriptors = torch.nn.functional.grid_sample(\n        descriptors, keypoints.view(b, 1, -1, 2), mode=\"bilinear\", align_corners=False\n    )\n    descriptors = torch.nn.functional.normalize(\n        descriptors.reshape(b, c, -1), p=2, dim=1",
        "detail": "hloc.extractors.superpoint",
        "documentation": {}
    },
    {
        "label": "AdaLAM",
        "kind": 6,
        "importPath": "hloc.matchers.adalam",
        "description": "hloc.matchers.adalam",
        "peekOfCode": "class AdaLAM(BaseModel):\n    default_conf = {\n        \"area_ratio\": 100,\n        \"search_expansion\": 4,\n        \"ransac_iters\": 128,\n        \"min_inliers\": 6,\n        \"min_confidence\": 200,\n        \"orientation_difference_threshold\": 30,\n        \"scale_rate_threshold\": 1.5,\n        \"detected_scale_rate_threshold\": 5,",
        "detail": "hloc.matchers.adalam",
        "documentation": {}
    },
    {
        "label": "LightGlue",
        "kind": 6,
        "importPath": "hloc.matchers.lightglue",
        "description": "hloc.matchers.lightglue",
        "peekOfCode": "class LightGlue(BaseModel):\n    default_conf = {\n        \"features\": \"superpoint\",\n        \"depth_confidence\": 0.95,\n        \"width_confidence\": 0.99,\n    }\n    required_inputs = [\n        \"image0\",\n        \"keypoints0\",\n        \"descriptors0\",",
        "detail": "hloc.matchers.lightglue",
        "documentation": {}
    },
    {
        "label": "LoFTR",
        "kind": 6,
        "importPath": "hloc.matchers.loftr",
        "description": "hloc.matchers.loftr",
        "peekOfCode": "class LoFTR(BaseModel):\n    default_conf = {\n        \"weights\": \"outdoor\",\n        \"match_threshold\": 0.2,\n        \"max_num_matches\": None,\n    }\n    required_inputs = [\"image0\", \"image1\"]\n    def _init(self, conf):\n        cfg = default_cfg\n        cfg[\"match_coarse\"][\"thr\"] = conf[\"match_threshold\"]",
        "detail": "hloc.matchers.loftr",
        "documentation": {}
    },
    {
        "label": "NearestNeighbor",
        "kind": 6,
        "importPath": "hloc.matchers.nearest_neighbor",
        "description": "hloc.matchers.nearest_neighbor",
        "peekOfCode": "class NearestNeighbor(BaseModel):\n    default_conf = {\n        \"ratio_threshold\": None,\n        \"distance_threshold\": None,\n        \"do_mutual_check\": True,\n    }\n    required_inputs = [\"descriptors0\", \"descriptors1\"]\n    def _init(self, conf):\n        pass\n    def _forward(self, data):",
        "detail": "hloc.matchers.nearest_neighbor",
        "documentation": {}
    },
    {
        "label": "find_nn",
        "kind": 2,
        "importPath": "hloc.matchers.nearest_neighbor",
        "description": "hloc.matchers.nearest_neighbor",
        "peekOfCode": "def find_nn(sim, ratio_thresh, distance_thresh):\n    sim_nn, ind_nn = sim.topk(2 if ratio_thresh else 1, dim=-1, largest=True)\n    dist_nn = 2 * (1 - sim_nn)\n    mask = torch.ones(ind_nn.shape[:-1], dtype=torch.bool, device=sim.device)\n    if ratio_thresh:\n        mask = mask & (dist_nn[..., 0] <= (ratio_thresh**2) * dist_nn[..., 1])\n    if distance_thresh:\n        mask = mask & (dist_nn[..., 0] <= distance_thresh**2)\n    matches = torch.where(mask, ind_nn[..., 0], ind_nn.new_tensor(-1))\n    scores = torch.where(mask, (sim_nn[..., 0] + 1) / 2, sim_nn.new_tensor(0))",
        "detail": "hloc.matchers.nearest_neighbor",
        "documentation": {}
    },
    {
        "label": "mutual_check",
        "kind": 2,
        "importPath": "hloc.matchers.nearest_neighbor",
        "description": "hloc.matchers.nearest_neighbor",
        "peekOfCode": "def mutual_check(m0, m1):\n    inds0 = torch.arange(m0.shape[-1], device=m0.device)\n    loop = torch.gather(m1, -1, torch.where(m0 > -1, m0, m0.new_tensor(0)))\n    ok = (m0 > -1) & (inds0 == loop)\n    m0_new = torch.where(ok, m0, m0.new_tensor(-1))\n    return m0_new\nclass NearestNeighbor(BaseModel):\n    default_conf = {\n        \"ratio_threshold\": None,\n        \"distance_threshold\": None,",
        "detail": "hloc.matchers.nearest_neighbor",
        "documentation": {}
    },
    {
        "label": "SuperGlue",
        "kind": 6,
        "importPath": "hloc.matchers.superglue",
        "description": "hloc.matchers.superglue",
        "peekOfCode": "class SuperGlue(BaseModel):\n    default_conf = {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 100,\n        \"match_threshold\": 0.2,\n    }\n    required_inputs = [\n        \"image0\",\n        \"keypoints0\",\n        \"scores0\",",
        "detail": "hloc.matchers.superglue",
        "documentation": {}
    },
    {
        "label": "relocalization_files",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "relocalization_files = {\n    \"training\": \"RelocalizationFilesTrain//relocalizationFile_recording_2020-03-24_17-36-22.txt\",  # noqa: E501\n    \"validation\": \"RelocalizationFilesVal/relocalizationFile_recording_2020-03-03_12-03-23.txt\",  # noqa: E501\n    \"test0\": \"RelocalizationFilesTest/relocalizationFile_recording_2020-03-24_17-45-31_*.txt\",  # noqa: E501\n    \"test1\": \"RelocalizationFilesTest/relocalizationFile_recording_2020-04-23_19-37-00_*.txt\",  # noqa: E501\n}\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--sequence\",\n    type=str,",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--sequence\",\n    type=str,\n    required=True,\n    choices=[\"training\", \"validation\", \"test0\", \"test1\"],\n    help=\"Sequence to be relocalized.\",\n)\nparser.add_argument(\n    \"--dataset\",",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "args = parser.parse_args()\nsequence = args.sequence\ndata_dir = args.dataset\nref_dir = data_dir / \"reference\"\nassert ref_dir.exists(), f\"{ref_dir} does not exist\"\nseq_dir = data_dir / sequence\nassert seq_dir.exists(), f\"{seq_dir} does not exist\"\nseq_images = seq_dir / \"undistorted_images\"\nreloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "sequence",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "sequence = args.sequence\ndata_dir = args.dataset\nref_dir = data_dir / \"reference\"\nassert ref_dir.exists(), f\"{ref_dir} does not exist\"\nseq_dir = data_dir / sequence\nassert seq_dir.exists(), f\"{seq_dir} does not exist\"\nseq_images = seq_dir / \"undistorted_images\"\nreloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "data_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "data_dir = args.dataset\nref_dir = data_dir / \"reference\"\nassert ref_dir.exists(), f\"{ref_dir} does not exist\"\nseq_dir = data_dir / sequence\nassert seq_dir.exists(), f\"{seq_dir} does not exist\"\nseq_images = seq_dir / \"undistorted_images\"\nreloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nquery_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "ref_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "ref_dir = data_dir / \"reference\"\nassert ref_dir.exists(), f\"{ref_dir} does not exist\"\nseq_dir = data_dir / sequence\nassert seq_dir.exists(), f\"{seq_dir} does not exist\"\nseq_images = seq_dir / \"undistorted_images\"\nreloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nquery_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"\nref_pairs = output_dir / \"pairs-db-dist20.txt\"",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "seq_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "seq_dir = data_dir / sequence\nassert seq_dir.exists(), f\"{seq_dir} does not exist\"\nseq_images = seq_dir / \"undistorted_images\"\nreloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nquery_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"\nref_pairs = output_dir / \"pairs-db-dist20.txt\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "seq_images",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "seq_images = seq_dir / \"undistorted_images\"\nreloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nquery_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"\nref_pairs = output_dir / \"pairs-db-dist20.txt\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "reloc",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "reloc = ref_dir / relocalization_files[sequence]\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nquery_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"\nref_pairs = output_dir / \"pairs-db-dist20.txt\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "output_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "output_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nquery_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"\nref_pairs = output_dir / \"pairs-db-dist20.txt\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "query_list",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "query_list = output_dir / f\"{sequence}_queries_with_intrinsics.txt\"\nref_pairs = output_dir / \"pairs-db-dist20.txt\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "ref_pairs",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "ref_pairs = output_dir / \"pairs-db-dist20.txt\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "ref_sfm",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "ref_sfm = output_dir / \"sfm_superpoint+superglue\"\nresults_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "results_path",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "results_path = output_dir / f\"localization_{sequence}_hloc+superglue.txt\"\nsubmission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "submission_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "submission_dir = output_dir / \"submission_hloc+superglue\"\nnum_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)\n# Generate a list of query images with their intrinsics.",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "num_loc_pairs",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "num_loc_pairs = 10\nloc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)\n# Generate a list of query images with their intrinsics.\ngenerate_query_lists(timestamps, seq_dir, query_list)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "loc_pairs",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "loc_pairs = output_dir / f\"pairs-query-{sequence}-dist{num_loc_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)\n# Generate a list of query images with their intrinsics.\ngenerate_query_lists(timestamps, seq_dir, query_list)\n# Generate the localization pairs from the given reference frames.",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "fconf",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "fconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)\n# Generate a list of query images with their intrinsics.\ngenerate_query_lists(timestamps, seq_dir, query_list)\n# Generate the localization pairs from the given reference frames.\ngenerate_localization_pairs(sequence, reloc, num_loc_pairs, ref_pairs, loc_pairs)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "mconf",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "mconf = match_features.confs[\"superglue\"]\n# Not all query images that are used for the evaluation\n# To save time in feature extraction, we delete unsused images.\ntimestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)\n# Generate a list of query images with their intrinsics.\ngenerate_query_lists(timestamps, seq_dir, query_list)\n# Generate the localization pairs from the given reference frames.\ngenerate_localization_pairs(sequence, reloc, num_loc_pairs, ref_pairs, loc_pairs)\n# Extract, match, amd localize.",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "timestamps",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "timestamps = get_timestamps(reloc, 1)\ndelete_unused_images(seq_images, timestamps)\n# Generate a list of query images with their intrinsics.\ngenerate_query_lists(timestamps, seq_dir, query_list)\n# Generate the localization pairs from the given reference frames.\ngenerate_localization_pairs(sequence, reloc, num_loc_pairs, ref_pairs, loc_pairs)\n# Extract, match, amd localize.\nffile = extract_features.main(fconf, seq_images, output_dir)\nmfile = match_features.main(mconf, loc_pairs, fconf[\"output\"], output_dir)\nlocalize_sfm.main(ref_sfm, query_list, loc_pairs, ffile, mfile, results_path)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "ffile",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "ffile = extract_features.main(fconf, seq_images, output_dir)\nmfile = match_features.main(mconf, loc_pairs, fconf[\"output\"], output_dir)\nlocalize_sfm.main(ref_sfm, query_list, loc_pairs, ffile, mfile, results_path)\n# Convert the absolute poses to relative poses with the reference frames.\nsubmission_dir.mkdir(exist_ok=True)\nprepare_submission(results_path, reloc, ref_dir / \"poses.txt\", submission_dir)\n# If not a test sequence: evaluation the localization accuracy\nif \"test\" not in sequence:\n    logger.info(\"Evaluating the relocalization submission...\")\n    evaluate_submission(submission_dir, reloc)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "mfile",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.localize",
        "description": "hloc.pipelines.4Seasons.localize",
        "peekOfCode": "mfile = match_features.main(mconf, loc_pairs, fconf[\"output\"], output_dir)\nlocalize_sfm.main(ref_sfm, query_list, loc_pairs, ffile, mfile, results_path)\n# Convert the absolute poses to relative poses with the reference frames.\nsubmission_dir.mkdir(exist_ok=True)\nprepare_submission(results_path, reloc, ref_dir / \"poses.txt\", submission_dir)\n# If not a test sequence: evaluation the localization accuracy\nif \"test\" not in sequence:\n    logger.info(\"Evaluating the relocalization submission...\")\n    evaluate_submission(submission_dir, reloc)",
        "detail": "hloc.pipelines.4Seasons.localize",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--dataset\",\n    type=Path,\n    default=\"datasets/4Seasons\",\n    help=\"Path to the dataset, default: %(default)s\",\n)\nparser.add_argument(\n    \"--outputs\",\n    type=Path,",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "args = parser.parse_args()\nref_dir = args.dataset / \"reference\"\nassert ref_dir.exists(), f\"{ref_dir} does not exist\"\nref_images = ref_dir / \"undistorted_images\"\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nref_sfm_empty = output_dir / \"sfm_reference_empty\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nnum_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "ref_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "ref_dir = args.dataset / \"reference\"\nassert ref_dir.exists(), f\"{ref_dir} does not exist\"\nref_images = ref_dir / \"undistorted_images\"\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nref_sfm_empty = output_dir / \"sfm_reference_empty\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nnum_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "ref_images",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "ref_images = ref_dir / \"undistorted_images\"\noutput_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nref_sfm_empty = output_dir / \"sfm_reference_empty\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nnum_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "output_dir",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "output_dir = args.outputs\noutput_dir.mkdir(exist_ok=True, parents=True)\nref_sfm_empty = output_dir / \"sfm_reference_empty\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nnum_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "ref_sfm_empty",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "ref_sfm_empty = output_dir / \"sfm_reference_empty\"\nref_sfm = output_dir / \"sfm_superpoint+superglue\"\nnum_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.\ndelete_unused_images(ref_images, get_timestamps(ref_dir / \"poses.txt\", 0))\n# Build an empty COLMAP model containing only camera and images",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "ref_sfm",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "ref_sfm = output_dir / \"sfm_superpoint+superglue\"\nnum_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.\ndelete_unused_images(ref_images, get_timestamps(ref_dir / \"poses.txt\", 0))\n# Build an empty COLMAP model containing only camera and images\n# from the provided poses and intrinsics.",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "num_ref_pairs",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "num_ref_pairs = 20\nref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.\ndelete_unused_images(ref_images, get_timestamps(ref_dir / \"poses.txt\", 0))\n# Build an empty COLMAP model containing only camera and images\n# from the provided poses and intrinsics.\nbuild_empty_colmap_model(ref_dir, ref_sfm_empty)",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "ref_pairs",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "ref_pairs = output_dir / f\"pairs-db-dist{num_ref_pairs}.txt\"\nfconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.\ndelete_unused_images(ref_images, get_timestamps(ref_dir / \"poses.txt\", 0))\n# Build an empty COLMAP model containing only camera and images\n# from the provided poses and intrinsics.\nbuild_empty_colmap_model(ref_dir, ref_sfm_empty)\n# Match reference images that are spatially close.",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "fconf",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "fconf = extract_features.confs[\"superpoint_max\"]\nmconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.\ndelete_unused_images(ref_images, get_timestamps(ref_dir / \"poses.txt\", 0))\n# Build an empty COLMAP model containing only camera and images\n# from the provided poses and intrinsics.\nbuild_empty_colmap_model(ref_dir, ref_sfm_empty)\n# Match reference images that are spatially close.\npairs_from_poses.main(ref_sfm_empty, ref_pairs, num_ref_pairs)",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "mconf",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "mconf = match_features.confs[\"superglue\"]\n# Only reference images that have a pose are used in the pipeline.\n# To save time in feature extraction, we delete unsused images.\ndelete_unused_images(ref_images, get_timestamps(ref_dir / \"poses.txt\", 0))\n# Build an empty COLMAP model containing only camera and images\n# from the provided poses and intrinsics.\nbuild_empty_colmap_model(ref_dir, ref_sfm_empty)\n# Match reference images that are spatially close.\npairs_from_poses.main(ref_sfm_empty, ref_pairs, num_ref_pairs)\n# Extract, match, and triangulate the reference SfM model.",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "ffile",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "ffile = extract_features.main(fconf, ref_images, output_dir)\nmfile = match_features.main(mconf, ref_pairs, fconf[\"output\"], output_dir)\ntriangulation.main(ref_sfm, ref_sfm_empty, ref_images, ref_pairs, ffile, mfile)",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "mfile",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.prepare_reference",
        "description": "hloc.pipelines.4Seasons.prepare_reference",
        "peekOfCode": "mfile = match_features.main(mconf, ref_pairs, fconf[\"output\"], output_dir)\ntriangulation.main(ref_sfm, ref_sfm_empty, ref_images, ref_pairs, ffile, mfile)",
        "detail": "hloc.pipelines.4Seasons.prepare_reference",
        "documentation": {}
    },
    {
        "label": "get_timestamps",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def get_timestamps(files, idx):\n    \"\"\"Extract timestamps from a pose or relocalization file.\"\"\"\n    lines = []\n    for p in files.parent.glob(files.name):\n        with open(p) as f:\n            lines += f.readlines()\n    timestamps = set()\n    for line in lines:\n        line = line.rstrip(\"\\n\")\n        if line[0] == \"#\" or line == \"\":",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "delete_unused_images",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def delete_unused_images(root, timestamps):\n    \"\"\"Delete all images in root if they are not contained in timestamps.\"\"\"\n    images = glob.glob((root / \"**/*.png\").as_posix(), recursive=True)\n    deleted = 0\n    for image in images:\n        ts = Path(image).stem\n        if ts not in timestamps:\n            os.remove(image)\n            deleted += 1\n    logger.info(f\"Deleted {deleted} images in {root}.\")",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "camera_from_calibration_file",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def camera_from_calibration_file(id_, path):\n    \"\"\"Create a COLMAP camera from an MLAD calibration file.\"\"\"\n    with open(path, \"r\") as f:\n        data = f.readlines()\n    model, fx, fy, cx, cy = data[0].split()[:5]\n    width, height = data[1].split()\n    assert model == \"Pinhole\"\n    model_name = \"PINHOLE\"\n    params = [float(i) for i in [fx, fy, cx, cy]]\n    camera = Camera(",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "parse_poses",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def parse_poses(path, colmap=False):\n    \"\"\"Parse a list of poses in COLMAP or MLAD quaternion convention.\"\"\"\n    poses = []\n    with open(path) as f:\n        for line in f.readlines():\n            line = line.rstrip(\"\\n\")\n            if line[0] == \"#\" or line == \"\":\n                continue\n            data = line.replace(\",\", \" \").split()\n            ts, p = data[0], np.array(data[1:], float)",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "parse_relocalization",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def parse_relocalization(path, has_poses=False):\n    \"\"\"Parse a relocalization file, possibly with poses.\"\"\"\n    reloc = []\n    with open(path) as f:\n        for line in f.readlines():\n            line = line.rstrip(\"\\n\")\n            if line[0] == \"#\" or line == \"\":\n                continue\n            data = line.replace(\",\", \" \").split()\n            out = data[:2]  # ref_ts, q_ts",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "build_empty_colmap_model",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def build_empty_colmap_model(root, sfm_dir):\n    \"\"\"Build a COLMAP model with images and cameras only.\"\"\"\n    calibration = \"Calibration/undistorted_calib_{}.txt\"\n    cam0 = camera_from_calibration_file(0, root / calibration.format(0))\n    cam1 = camera_from_calibration_file(1, root / calibration.format(1))\n    cameras = {0: cam0, 1: cam1}\n    T_0to1 = np.loadtxt(root / \"Calibration/undistorted_calib_stereo.txt\")\n    poses = parse_poses(root / \"poses.txt\")\n    images = {}\n    id_ = 0",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "generate_query_lists",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def generate_query_lists(timestamps, seq_dir, out_path):\n    \"\"\"Create a list of query images with intrinsics from timestamps.\"\"\"\n    cam0 = camera_from_calibration_file(\n        0, seq_dir / \"Calibration/undistorted_calib_0.txt\"\n    )\n    intrinsics = [cam0.model, cam0.width, cam0.height] + cam0.params\n    intrinsics = [str(p) for p in intrinsics]\n    data = map(lambda ts: \" \".join([f\"cam0/{ts}.png\"] + intrinsics), timestamps)\n    with open(out_path, \"w\") as f:\n        f.write(\"\\n\".join(data))",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "generate_localization_pairs",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def generate_localization_pairs(sequence, reloc, num, ref_pairs, out_path):\n    \"\"\"Create the matching pairs for the localization.\n    We simply lookup the corresponding reference frame\n    and extract its `num` closest frames from the existing pair list.\n    \"\"\"\n    if \"test\" in sequence:\n        # hard pairs will be overwritten by easy ones if available\n        relocs = [str(reloc).replace(\"*\", d) for d in [\"hard\", \"moderate\", \"easy\"]]\n    else:\n        relocs = [reloc]",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "prepare_submission",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def prepare_submission(results, relocs, poses_path, out_dir):\n    \"\"\"Obtain relative poses from estimated absolute and reference poses.\"\"\"\n    gt_poses = parse_poses(poses_path)\n    all_T_ref0_to_w = {ts: (R, t) for ts, R, t in gt_poses}\n    pred_poses = parse_poses(results, colmap=True)\n    all_T_w_to_q0 = {Path(name).stem: (R, t) for name, R, t in pred_poses}\n    for reloc in relocs.parent.glob(relocs.name):\n        relative_poses = []\n        reloc_ts = parse_relocalization(reloc)\n        for ref_ts, q_ts in reloc_ts:",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "evaluate_submission",
        "kind": 2,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "def evaluate_submission(submission_dir, relocs, ths=[0.1, 0.2, 0.5]):\n    \"\"\"Compute the relocalization recall from predicted and ground truth poses.\"\"\"\n    for reloc in relocs.parent.glob(relocs.name):\n        poses_gt = parse_relocalization(reloc, has_poses=True)\n        poses_pred = parse_relocalization(submission_dir / reloc.name, has_poses=True)\n        poses_pred = {(ref_ts, q_ts): (R, t) for ref_ts, q_ts, R, t in poses_pred}\n        error = []\n        for ref_ts, q_ts, R_gt, t_gt in poses_gt:\n            R, t = poses_pred[(ref_ts, q_ts)]\n            e = np.linalg.norm(t - t_gt)",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.pipelines.4Seasons.utils",
        "description": "hloc.pipelines.4Seasons.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef get_timestamps(files, idx):\n    \"\"\"Extract timestamps from a pose or relocalization file.\"\"\"\n    lines = []\n    for p in files.parent.glob(files.name):\n        with open(p) as f:\n            lines += f.readlines()\n    timestamps = set()\n    for line in lines:\n        line = line.rstrip(\"\\n\")",
        "detail": "hloc.pipelines.4Seasons.utils",
        "documentation": {}
    },
    {
        "label": "scene_coordinates",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.create_gt_sfm",
        "description": "hloc.pipelines.7Scenes.create_gt_sfm",
        "peekOfCode": "def scene_coordinates(p2D, R_w2c, t_w2c, depth, camera):\n    assert len(depth) == len(p2D)\n    p2D_norm = np.stack(pycolmap.Camera(camera._asdict()).image_to_world(p2D))\n    p2D_h = np.concatenate([p2D_norm, np.ones_like(p2D_norm[:, :1])], 1)\n    p3D_c = p2D_h * depth[:, None]\n    p3D_w = (p3D_c - t_w2c) @ R_w2c\n    return p3D_w\ndef interpolate_depth(depth, kp):\n    h, w = depth.shape\n    kp = kp / np.array([[w - 1, h - 1]]) * 2 - 1",
        "detail": "hloc.pipelines.7Scenes.create_gt_sfm",
        "documentation": {}
    },
    {
        "label": "interpolate_depth",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.create_gt_sfm",
        "description": "hloc.pipelines.7Scenes.create_gt_sfm",
        "peekOfCode": "def interpolate_depth(depth, kp):\n    h, w = depth.shape\n    kp = kp / np.array([[w - 1, h - 1]]) * 2 - 1\n    assert np.all(kp > -1) and np.all(kp < 1)\n    depth = torch.from_numpy(depth)[None, None]\n    kp = torch.from_numpy(kp)[None, None]\n    grid_sample = torch.nn.functional.grid_sample\n    # To maximize the number of points that have depth:\n    # do bilinear interpolation first and then nearest for the remaining points\n    interp_lin = grid_sample(depth, kp, align_corners=True, mode=\"bilinear\")[0, :, 0]",
        "detail": "hloc.pipelines.7Scenes.create_gt_sfm",
        "documentation": {}
    },
    {
        "label": "image_path_to_rendered_depth_path",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.create_gt_sfm",
        "description": "hloc.pipelines.7Scenes.create_gt_sfm",
        "peekOfCode": "def image_path_to_rendered_depth_path(image_name):\n    parts = image_name.split(\"/\")\n    name = \"_\".join([\"\".join(parts[0].split(\"-\")), parts[1]])\n    name = name.replace(\"color\", \"pose\")\n    name = name.replace(\"png\", \"depth.tiff\")\n    return name\ndef project_to_image(p3D, R, t, camera, eps: float = 1e-4, pad: int = 1):\n    p3D = (p3D @ R.T) + t\n    visible = p3D[:, -1] >= eps  # keep points in front of the camera\n    p2D_norm = p3D[:, :-1] / p3D[:, -1:].clip(min=eps)",
        "detail": "hloc.pipelines.7Scenes.create_gt_sfm",
        "documentation": {}
    },
    {
        "label": "project_to_image",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.create_gt_sfm",
        "description": "hloc.pipelines.7Scenes.create_gt_sfm",
        "peekOfCode": "def project_to_image(p3D, R, t, camera, eps: float = 1e-4, pad: int = 1):\n    p3D = (p3D @ R.T) + t\n    visible = p3D[:, -1] >= eps  # keep points in front of the camera\n    p2D_norm = p3D[:, :-1] / p3D[:, -1:].clip(min=eps)\n    p2D = np.stack(pycolmap.Camera(camera._asdict()).world_to_image(p2D_norm))\n    size = np.array([camera.width - pad - 1, camera.height - pad - 1])\n    valid = np.all((p2D >= pad) & (p2D <= size), -1)\n    valid &= visible\n    return p2D[valid], valid\ndef correct_sfm_with_gt_depth(sfm_path, depth_folder_path, output_path):",
        "detail": "hloc.pipelines.7Scenes.create_gt_sfm",
        "documentation": {}
    },
    {
        "label": "correct_sfm_with_gt_depth",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.create_gt_sfm",
        "description": "hloc.pipelines.7Scenes.create_gt_sfm",
        "peekOfCode": "def correct_sfm_with_gt_depth(sfm_path, depth_folder_path, output_path):\n    cameras, images, points3D = read_model(sfm_path)\n    for imgid, img in tqdm(images.items()):\n        image_name = img.name\n        depth_name = image_path_to_rendered_depth_path(image_name)\n        depth = PIL.Image.open(Path(depth_folder_path) / depth_name)\n        depth = np.array(depth).astype(\"float64\")\n        depth = depth / 1000.0  # mm to meter\n        depth[(depth == 0.0) | (depth > 1000.0)] = np.nan\n        R_w2c, t_w2c = img.qvec2rotmat(), img.tvec",
        "detail": "hloc.pipelines.7Scenes.create_gt_sfm",
        "documentation": {}
    },
    {
        "label": "run_scene",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.pipeline",
        "description": "hloc.pipelines.7Scenes.pipeline",
        "peekOfCode": "def run_scene(\n    images,\n    gt_dir,\n    retrieval,\n    outputs,\n    results,\n    num_covis,\n    use_dense_depth,\n    depth_dir=None,\n):",
        "detail": "hloc.pipelines.7Scenes.pipeline",
        "documentation": {}
    },
    {
        "label": "SCENES",
        "kind": 5,
        "importPath": "hloc.pipelines.7Scenes.pipeline",
        "description": "hloc.pipelines.7Scenes.pipeline",
        "peekOfCode": "SCENES = [\"chess\", \"fire\", \"heads\", \"office\", \"pumpkin\", \"redkitchen\", \"stairs\"]\ndef run_scene(\n    images,\n    gt_dir,\n    retrieval,\n    outputs,\n    results,\n    num_covis,\n    use_dense_depth,\n    depth_dir=None,",
        "detail": "hloc.pipelines.7Scenes.pipeline",
        "documentation": {}
    },
    {
        "label": "create_reference_sfm",
        "kind": 2,
        "importPath": "hloc.pipelines.7Scenes.utils",
        "description": "hloc.pipelines.7Scenes.utils",
        "peekOfCode": "def create_reference_sfm(full_model, ref_model, blacklist=None, ext=\".bin\"):\n    \"\"\"Create a new COLMAP model with only training images.\"\"\"\n    logger.info(\"Creating the reference model.\")\n    ref_model.mkdir(exist_ok=True)\n    cameras, images, points3D = read_model(full_model, ext)\n    if blacklist is not None:\n        with open(blacklist, \"r\") as f:\n            blacklist = f.read().rstrip().split(\"\\n\")\n    images_ref = dict()\n    for id_, image in images.items():",
        "detail": "hloc.pipelines.7Scenes.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.pipelines.7Scenes.utils",
        "description": "hloc.pipelines.7Scenes.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef create_reference_sfm(full_model, ref_model, blacklist=None, ext=\".bin\"):\n    \"\"\"Create a new COLMAP model with only training images.\"\"\"\n    logger.info(\"Creating the reference model.\")\n    ref_model.mkdir(exist_ok=True)\n    cameras, images, points3D = read_model(full_model, ext)\n    if blacklist is not None:\n        with open(blacklist, \"r\") as f:\n            blacklist = f.read().rstrip().split(\"\\n\")\n    images_ref = dict()",
        "detail": "hloc.pipelines.7Scenes.utils",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "hloc.pipelines.Aachen.pipeline",
        "description": "hloc.pipelines.Aachen.pipeline",
        "peekOfCode": "def run(args):\n    # Setup the paths\n    dataset = args.dataset\n    images = dataset / \"images_upright/\"\n    outputs = args.outputs  # where everything will be saved\n    sift_sfm = outputs / \"sfm_sift\"  # from which we extract the reference poses\n    reference_sfm = outputs / \"sfm_superpoint+superglue\"  # the SfM model we will build\n    sfm_pairs = (\n        outputs / f\"pairs-db-covis{args.num_covis}.txt\"\n    )  # top-k most covisible in SIFT model",
        "detail": "hloc.pipelines.Aachen.pipeline",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "hloc.pipelines.Aachen_v1_1.pipeline",
        "description": "hloc.pipelines.Aachen_v1_1.pipeline",
        "peekOfCode": "def run(args):\n    # Setup the paths\n    dataset = args.dataset\n    images = dataset / \"images_upright/\"\n    sift_sfm = dataset / \"3D-models/aachen_v_1_1\"\n    outputs = args.outputs  # where everything will be saved\n    reference_sfm = outputs / \"sfm_superpoint+superglue\"  # the SfM model we will build\n    sfm_pairs = (\n        outputs / f\"pairs-db-covis{args.num_covis}.txt\"\n    )  # top-k most covisible in SIFT model",
        "detail": "hloc.pipelines.Aachen_v1_1.pipeline",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "hloc.pipelines.Aachen_v1_1.pipeline_loftr",
        "description": "hloc.pipelines.Aachen_v1_1.pipeline_loftr",
        "peekOfCode": "def run(args):\n    # Setup the paths\n    dataset = args.dataset\n    images = dataset / \"images_upright/\"\n    sift_sfm = dataset / \"3D-models/aachen_v_1_1\"\n    outputs = args.outputs  # where everything will be saved\n    outputs.mkdir()\n    reference_sfm = outputs / \"sfm_loftr\"  # the SfM model we will build\n    sfm_pairs = (\n        outputs / f\"pairs-db-covis{args.num_covis}.txt\"",
        "detail": "hloc.pipelines.Aachen_v1_1.pipeline_loftr",
        "documentation": {}
    },
    {
        "label": "generate_query_list",
        "kind": 2,
        "importPath": "hloc.pipelines.CMU.pipeline",
        "description": "hloc.pipelines.CMU.pipeline",
        "peekOfCode": "def generate_query_list(dataset, path, slice_):\n    cameras = {}\n    with open(dataset / \"intrinsics.txt\", \"r\") as f:\n        for line in f.readlines():\n            if line[0] == \"#\" or line == \"\\n\":\n                continue\n            data = line.split()\n            cameras[data[0]] = data[1:]\n    assert len(cameras) == 2\n    queries = dataset / f\"{slice_}/test-images-{slice_}.txt\"",
        "detail": "hloc.pipelines.CMU.pipeline",
        "documentation": {}
    },
    {
        "label": "run_slice",
        "kind": 2,
        "importPath": "hloc.pipelines.CMU.pipeline",
        "description": "hloc.pipelines.CMU.pipeline",
        "peekOfCode": "def run_slice(slice_, root, outputs, num_covis, num_loc):\n    dataset = root / slice_\n    ref_images = dataset / \"database\"\n    query_images = dataset / \"query\"\n    sift_sfm = dataset / \"sparse\"\n    outputs = outputs / slice_\n    outputs.mkdir(exist_ok=True, parents=True)\n    query_list = dataset / \"queries_with_intrinsics.txt\"\n    sfm_pairs = outputs / f\"pairs-db-covis{num_covis}.txt\"\n    loc_pairs = outputs / f\"pairs-query-netvlad{num_loc}.txt\"",
        "detail": "hloc.pipelines.CMU.pipeline",
        "documentation": {}
    },
    {
        "label": "TEST_SLICES",
        "kind": 5,
        "importPath": "hloc.pipelines.CMU.pipeline",
        "description": "hloc.pipelines.CMU.pipeline",
        "peekOfCode": "TEST_SLICES = [2, 3, 4, 5, 6, 13, 14, 15, 16, 17, 18, 19, 20, 21]\ndef generate_query_list(dataset, path, slice_):\n    cameras = {}\n    with open(dataset / \"intrinsics.txt\", \"r\") as f:\n        for line in f.readlines():\n            if line[0] == \"#\" or line == \"\\n\":\n                continue\n            data = line.split()\n            cameras[data[0]] = data[1:]\n    assert len(cameras) == 2",
        "detail": "hloc.pipelines.CMU.pipeline",
        "documentation": {}
    },
    {
        "label": "run_scene",
        "kind": 2,
        "importPath": "hloc.pipelines.Cambridge.pipeline",
        "description": "hloc.pipelines.Cambridge.pipeline",
        "peekOfCode": "def run_scene(images, gt_dir, outputs, results, num_covis, num_loc):\n    ref_sfm_sift = gt_dir / \"model_train\"\n    test_list = gt_dir / \"list_query.txt\"\n    outputs.mkdir(exist_ok=True, parents=True)\n    ref_sfm = outputs / \"sfm_superpoint+superglue\"\n    ref_sfm_scaled = outputs / \"sfm_sift_scaled\"\n    query_list = outputs / \"query_list_with_intrinsics.txt\"\n    sfm_pairs = outputs / f\"pairs-db-covis{num_covis}.txt\"\n    loc_pairs = outputs / f\"pairs-query-netvlad{num_loc}.txt\"\n    feature_conf = {",
        "detail": "hloc.pipelines.Cambridge.pipeline",
        "documentation": {}
    },
    {
        "label": "SCENES",
        "kind": 5,
        "importPath": "hloc.pipelines.Cambridge.pipeline",
        "description": "hloc.pipelines.Cambridge.pipeline",
        "peekOfCode": "SCENES = [\"KingsCollege\", \"OldHospital\", \"ShopFacade\", \"StMarysChurch\", \"GreatCourt\"]\ndef run_scene(images, gt_dir, outputs, results, num_covis, num_loc):\n    ref_sfm_sift = gt_dir / \"model_train\"\n    test_list = gt_dir / \"list_query.txt\"\n    outputs.mkdir(exist_ok=True, parents=True)\n    ref_sfm = outputs / \"sfm_superpoint+superglue\"\n    ref_sfm_scaled = outputs / \"sfm_sift_scaled\"\n    query_list = outputs / \"query_list_with_intrinsics.txt\"\n    sfm_pairs = outputs / f\"pairs-db-covis{num_covis}.txt\"\n    loc_pairs = outputs / f\"pairs-query-netvlad{num_loc}.txt\"",
        "detail": "hloc.pipelines.Cambridge.pipeline",
        "documentation": {}
    },
    {
        "label": "scale_sfm_images",
        "kind": 2,
        "importPath": "hloc.pipelines.Cambridge.utils",
        "description": "hloc.pipelines.Cambridge.utils",
        "peekOfCode": "def scale_sfm_images(full_model, scaled_model, image_dir):\n    \"\"\"Duplicate the provided model and scale the camera intrinsics so that\n    they match the original image resolution - makes everything easier.\n    \"\"\"\n    logger.info(\"Scaling the COLMAP model to the original image size.\")\n    scaled_model.mkdir(exist_ok=True)\n    cameras, images, points3D = read_model(full_model)\n    scaled_cameras = {}\n    for id_, image in images.items():\n        name = image.name",
        "detail": "hloc.pipelines.Cambridge.utils",
        "documentation": {}
    },
    {
        "label": "create_query_list_with_intrinsics",
        "kind": 2,
        "importPath": "hloc.pipelines.Cambridge.utils",
        "description": "hloc.pipelines.Cambridge.utils",
        "peekOfCode": "def create_query_list_with_intrinsics(\n    model, out, list_file=None, ext=\".bin\", image_dir=None\n):\n    \"\"\"Create a list of query images with intrinsics from the colmap model.\"\"\"\n    if ext == \".bin\":\n        images = read_images_binary(model / \"images.bin\")\n        cameras = read_cameras_binary(model / \"cameras.bin\")\n    else:\n        images = read_images_text(model / \"images.txt\")\n        cameras = read_cameras_text(model / \"cameras.txt\")",
        "detail": "hloc.pipelines.Cambridge.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "hloc.pipelines.Cambridge.utils",
        "description": "hloc.pipelines.Cambridge.utils",
        "peekOfCode": "def evaluate(model, results, list_file=None, ext=\".bin\", only_localized=False):\n    predictions = {}\n    with open(results, \"r\") as f:\n        for data in f.read().rstrip().split(\"\\n\"):\n            data = data.split()\n            name = data[0]\n            q, t = np.split(np.array(data[1:], float), [4])\n            predictions[name] = (qvec2rotmat(q), t)\n    if ext == \".bin\":\n        images = read_images_binary(model / \"images.bin\")",
        "detail": "hloc.pipelines.Cambridge.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.pipelines.Cambridge.utils",
        "description": "hloc.pipelines.Cambridge.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef scale_sfm_images(full_model, scaled_model, image_dir):\n    \"\"\"Duplicate the provided model and scale the camera intrinsics so that\n    they match the original image resolution - makes everything easier.\n    \"\"\"\n    logger.info(\"Scaling the COLMAP model to the original image size.\")\n    scaled_model.mkdir(exist_ok=True)\n    cameras, images, points3D = read_model(full_model)\n    scaled_cameras = {}\n    for id_, image in images.items():",
        "detail": "hloc.pipelines.Cambridge.utils",
        "documentation": {}
    },
    {
        "label": "read_nvm_model",
        "kind": 2,
        "importPath": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "description": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "peekOfCode": "def read_nvm_model(nvm_path, database_path, image_ids, camera_ids, skip_points=False):\n    # Extract the intrinsics from the db file instead of the NVM model\n    db = sqlite3.connect(str(database_path))\n    ret = db.execute(\"SELECT camera_id, model, width, height, params FROM cameras;\")\n    cameras = {}\n    for camera_id, camera_model, width, height, params in ret:\n        params = np.fromstring(params, dtype=np.double).reshape(-1)\n        camera_model = CAMERA_MODEL_IDS[camera_model]\n        assert len(params) == camera_model.num_params, (\n            len(params),",
        "detail": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "description": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "peekOfCode": "def main(nvm, database, output, skip_points=False):\n    assert nvm.exists(), nvm\n    assert database.exists(), database\n    image_ids, camera_ids = recover_database_images_and_ids(database)\n    logger.info(\"Reading the NVM model...\")\n    model = read_nvm_model(\n        nvm, database, image_ids, camera_ids, skip_points=skip_points\n    )\n    logger.info(\"Writing the COLMAP model...\")\n    output.mkdir(exist_ok=True, parents=True)",
        "detail": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "description": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef read_nvm_model(nvm_path, database_path, image_ids, camera_ids, skip_points=False):\n    # Extract the intrinsics from the db file instead of the NVM model\n    db = sqlite3.connect(str(database_path))\n    ret = db.execute(\"SELECT camera_id, model, width, height, params FROM cameras;\")\n    cameras = {}\n    for camera_id, camera_model, width, height, params in ret:\n        params = np.fromstring(params, dtype=np.double).reshape(-1)\n        camera_model = CAMERA_MODEL_IDS[camera_model]\n        assert len(params) == camera_model.num_params, (",
        "detail": "hloc.pipelines.RobotCar.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "generate_query_list",
        "kind": 2,
        "importPath": "hloc.pipelines.RobotCar.pipeline",
        "description": "hloc.pipelines.RobotCar.pipeline",
        "peekOfCode": "def generate_query_list(dataset, image_dir, path):\n    h, w = 1024, 1024\n    intrinsics_filename = \"intrinsics/{}_intrinsics.txt\"\n    cameras = {}\n    for side in [\"left\", \"right\", \"rear\"]:\n        with open(dataset / intrinsics_filename.format(side), \"r\") as f:\n            fx = f.readline().split()[1]\n            fy = f.readline().split()[1]\n            cx = f.readline().split()[1]\n            cy = f.readline().split()[1]",
        "detail": "hloc.pipelines.RobotCar.pipeline",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "hloc.pipelines.RobotCar.pipeline",
        "description": "hloc.pipelines.RobotCar.pipeline",
        "peekOfCode": "def run(args):\n    # Setup the paths\n    dataset = args.dataset\n    images = dataset / \"images/\"\n    outputs = args.outputs  # where everything will be saved\n    outputs.mkdir(exist_ok=True, parents=True)\n    query_list = outputs / \"{condition}_queries_with_intrinsics.txt\"\n    sift_sfm = outputs / \"sfm_sift\"\n    reference_sfm = outputs / \"sfm_superpoint+superglue\"\n    sfm_pairs = outputs / f\"pairs-db-covis{args.num_covis}.txt\"",
        "detail": "hloc.pipelines.RobotCar.pipeline",
        "documentation": {}
    },
    {
        "label": "CONDITIONS",
        "kind": 5,
        "importPath": "hloc.pipelines.RobotCar.pipeline",
        "description": "hloc.pipelines.RobotCar.pipeline",
        "peekOfCode": "CONDITIONS = [\n    \"dawn\",\n    \"dusk\",\n    \"night\",\n    \"night-rain\",\n    \"overcast-summer\",\n    \"overcast-winter\",\n    \"rain\",\n    \"snow\",\n    \"sun\",",
        "detail": "hloc.pipelines.RobotCar.pipeline",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "hloc.utils.base_model",
        "description": "hloc.utils.base_model",
        "peekOfCode": "class BaseModel(nn.Module, metaclass=ABCMeta):\n    default_conf = {}\n    required_inputs = []\n    def __init__(self, conf):\n        \"\"\"Perform some logic and call the _init method of the child model.\"\"\"\n        super().__init__()\n        self.conf = conf = {**self.default_conf, **conf}\n        self.required_inputs = copy(self.required_inputs)\n        self._init(conf)\n        sys.stdout.flush()",
        "detail": "hloc.utils.base_model",
        "documentation": {}
    },
    {
        "label": "dynamic_load",
        "kind": 2,
        "importPath": "hloc.utils.base_model",
        "description": "hloc.utils.base_model",
        "peekOfCode": "def dynamic_load(root, model):\n    module_path = f\"{root.__name__}.{model}\"\n    module = __import__(module_path, fromlist=[\"\"])\n    classes = inspect.getmembers(module, inspect.isclass)\n    # Filter classes defined in the module\n    classes = [c for c in classes if c[1].__module__ == module_path]\n    # Filter classes inherited from BaseModel\n    classes = [c for c in classes if issubclass(c[1], BaseModel)]\n    assert len(classes) == 1, classes\n    return classes[0][1]",
        "detail": "hloc.utils.base_model",
        "documentation": {}
    },
    {
        "label": "COLMAPDatabase",
        "kind": 6,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "class COLMAPDatabase(sqlite3.Connection):\n    @staticmethod\n    def connect(database_path):\n        return sqlite3.connect(str(database_path), factory=COLMAPDatabase)\n    def __init__(self, *args, **kwargs):\n        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n        self.create_tables = lambda: self.executescript(CREATE_ALL)\n        self.create_cameras_table = lambda: self.executescript(CREATE_CAMERAS_TABLE)\n        self.create_descriptors_table = lambda: self.executescript(\n            CREATE_DESCRIPTORS_TABLE",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "image_ids_to_pair_id",
        "kind": 2,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "def image_ids_to_pair_id(image_id1, image_id2):\n    if image_id1 > image_id2:\n        image_id1, image_id2 = image_id2, image_id1\n    return image_id1 * MAX_IMAGE_ID + image_id2\ndef pair_id_to_image_ids(pair_id):\n    image_id2 = pair_id % MAX_IMAGE_ID\n    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n    return image_id1, image_id2\ndef array_to_blob(array):\n    if IS_PYTHON3:",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "pair_id_to_image_ids",
        "kind": 2,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "def pair_id_to_image_ids(pair_id):\n    image_id2 = pair_id % MAX_IMAGE_ID\n    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n    return image_id1, image_id2\ndef array_to_blob(array):\n    if IS_PYTHON3:\n        return array.tobytes()\n    else:\n        return np.getbuffer(array)\ndef blob_to_array(blob, dtype, shape=(-1,)):",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "array_to_blob",
        "kind": 2,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "def array_to_blob(array):\n    if IS_PYTHON3:\n        return array.tobytes()\n    else:\n        return np.getbuffer(array)\ndef blob_to_array(blob, dtype, shape=(-1,)):\n    if IS_PYTHON3:\n        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n    else:\n        return np.frombuffer(blob, dtype=dtype).reshape(*shape)",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "blob_to_array",
        "kind": 2,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "def blob_to_array(blob, dtype, shape=(-1,)):\n    if IS_PYTHON3:\n        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n    else:\n        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\nclass COLMAPDatabase(sqlite3.Connection):\n    @staticmethod\n    def connect(database_path):\n        return sqlite3.connect(str(database_path), factory=COLMAPDatabase)\n    def __init__(self, *args, **kwargs):",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "IS_PYTHON3",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "IS_PYTHON3 = sys.version_info[0] >= 3\nMAX_IMAGE_ID = 2**31 - 1\nCREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    model INTEGER NOT NULL,\n    width INTEGER NOT NULL,\n    height INTEGER NOT NULL,\n    params BLOB,\n    prior_focal_length INTEGER NOT NULL)\"\"\"\nCREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "MAX_IMAGE_ID",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "MAX_IMAGE_ID = 2**31 - 1\nCREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    model INTEGER NOT NULL,\n    width INTEGER NOT NULL,\n    height INTEGER NOT NULL,\n    params BLOB,\n    prior_focal_length INTEGER NOT NULL)\"\"\"\nCREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n    image_id INTEGER PRIMARY KEY NOT NULL,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_CAMERAS_TABLE",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    model INTEGER NOT NULL,\n    width INTEGER NOT NULL,\n    height INTEGER NOT NULL,\n    params BLOB,\n    prior_focal_length INTEGER NOT NULL)\"\"\"\nCREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_DESCRIPTORS_TABLE",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\nCREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    name TEXT NOT NULL UNIQUE,\n    camera_id INTEGER NOT NULL,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_IMAGES_TABLE",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    name TEXT NOT NULL UNIQUE,\n    camera_id INTEGER NOT NULL,\n    prior_qw REAL,\n    prior_qx REAL,\n    prior_qy REAL,\n    prior_qz REAL,\n    prior_tx REAL,\n    prior_ty REAL,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_TWO_VIEW_GEOMETRIES_TABLE",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\nCREATE TABLE IF NOT EXISTS two_view_geometries (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    config INTEGER NOT NULL,\n    F BLOB,\n    E BLOB,\n    H BLOB,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_KEYPOINTS_TABLE",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n\"\"\"\nCREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_MATCHES_TABLE",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB)\"\"\"\nCREATE_NAME_INDEX = \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\nCREATE_ALL = \"; \".join(\n    [\n        CREATE_CAMERAS_TABLE,\n        CREATE_IMAGES_TABLE,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_NAME_INDEX",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_NAME_INDEX = \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\nCREATE_ALL = \"; \".join(\n    [\n        CREATE_CAMERAS_TABLE,\n        CREATE_IMAGES_TABLE,\n        CREATE_KEYPOINTS_TABLE,\n        CREATE_DESCRIPTORS_TABLE,\n        CREATE_MATCHES_TABLE,\n        CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n        CREATE_NAME_INDEX,",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "CREATE_ALL",
        "kind": 5,
        "importPath": "hloc.utils.database",
        "description": "hloc.utils.database",
        "peekOfCode": "CREATE_ALL = \"; \".join(\n    [\n        CREATE_CAMERAS_TABLE,\n        CREATE_IMAGES_TABLE,\n        CREATE_KEYPOINTS_TABLE,\n        CREATE_DESCRIPTORS_TABLE,\n        CREATE_MATCHES_TABLE,\n        CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n        CREATE_NAME_INDEX,\n    ]",
        "detail": "hloc.utils.database",
        "documentation": {}
    },
    {
        "label": "to_homogeneous",
        "kind": 2,
        "importPath": "hloc.utils.geometry",
        "description": "hloc.utils.geometry",
        "peekOfCode": "def to_homogeneous(p):\n    return np.pad(p, ((0, 0),) * (p.ndim - 1) + ((0, 1),), constant_values=1)\ndef compute_epipolar_errors(j_from_i: pycolmap.Rigid3d, p2d_i, p2d_j):\n    j_E_i = j_from_i.essential_matrix()\n    l2d_j = to_homogeneous(p2d_i) @ j_E_i.T\n    l2d_i = to_homogeneous(p2d_j) @ j_E_i\n    dist = np.abs(np.sum(to_homogeneous(p2d_i) * l2d_i, axis=1))\n    errors_i = dist / np.linalg.norm(l2d_i[:, :2], axis=1)\n    errors_j = dist / np.linalg.norm(l2d_j[:, :2], axis=1)\n    return errors_i, errors_j",
        "detail": "hloc.utils.geometry",
        "documentation": {}
    },
    {
        "label": "compute_epipolar_errors",
        "kind": 2,
        "importPath": "hloc.utils.geometry",
        "description": "hloc.utils.geometry",
        "peekOfCode": "def compute_epipolar_errors(j_from_i: pycolmap.Rigid3d, p2d_i, p2d_j):\n    j_E_i = j_from_i.essential_matrix()\n    l2d_j = to_homogeneous(p2d_i) @ j_E_i.T\n    l2d_i = to_homogeneous(p2d_j) @ j_E_i\n    dist = np.abs(np.sum(to_homogeneous(p2d_i) * l2d_i, axis=1))\n    errors_i = dist / np.linalg.norm(l2d_i[:, :2], axis=1)\n    errors_j = dist / np.linalg.norm(l2d_j[:, :2], axis=1)\n    return errors_i, errors_j",
        "detail": "hloc.utils.geometry",
        "documentation": {}
    },
    {
        "label": "read_image",
        "kind": 2,
        "importPath": "hloc.utils.io",
        "description": "hloc.utils.io",
        "peekOfCode": "def read_image(path, grayscale=False):\n    if grayscale:\n        mode = cv2.IMREAD_GRAYSCALE\n    else:\n        mode = cv2.IMREAD_COLOR\n    image = cv2.imread(str(path), mode | cv2.IMREAD_IGNORE_ORIENTATION)\n    if image is None:\n        raise ValueError(f\"Cannot read image {path}.\")\n    if not grayscale and len(image.shape) == 3:\n        image = image[:, :, ::-1]  # BGR to RGB",
        "detail": "hloc.utils.io",
        "documentation": {}
    },
    {
        "label": "list_h5_names",
        "kind": 2,
        "importPath": "hloc.utils.io",
        "description": "hloc.utils.io",
        "peekOfCode": "def list_h5_names(path):\n    names = []\n    with h5py.File(str(path), \"r\", libver=\"latest\") as fd:\n        def visit_fn(_, obj):\n            if isinstance(obj, h5py.Dataset):\n                names.append(obj.parent.name.strip(\"/\"))\n        fd.visititems(visit_fn)\n    return list(set(names))\ndef get_keypoints(\n    path: Path, name: str, return_uncertainty: bool = False",
        "detail": "hloc.utils.io",
        "documentation": {}
    },
    {
        "label": "get_keypoints",
        "kind": 2,
        "importPath": "hloc.utils.io",
        "description": "hloc.utils.io",
        "peekOfCode": "def get_keypoints(\n    path: Path, name: str, return_uncertainty: bool = False\n) -> np.ndarray:\n    with h5py.File(str(path), \"r\", libver=\"latest\") as hfile:\n        dset = hfile[name][\"keypoints\"]\n        p = dset.__array__()\n        uncertainty = dset.attrs.get(\"uncertainty\")\n    if return_uncertainty:\n        return p, uncertainty\n    return p",
        "detail": "hloc.utils.io",
        "documentation": {}
    },
    {
        "label": "find_pair",
        "kind": 2,
        "importPath": "hloc.utils.io",
        "description": "hloc.utils.io",
        "peekOfCode": "def find_pair(hfile: h5py.File, name0: str, name1: str):\n    pair = names_to_pair(name0, name1)\n    if pair in hfile:\n        return pair, False\n    pair = names_to_pair(name1, name0)\n    if pair in hfile:\n        return pair, True\n    # older, less efficient format\n    pair = names_to_pair_old(name0, name1)\n    if pair in hfile:",
        "detail": "hloc.utils.io",
        "documentation": {}
    },
    {
        "label": "get_matches",
        "kind": 2,
        "importPath": "hloc.utils.io",
        "description": "hloc.utils.io",
        "peekOfCode": "def get_matches(path: Path, name0: str, name1: str) -> Tuple[np.ndarray]:\n    with h5py.File(str(path), \"r\", libver=\"latest\") as hfile:\n        pair, reverse = find_pair(hfile, name0, name1)\n        matches = hfile[pair][\"matches0\"].__array__()\n        scores = hfile[pair][\"matching_scores0\"].__array__()\n    idx = np.where(matches != -1)[0]\n    matches = np.stack([idx, matches[idx]], -1)\n    if reverse:\n        matches = np.flip(matches, -1)\n    scores = scores[idx]",
        "detail": "hloc.utils.io",
        "documentation": {}
    },
    {
        "label": "parse_image_list",
        "kind": 2,
        "importPath": "hloc.utils.parsers",
        "description": "hloc.utils.parsers",
        "peekOfCode": "def parse_image_list(path, with_intrinsics=False):\n    images = []\n    with open(path, \"r\") as f:\n        for line in f:\n            line = line.strip(\"\\n\")\n            if len(line) == 0 or line[0] == \"#\":\n                continue\n            name, *data = line.split()\n            if with_intrinsics:\n                model, width, height, *params = data",
        "detail": "hloc.utils.parsers",
        "documentation": {}
    },
    {
        "label": "parse_image_lists",
        "kind": 2,
        "importPath": "hloc.utils.parsers",
        "description": "hloc.utils.parsers",
        "peekOfCode": "def parse_image_lists(paths, with_intrinsics=False):\n    images = []\n    files = list(Path(paths.parent).glob(paths.name))\n    assert len(files) > 0\n    for lfile in files:\n        images += parse_image_list(lfile, with_intrinsics=with_intrinsics)\n    return images\ndef parse_retrieval(path):\n    retrieval = defaultdict(list)\n    with open(path, \"r\") as f:",
        "detail": "hloc.utils.parsers",
        "documentation": {}
    },
    {
        "label": "parse_retrieval",
        "kind": 2,
        "importPath": "hloc.utils.parsers",
        "description": "hloc.utils.parsers",
        "peekOfCode": "def parse_retrieval(path):\n    retrieval = defaultdict(list)\n    with open(path, \"r\") as f:\n        for p in f.read().rstrip(\"\\n\").split(\"\\n\"):\n            if len(p) == 0:\n                continue\n            q, r = p.split()\n            retrieval[q].append(r)\n    return dict(retrieval)\ndef names_to_pair(name0, name1, separator=\"/\"):",
        "detail": "hloc.utils.parsers",
        "documentation": {}
    },
    {
        "label": "names_to_pair",
        "kind": 2,
        "importPath": "hloc.utils.parsers",
        "description": "hloc.utils.parsers",
        "peekOfCode": "def names_to_pair(name0, name1, separator=\"/\"):\n    return separator.join((name0.replace(\"/\", \"-\"), name1.replace(\"/\", \"-\")))\ndef names_to_pair_old(name0, name1):\n    return names_to_pair(name0, name1, separator=\"_\")",
        "detail": "hloc.utils.parsers",
        "documentation": {}
    },
    {
        "label": "names_to_pair_old",
        "kind": 2,
        "importPath": "hloc.utils.parsers",
        "description": "hloc.utils.parsers",
        "peekOfCode": "def names_to_pair_old(name0, name1):\n    return names_to_pair(name0, name1, separator=\"_\")",
        "detail": "hloc.utils.parsers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.utils.parsers",
        "description": "hloc.utils.parsers",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef parse_image_list(path, with_intrinsics=False):\n    images = []\n    with open(path, \"r\") as f:\n        for line in f:\n            line = line.strip(\"\\n\")\n            if len(line) == 0 or line[0] == \"#\":\n                continue\n            name, *data = line.split()\n            if with_intrinsics:",
        "detail": "hloc.utils.parsers",
        "documentation": {}
    },
    {
        "label": "Image",
        "kind": 6,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "class Image(BaseImage):\n    def qvec2rotmat(self):\n        return qvec2rotmat(self.qvec)\nCAMERA_MODELS = {\n    CameraModel(model_id=0, model_name=\"SIMPLE_PINHOLE\", num_params=3),\n    CameraModel(model_id=1, model_name=\"PINHOLE\", num_params=4),\n    CameraModel(model_id=2, model_name=\"SIMPLE_RADIAL\", num_params=4),\n    CameraModel(model_id=3, model_name=\"RADIAL\", num_params=5),\n    CameraModel(model_id=4, model_name=\"OPENCV\", num_params=8),\n    CameraModel(model_id=5, model_name=\"OPENCV_FISHEYE\", num_params=8),",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_next_bytes",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_next_bytes(fid, num_bytes, format_char_sequence, endian_character=\"<\"):\n    \"\"\"Read and unpack the next bytes from a binary file.\n    :param fid:\n    :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.\n    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n    :param endian_character: Any of {@, =, <, >, !}\n    :return: Tuple of read and unpacked values.\n    \"\"\"\n    data = fid.read(num_bytes)\n    return struct.unpack(endian_character + format_char_sequence, data)",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_next_bytes",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_next_bytes(fid, data, format_char_sequence, endian_character=\"<\"):\n    \"\"\"pack and write to a binary file.\n    :param fid:\n    :param data: data to send, if multiple elements are sent at the same time,\n    they should be encapsuled either in a list or a tuple\n    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n    should be the same length as the data list or tuple\n    :param endian_character: Any of {@, =, <, >, !}\n    \"\"\"\n    if isinstance(data, (list, tuple)):",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_cameras_text",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_cameras_text(path):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::WriteCamerasText(const std::string& path)\n        void Reconstruction::ReadCamerasText(const std::string& path)\n    \"\"\"\n    cameras = {}\n    with open(path, \"r\") as fid:\n        while True:\n            line = fid.readline()",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_cameras_binary",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_cameras_binary(path_to_model_file):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::WriteCamerasBinary(const std::string& path)\n        void Reconstruction::ReadCamerasBinary(const std::string& path)\n    \"\"\"\n    cameras = {}\n    with open(path_to_model_file, \"rb\") as fid:\n        num_cameras = read_next_bytes(fid, 8, \"Q\")[0]\n        for _ in range(num_cameras):",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_cameras_text",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_cameras_text(cameras, path):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::WriteCamerasText(const std::string& path)\n        void Reconstruction::ReadCamerasText(const std::string& path)\n    \"\"\"\n    HEADER = (\n        \"# Camera list with one line of data per camera:\\n\"\n        + \"#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\\n\"\n        + \"# Number of cameras: {}\\n\".format(len(cameras))",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_cameras_binary",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_cameras_binary(cameras, path_to_model_file):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::WriteCamerasBinary(const std::string& path)\n        void Reconstruction::ReadCamerasBinary(const std::string& path)\n    \"\"\"\n    with open(path_to_model_file, \"wb\") as fid:\n        write_next_bytes(fid, len(cameras), \"Q\")\n        for _, cam in cameras.items():\n            model_id = CAMERA_MODEL_NAMES[cam.model].model_id",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_images_text",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_images_text(path):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadImagesText(const std::string& path)\n        void Reconstruction::WriteImagesText(const std::string& path)\n    \"\"\"\n    images = {}\n    with open(path, \"r\") as fid:\n        while True:\n            line = fid.readline()",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_images_binary",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_images_binary(path_to_model_file):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadImagesBinary(const std::string& path)\n        void Reconstruction::WriteImagesBinary(const std::string& path)\n    \"\"\"\n    images = {}\n    with open(path_to_model_file, \"rb\") as fid:\n        num_reg_images = read_next_bytes(fid, 8, \"Q\")[0]\n        for _ in range(num_reg_images):",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_images_text",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_images_text(images, path):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadImagesText(const std::string& path)\n        void Reconstruction::WriteImagesText(const std::string& path)\n    \"\"\"\n    if len(images) == 0:\n        mean_observations = 0\n    else:\n        mean_observations = sum(",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_images_binary",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_images_binary(images, path_to_model_file):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadImagesBinary(const std::string& path)\n        void Reconstruction::WriteImagesBinary(const std::string& path)\n    \"\"\"\n    with open(path_to_model_file, \"wb\") as fid:\n        write_next_bytes(fid, len(images), \"Q\")\n        for _, img in images.items():\n            write_next_bytes(fid, img.id, \"i\")",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_points3D_text",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_points3D_text(path):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadPoints3DText(const std::string& path)\n        void Reconstruction::WritePoints3DText(const std::string& path)\n    \"\"\"\n    points3D = {}\n    with open(path, \"r\") as fid:\n        while True:\n            line = fid.readline()",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_points3D_binary",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_points3D_binary(path_to_model_file):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadPoints3DBinary(const std::string& path)\n        void Reconstruction::WritePoints3DBinary(const std::string& path)\n    \"\"\"\n    points3D = {}\n    with open(path_to_model_file, \"rb\") as fid:\n        num_points = read_next_bytes(fid, 8, \"Q\")[0]\n        for _ in range(num_points):",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_points3D_text",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_points3D_text(points3D, path):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadPoints3DText(const std::string& path)\n        void Reconstruction::WritePoints3DText(const std::string& path)\n    \"\"\"\n    if len(points3D) == 0:\n        mean_track_length = 0\n    else:\n        mean_track_length = sum(",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_points3D_binary",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_points3D_binary(points3D, path_to_model_file):\n    \"\"\"\n    see: src/base/reconstruction.cc\n        void Reconstruction::ReadPoints3DBinary(const std::string& path)\n        void Reconstruction::WritePoints3DBinary(const std::string& path)\n    \"\"\"\n    with open(path_to_model_file, \"wb\") as fid:\n        write_next_bytes(fid, len(points3D), \"Q\")\n        for _, pt in points3D.items():\n            write_next_bytes(fid, pt.id, \"Q\")",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "detect_model_format",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def detect_model_format(path, ext):\n    if (\n        os.path.isfile(os.path.join(path, \"cameras\" + ext))\n        and os.path.isfile(os.path.join(path, \"images\" + ext))\n        and os.path.isfile(os.path.join(path, \"points3D\" + ext))\n    ):\n        return True\n    return False\ndef read_model(path, ext=\"\"):\n    # try to detect the extension automatically",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "read_model",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def read_model(path, ext=\"\"):\n    # try to detect the extension automatically\n    if ext == \"\":\n        if detect_model_format(path, \".bin\"):\n            ext = \".bin\"\n        elif detect_model_format(path, \".txt\"):\n            ext = \".txt\"\n        else:\n            try:\n                cameras, images, points3D = read_model(os.path.join(path, \"model/\"))",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "write_model",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def write_model(cameras, images, points3D, path, ext=\".bin\"):\n    if ext == \".txt\":\n        write_cameras_text(cameras, os.path.join(path, \"cameras\" + ext))\n        write_images_text(images, os.path.join(path, \"images\" + ext))\n        write_points3D_text(points3D, os.path.join(path, \"points3D\") + ext)\n    else:\n        write_cameras_binary(cameras, os.path.join(path, \"cameras\" + ext))\n        write_images_binary(images, os.path.join(path, \"images\" + ext))\n        write_points3D_binary(points3D, os.path.join(path, \"points3D\") + ext)\n    return cameras, images, points3D",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "qvec2rotmat",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def qvec2rotmat(qvec):\n    return np.array(\n        [\n            [\n                1 - 2 * qvec[2] ** 2 - 2 * qvec[3] ** 2,\n                2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n                2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2],\n            ],\n            [\n                2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "rotmat2qvec",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def rotmat2qvec(R):\n    Rxx, Ryx, Rzx, Rxy, Ryy, Rzy, Rxz, Ryz, Rzz = R.flat\n    K = (\n        np.array(\n            [\n                [Rxx - Ryy - Rzz, 0, 0, 0],\n                [Ryx + Rxy, Ryy - Rxx - Rzz, 0, 0],\n                [Rzx + Rxz, Rzy + Ryz, Rzz - Rxx - Ryy, 0],\n                [Ryz - Rzy, Rzx - Rxz, Rxy - Ryx, Rxx + Ryy + Rzz],\n            ]",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Read and write COLMAP binary and text models\"\n    )\n    parser.add_argument(\"--input_model\", help=\"path to input model folder\")\n    parser.add_argument(\n        \"--input_format\",\n        choices=[\".bin\", \".txt\"],\n        help=\"input model format\",\n        default=\"\",",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "logger = logging.getLogger(__name__)\nCameraModel = collections.namedtuple(\n    \"CameraModel\", [\"model_id\", \"model_name\", \"num_params\"]\n)\nCamera = collections.namedtuple(\"Camera\", [\"id\", \"model\", \"width\", \"height\", \"params\"])\nBaseImage = collections.namedtuple(\n    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"]\n)\nPoint3D = collections.namedtuple(\n    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"]",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "CameraModel",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "CameraModel = collections.namedtuple(\n    \"CameraModel\", [\"model_id\", \"model_name\", \"num_params\"]\n)\nCamera = collections.namedtuple(\"Camera\", [\"id\", \"model\", \"width\", \"height\", \"params\"])\nBaseImage = collections.namedtuple(\n    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"]\n)\nPoint3D = collections.namedtuple(\n    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"]\n)",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "Camera",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "Camera = collections.namedtuple(\"Camera\", [\"id\", \"model\", \"width\", \"height\", \"params\"])\nBaseImage = collections.namedtuple(\n    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"]\n)\nPoint3D = collections.namedtuple(\n    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"]\n)\nclass Image(BaseImage):\n    def qvec2rotmat(self):\n        return qvec2rotmat(self.qvec)",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "BaseImage",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "BaseImage = collections.namedtuple(\n    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"]\n)\nPoint3D = collections.namedtuple(\n    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"]\n)\nclass Image(BaseImage):\n    def qvec2rotmat(self):\n        return qvec2rotmat(self.qvec)\nCAMERA_MODELS = {",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "Point3D",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "Point3D = collections.namedtuple(\n    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"]\n)\nclass Image(BaseImage):\n    def qvec2rotmat(self):\n        return qvec2rotmat(self.qvec)\nCAMERA_MODELS = {\n    CameraModel(model_id=0, model_name=\"SIMPLE_PINHOLE\", num_params=3),\n    CameraModel(model_id=1, model_name=\"PINHOLE\", num_params=4),\n    CameraModel(model_id=2, model_name=\"SIMPLE_RADIAL\", num_params=4),",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "CAMERA_MODELS",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "CAMERA_MODELS = {\n    CameraModel(model_id=0, model_name=\"SIMPLE_PINHOLE\", num_params=3),\n    CameraModel(model_id=1, model_name=\"PINHOLE\", num_params=4),\n    CameraModel(model_id=2, model_name=\"SIMPLE_RADIAL\", num_params=4),\n    CameraModel(model_id=3, model_name=\"RADIAL\", num_params=5),\n    CameraModel(model_id=4, model_name=\"OPENCV\", num_params=8),\n    CameraModel(model_id=5, model_name=\"OPENCV_FISHEYE\", num_params=8),\n    CameraModel(model_id=6, model_name=\"FULL_OPENCV\", num_params=12),\n    CameraModel(model_id=7, model_name=\"FOV\", num_params=5),\n    CameraModel(model_id=8, model_name=\"SIMPLE_RADIAL_FISHEYE\", num_params=4),",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "CAMERA_MODEL_IDS",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "CAMERA_MODEL_IDS = dict(\n    [(camera_model.model_id, camera_model) for camera_model in CAMERA_MODELS]\n)\nCAMERA_MODEL_NAMES = dict(\n    [(camera_model.model_name, camera_model) for camera_model in CAMERA_MODELS]\n)\ndef read_next_bytes(fid, num_bytes, format_char_sequence, endian_character=\"<\"):\n    \"\"\"Read and unpack the next bytes from a binary file.\n    :param fid:\n    :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "CAMERA_MODEL_NAMES",
        "kind": 5,
        "importPath": "hloc.utils.read_write_model",
        "description": "hloc.utils.read_write_model",
        "peekOfCode": "CAMERA_MODEL_NAMES = dict(\n    [(camera_model.model_name, camera_model) for camera_model in CAMERA_MODELS]\n)\ndef read_next_bytes(fid, num_bytes, format_char_sequence, endian_character=\"<\"):\n    \"\"\"Read and unpack the next bytes from a binary file.\n    :param fid:\n    :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.\n    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n    :param endian_character: Any of {@, =, <, >, !}\n    :return: Tuple of read and unpacked values.",
        "detail": "hloc.utils.read_write_model",
        "documentation": {}
    },
    {
        "label": "cm_RdGn",
        "kind": 2,
        "importPath": "hloc.utils.viz",
        "description": "hloc.utils.viz",
        "peekOfCode": "def cm_RdGn(x):\n    \"\"\"Custom colormap: red (0) -> yellow (0.5) -> green (1).\"\"\"\n    x = np.clip(x, 0, 1)[..., None] * 2\n    c = x * np.array([[0, 1.0, 0]]) + (2 - x) * np.array([[1.0, 0, 0]])\n    return np.clip(c, 0, 1)\ndef plot_images(\n    imgs, titles=None, cmaps=\"gray\", dpi=100, pad=0.5, adaptive=True, figsize=4.5\n):\n    \"\"\"Plot a set of images horizontally.\n    Args:",
        "detail": "hloc.utils.viz",
        "documentation": {}
    },
    {
        "label": "plot_images",
        "kind": 2,
        "importPath": "hloc.utils.viz",
        "description": "hloc.utils.viz",
        "peekOfCode": "def plot_images(\n    imgs, titles=None, cmaps=\"gray\", dpi=100, pad=0.5, adaptive=True, figsize=4.5\n):\n    \"\"\"Plot a set of images horizontally.\n    Args:\n        imgs: a list of NumPy or PyTorch images, RGB (H, W, 3) or mono (H, W).\n        titles: a list of strings, as titles for each image.\n        cmaps: colormaps for monochrome images.\n        adaptive: whether the figure size should fit the image aspect ratios.\n    \"\"\"",
        "detail": "hloc.utils.viz",
        "documentation": {}
    },
    {
        "label": "plot_keypoints",
        "kind": 2,
        "importPath": "hloc.utils.viz",
        "description": "hloc.utils.viz",
        "peekOfCode": "def plot_keypoints(kpts, colors=\"lime\", ps=4):\n    \"\"\"Plot keypoints for existing images.\n    Args:\n        kpts: list of ndarrays of size (N, 2).\n        colors: string, or list of list of tuples (one for each keypoints).\n        ps: size of the keypoints as float.\n    \"\"\"\n    if not isinstance(colors, list):\n        colors = [colors] * len(kpts)\n    axes = plt.gcf().axes",
        "detail": "hloc.utils.viz",
        "documentation": {}
    },
    {
        "label": "plot_matches",
        "kind": 2,
        "importPath": "hloc.utils.viz",
        "description": "hloc.utils.viz",
        "peekOfCode": "def plot_matches(kpts0, kpts1, color=None, lw=1.5, ps=4, indices=(0, 1), a=1.0):\n    \"\"\"Plot matches for a pair of existing images.\n    Args:\n        kpts0, kpts1: corresponding keypoints of size (N, 2).\n        color: color of each match, string or RGB tuple. Random if not given.\n        lw: width of the lines.\n        ps: size of the end points (no endpoint if ps=0)\n        indices: indices of the images to draw the matches on.\n        a: alpha opacity of the match lines.\n    \"\"\"",
        "detail": "hloc.utils.viz",
        "documentation": {}
    },
    {
        "label": "add_text",
        "kind": 2,
        "importPath": "hloc.utils.viz",
        "description": "hloc.utils.viz",
        "peekOfCode": "def add_text(\n    idx,\n    text,\n    pos=(0.01, 0.99),\n    fs=15,\n    color=\"w\",\n    lcolor=\"k\",\n    lwidth=2,\n    ha=\"left\",\n    va=\"top\",",
        "detail": "hloc.utils.viz",
        "documentation": {}
    },
    {
        "label": "save_plot",
        "kind": 2,
        "importPath": "hloc.utils.viz",
        "description": "hloc.utils.viz",
        "peekOfCode": "def save_plot(path, **kw):\n    \"\"\"Save the current figure without any white margin.\"\"\"\n    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0, **kw)",
        "detail": "hloc.utils.viz",
        "documentation": {}
    },
    {
        "label": "to_homogeneous",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def to_homogeneous(points):\n    pad = np.ones((points.shape[:-1] + (1,)), dtype=points.dtype)\n    return np.concatenate([points, pad], axis=-1)\ndef init_figure(height: int = 800) -> go.Figure:\n    \"\"\"Initialize a 3D figure.\"\"\"\n    fig = go.Figure()\n    axes = dict(\n        visible=False,\n        showbackground=False,\n        showgrid=False,",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "init_figure",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def init_figure(height: int = 800) -> go.Figure:\n    \"\"\"Initialize a 3D figure.\"\"\"\n    fig = go.Figure()\n    axes = dict(\n        visible=False,\n        showbackground=False,\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n        autorange=True,",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "plot_points",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def plot_points(\n    fig: go.Figure,\n    pts: np.ndarray,\n    color: str = \"rgba(255, 0, 0, 1)\",\n    ps: int = 2,\n    colorscale: Optional[str] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Plot a set of 3D points.\"\"\"\n    x, y, z = pts.T",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "plot_camera",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def plot_camera(\n    fig: go.Figure,\n    R: np.ndarray,\n    t: np.ndarray,\n    K: np.ndarray,\n    color: str = \"rgb(0, 0, 255)\",\n    name: Optional[str] = None,\n    legendgroup: Optional[str] = None,\n    fill: bool = False,\n    size: float = 1.0,",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "plot_camera_colmap",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def plot_camera_colmap(\n    fig: go.Figure,\n    image: pycolmap.Image,\n    camera: pycolmap.Camera,\n    name: Optional[str] = None,\n    **kwargs\n):\n    \"\"\"Plot a camera frustum from PyCOLMAP objects\"\"\"\n    world_t_camera = image.cam_from_world.inverse()\n    plot_camera(",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "plot_cameras",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def plot_cameras(fig: go.Figure, reconstruction: pycolmap.Reconstruction, **kwargs):\n    \"\"\"Plot a camera as a cone with camera frustum.\"\"\"\n    for image_id, image in reconstruction.images.items():\n        plot_camera_colmap(\n            fig, image, reconstruction.cameras[image.camera_id], **kwargs\n        )\ndef plot_reconstruction(\n    fig: go.Figure,\n    rec: pycolmap.Reconstruction,\n    max_reproj_error: float = 6.0,",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "plot_reconstruction",
        "kind": 2,
        "importPath": "hloc.utils.viz_3d",
        "description": "hloc.utils.viz_3d",
        "peekOfCode": "def plot_reconstruction(\n    fig: go.Figure,\n    rec: pycolmap.Reconstruction,\n    max_reproj_error: float = 6.0,\n    color: str = \"rgb(0, 0, 255)\",\n    name: Optional[str] = None,\n    min_track_length: int = 2,\n    points: bool = True,\n    cameras: bool = True,\n    points_rgb: bool = True,",
        "detail": "hloc.utils.viz_3d",
        "documentation": {}
    },
    {
        "label": "recover_database_images_and_ids",
        "kind": 2,
        "importPath": "hloc.colmap_from_nvm",
        "description": "hloc.colmap_from_nvm",
        "peekOfCode": "def recover_database_images_and_ids(database_path):\n    images = {}\n    cameras = {}\n    db = sqlite3.connect(str(database_path))\n    ret = db.execute(\"SELECT name, image_id, camera_id FROM images;\")\n    for name, image_id, camera_id in ret:\n        images[name] = image_id\n        cameras[name] = camera_id\n    db.close()\n    logger.info(f\"Found {len(images)} images and {len(cameras)} cameras in database.\")",
        "detail": "hloc.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "quaternion_to_rotation_matrix",
        "kind": 2,
        "importPath": "hloc.colmap_from_nvm",
        "description": "hloc.colmap_from_nvm",
        "peekOfCode": "def quaternion_to_rotation_matrix(qvec):\n    qvec = qvec / np.linalg.norm(qvec)\n    w, x, y, z = qvec\n    R = np.array(\n        [\n            [1 - 2 * y * y - 2 * z * z, 2 * x * y - 2 * z * w, 2 * x * z + 2 * y * w],\n            [2 * x * y + 2 * z * w, 1 - 2 * x * x - 2 * z * z, 2 * y * z - 2 * x * w],\n            [2 * x * z - 2 * y * w, 2 * y * z + 2 * x * w, 1 - 2 * x * x - 2 * y * y],\n        ]\n    )",
        "detail": "hloc.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "camera_center_to_translation",
        "kind": 2,
        "importPath": "hloc.colmap_from_nvm",
        "description": "hloc.colmap_from_nvm",
        "peekOfCode": "def camera_center_to_translation(c, qvec):\n    R = quaternion_to_rotation_matrix(qvec)\n    return (-1) * np.matmul(R, c)\ndef read_nvm_model(nvm_path, intrinsics_path, image_ids, camera_ids, skip_points=False):\n    with open(intrinsics_path, \"r\") as f:\n        raw_intrinsics = f.readlines()\n    logger.info(f\"Reading {len(raw_intrinsics)} cameras...\")\n    cameras = {}\n    for intrinsics in raw_intrinsics:\n        intrinsics = intrinsics.strip(\"\\n\").split(\" \")",
        "detail": "hloc.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "read_nvm_model",
        "kind": 2,
        "importPath": "hloc.colmap_from_nvm",
        "description": "hloc.colmap_from_nvm",
        "peekOfCode": "def read_nvm_model(nvm_path, intrinsics_path, image_ids, camera_ids, skip_points=False):\n    with open(intrinsics_path, \"r\") as f:\n        raw_intrinsics = f.readlines()\n    logger.info(f\"Reading {len(raw_intrinsics)} cameras...\")\n    cameras = {}\n    for intrinsics in raw_intrinsics:\n        intrinsics = intrinsics.strip(\"\\n\").split(\" \")\n        name, camera_model, width, height = intrinsics[:4]\n        params = [float(p) for p in intrinsics[4:]]\n        camera_model = CAMERA_MODEL_NAMES[camera_model]",
        "detail": "hloc.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.colmap_from_nvm",
        "description": "hloc.colmap_from_nvm",
        "peekOfCode": "def main(nvm, intrinsics, database, output, skip_points=False):\n    assert nvm.exists(), nvm\n    assert intrinsics.exists(), intrinsics\n    assert database.exists(), database\n    image_ids, camera_ids = recover_database_images_and_ids(database)\n    logger.info(\"Reading the NVM model...\")\n    model = read_nvm_model(\n        nvm, intrinsics, image_ids, camera_ids, skip_points=skip_points\n    )\n    logger.info(\"Writing the COLMAP model...\")",
        "detail": "hloc.colmap_from_nvm",
        "documentation": {}
    },
    {
        "label": "ImageDataset",
        "kind": 6,
        "importPath": "hloc.extract_features",
        "description": "hloc.extract_features",
        "peekOfCode": "class ImageDataset(torch.utils.data.Dataset):\n    default_conf = {\n        \"globs\": [\"*.jpg\", \"*.png\", \"*.jpeg\", \"*.JPG\", \"*.PNG\"],\n        \"grayscale\": False,\n        \"resize_max\": None,\n        \"resize_force\": False,\n        \"interpolation\": \"cv2_area\",  # pil_linear is more accurate but slower\n    }\n    def __init__(self, root, conf, paths=None):\n        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf})",
        "detail": "hloc.extract_features",
        "documentation": {}
    },
    {
        "label": "resize_image",
        "kind": 2,
        "importPath": "hloc.extract_features",
        "description": "hloc.extract_features",
        "peekOfCode": "def resize_image(image, size, interp):\n    if interp.startswith(\"cv2_\"):\n        interp = getattr(cv2, \"INTER_\" + interp[len(\"cv2_\") :].upper())\n        h, w = image.shape[:2]\n        if interp == cv2.INTER_AREA and (w < size[0] or h < size[1]):\n            interp = cv2.INTER_LINEAR\n        resized = cv2.resize(image, size, interpolation=interp)\n    elif interp.startswith(\"pil_\"):\n        interp = getattr(PIL.Image, interp[len(\"pil_\") :].upper())\n        resized = PIL.Image.fromarray(image.astype(np.uint8))",
        "detail": "hloc.extract_features",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.extract_features",
        "description": "hloc.extract_features",
        "peekOfCode": "def main(\n    conf: Dict,\n    image_dir: Path,\n    export_dir: Optional[Path] = None,\n    as_half: bool = True,\n    image_list: Optional[Union[Path, List[str]]] = None,\n    feature_path: Optional[Path] = None,\n    overwrite: bool = False,\n) -> Path:\n    logger.info(",
        "detail": "hloc.extract_features",
        "documentation": {}
    },
    {
        "label": "confs",
        "kind": 5,
        "importPath": "hloc.extract_features",
        "description": "hloc.extract_features",
        "peekOfCode": "confs = {\n    \"superpoint_aachen\": {\n        \"output\": \"feats-superpoint-n4096-r1024\",\n        \"model\": {\n            \"name\": \"superpoint\",\n            \"nms_radius\": 3,\n            \"max_keypoints\": 4096,\n        },\n        \"preprocessing\": {\n            \"grayscale\": True,",
        "detail": "hloc.extract_features",
        "documentation": {}
    },
    {
        "label": "interpolate_scan",
        "kind": 2,
        "importPath": "hloc.localize_inloc",
        "description": "hloc.localize_inloc",
        "peekOfCode": "def interpolate_scan(scan, kp):\n    h, w, c = scan.shape\n    kp = kp / np.array([[w - 1, h - 1]]) * 2 - 1\n    assert np.all(kp > -1) and np.all(kp < 1)\n    scan = torch.from_numpy(scan).permute(2, 0, 1)[None]\n    kp = torch.from_numpy(kp)[None, None]\n    grid_sample = torch.nn.functional.grid_sample\n    # To maximize the number of points that have depth:\n    # do bilinear interpolation first and then nearest for the remaining points\n    interp_lin = grid_sample(scan, kp, align_corners=True, mode=\"bilinear\")[0, :, 0]",
        "detail": "hloc.localize_inloc",
        "documentation": {}
    },
    {
        "label": "get_scan_pose",
        "kind": 2,
        "importPath": "hloc.localize_inloc",
        "description": "hloc.localize_inloc",
        "peekOfCode": "def get_scan_pose(dataset_dir, rpath):\n    split_image_rpath = rpath.split(\"/\")\n    floor_name = split_image_rpath[-3]\n    scan_id = split_image_rpath[-2]\n    image_name = split_image_rpath[-1]\n    building_name = image_name[:3]\n    path = Path(\n        dataset_dir,\n        \"database/alignments\",\n        floor_name,",
        "detail": "hloc.localize_inloc",
        "documentation": {}
    },
    {
        "label": "pose_from_cluster",
        "kind": 2,
        "importPath": "hloc.localize_inloc",
        "description": "hloc.localize_inloc",
        "peekOfCode": "def pose_from_cluster(dataset_dir, q, retrieved, feature_file, match_file, skip=None):\n    height, width = cv2.imread(str(dataset_dir / q)).shape[:2]\n    cx = 0.5 * width\n    cy = 0.5 * height\n    focal_length = 4032.0 * 28.0 / 36.0\n    all_mkpq = []\n    all_mkpr = []\n    all_mkp3d = []\n    all_indices = []\n    kpq = feature_file[q][\"keypoints\"].__array__()",
        "detail": "hloc.localize_inloc",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.localize_inloc",
        "description": "hloc.localize_inloc",
        "peekOfCode": "def main(dataset_dir, retrieval, features, matches, results, skip_matches=None):\n    assert retrieval.exists(), retrieval\n    assert features.exists(), features\n    assert matches.exists(), matches\n    retrieval_dict = parse_retrieval(retrieval)\n    queries = list(retrieval_dict.keys())\n    feature_file = h5py.File(features, \"r\", libver=\"latest\")\n    match_file = h5py.File(matches, \"r\", libver=\"latest\")\n    poses = {}\n    logs = {",
        "detail": "hloc.localize_inloc",
        "documentation": {}
    },
    {
        "label": "QueryLocalizer",
        "kind": 6,
        "importPath": "hloc.localize_sfm",
        "description": "hloc.localize_sfm",
        "peekOfCode": "class QueryLocalizer:\n    def __init__(self, reconstruction, config=None):\n        self.reconstruction = reconstruction\n        self.config = config or {}\n    def localize(self, points2D_all, points2D_idxs, points3D_id, query_camera):\n        points2D = points2D_all[points2D_idxs]\n        points3D = [self.reconstruction.points3D[j].xyz for j in points3D_id]\n        ret = pycolmap.absolute_pose_estimation(\n            points2D,\n            points3D,",
        "detail": "hloc.localize_sfm",
        "documentation": {}
    },
    {
        "label": "do_covisibility_clustering",
        "kind": 2,
        "importPath": "hloc.localize_sfm",
        "description": "hloc.localize_sfm",
        "peekOfCode": "def do_covisibility_clustering(\n    frame_ids: List[int], reconstruction: pycolmap.Reconstruction\n):\n    clusters = []\n    visited = set()\n    for frame_id in frame_ids:\n        # Check if already labeled\n        if frame_id in visited:\n            continue\n        # New component",
        "detail": "hloc.localize_sfm",
        "documentation": {}
    },
    {
        "label": "pose_from_cluster",
        "kind": 2,
        "importPath": "hloc.localize_sfm",
        "description": "hloc.localize_sfm",
        "peekOfCode": "def pose_from_cluster(\n    localizer: QueryLocalizer,\n    qname: str,\n    query_camera: pycolmap.Camera,\n    db_ids: List[int],\n    features_path: Path,\n    matches_path: Path,\n    **kwargs,\n):\n    kpq = get_keypoints(features_path, qname)",
        "detail": "hloc.localize_sfm",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.localize_sfm",
        "description": "hloc.localize_sfm",
        "peekOfCode": "def main(\n    reference_sfm: Union[Path, pycolmap.Reconstruction],\n    queries: Path,\n    retrieval: Path,\n    features: Path,\n    matches: Path,\n    results: Path,\n    ransac_thresh: int = 12,\n    covisibility_clustering: bool = False,\n    prepend_camera_name: bool = False,",
        "detail": "hloc.localize_sfm",
        "documentation": {}
    },
    {
        "label": "ImagePairDataset",
        "kind": 6,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "class ImagePairDataset(torch.utils.data.Dataset):\n    default_conf = {\n        \"grayscale\": True,\n        \"resize_max\": 1024,\n        \"dfactor\": 8,\n        \"cache_images\": False,\n    }\n    def __init__(self, image_dir, conf, pairs):\n        self.image_dir = image_dir\n        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf})",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "to_cpts",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def to_cpts(kpts, ps):\n    if ps > 0.0:\n        kpts = np.round(np.round((kpts + 0.5) / ps) * ps - 0.5, 2)\n    return [tuple(cpt) for cpt in kpts]\ndef assign_keypoints(\n    kpts: np.ndarray,\n    other_cpts: Union[List[Tuple], np.ndarray],\n    max_error: float,\n    update: bool = False,\n    ref_bins: Optional[List[Counter]] = None,",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "assign_keypoints",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def assign_keypoints(\n    kpts: np.ndarray,\n    other_cpts: Union[List[Tuple], np.ndarray],\n    max_error: float,\n    update: bool = False,\n    ref_bins: Optional[List[Counter]] = None,\n    scores: Optional[np.ndarray] = None,\n    cell_size: Optional[int] = None,\n):\n    if not update:",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "get_grouped_ids",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def get_grouped_ids(array):\n    # Group array indices based on its values\n    # all duplicates are grouped as a set\n    idx_sort = np.argsort(array)\n    sorted_array = array[idx_sort]\n    _, ids, _ = np.unique(sorted_array, return_counts=True, return_index=True)\n    res = np.split(idx_sort, ids[1:])\n    return res\ndef get_unique_matches(match_ids, scores):\n    if len(match_ids.shape) == 1:",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "get_unique_matches",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def get_unique_matches(match_ids, scores):\n    if len(match_ids.shape) == 1:\n        return [0]\n    isets1 = get_grouped_ids(match_ids[:, 0])\n    isets2 = get_grouped_ids(match_ids[:, 1])\n    uid1s = [ids[scores[ids].argmax()] for ids in isets1 if len(ids) > 0]\n    uid2s = [ids[scores[ids].argmax()] for ids in isets2 if len(ids) > 0]\n    uids = list(set(uid1s).intersection(uid2s))\n    return match_ids[uids], scores[uids]\ndef matches_to_matches0(matches, scores):",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "matches_to_matches0",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def matches_to_matches0(matches, scores):\n    if len(matches) == 0:\n        return np.zeros(0, dtype=np.int32), np.zeros(0, dtype=np.float16)\n    n_kps0 = np.max(matches[:, 0]) + 1\n    matches0 = -np.ones((n_kps0,))\n    scores0 = np.zeros((n_kps0,))\n    matches0[matches[:, 0]] = matches[:, 1]\n    scores0[matches[:, 0]] = scores\n    return matches0.astype(np.int32), scores0.astype(np.float16)\ndef kpids_to_matches0(kpt_ids0, kpt_ids1, scores):",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "kpids_to_matches0",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def kpids_to_matches0(kpt_ids0, kpt_ids1, scores):\n    valid = (kpt_ids0 != -1) & (kpt_ids1 != -1)\n    matches = np.dstack([kpt_ids0[valid], kpt_ids1[valid]])\n    matches = matches.reshape(-1, 2)\n    scores = scores[valid]\n    # Remove n-to-1 matches\n    matches, scores = get_unique_matches(matches, scores)\n    return matches_to_matches0(matches, scores)\ndef scale_keypoints(kpts, scale):\n    if np.any(scale != 1.0):",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "scale_keypoints",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def scale_keypoints(kpts, scale):\n    if np.any(scale != 1.0):\n        kpts *= kpts.new_tensor(scale)\n    return kpts\nclass ImagePairDataset(torch.utils.data.Dataset):\n    default_conf = {\n        \"grayscale\": True,\n        \"resize_max\": 1024,\n        \"dfactor\": 8,\n        \"cache_images\": False,",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "match_dense",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def match_dense(\n    conf: Dict,\n    pairs: List[Tuple[str, str]],\n    image_dir: Path,\n    match_path: Path,  # out\n    existing_refs: Optional[List] = [],\n):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    Model = dynamic_load(matchers, conf[\"model\"][\"name\"])\n    model = Model(conf[\"model\"]).eval().to(device)",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "load_keypoints",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def load_keypoints(\n    conf: Dict, feature_paths_refs: List[Path], quantize: Optional[set] = None\n):\n    name2ref = {\n        n: i for i, p in enumerate(feature_paths_refs) for n in list_h5_names(p)\n    }\n    existing_refs = set(name2ref.keys())\n    if quantize is None:\n        quantize = existing_refs  # quantize all\n    if len(existing_refs) > 0:",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "aggregate_matches",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def aggregate_matches(\n    conf: Dict,\n    pairs: List[Tuple[str, str]],\n    match_path: Path,\n    feature_path: Path,\n    required_queries: Optional[Set[str]] = None,\n    max_kps: Optional[int] = None,\n    cpdict: Dict[str, Iterable] = defaultdict(list),\n    bindict: Dict[str, List[Counter]] = defaultdict(list),\n):",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "assign_matches",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def assign_matches(\n    pairs: List[Tuple[str, str]],\n    match_path: Path,\n    keypoints: Union[List[Path], Dict[str, np.array]],\n    max_error: float,\n):\n    if isinstance(keypoints, list):\n        keypoints = load_keypoints({}, keypoints, kpts_as_bin=set([]))\n    assert len(set(sum(pairs, ())) - set(keypoints.keys())) == 0\n    with h5py.File(str(match_path), \"a\") as fd:",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "match_and_assign",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def match_and_assign(\n    conf: Dict,\n    pairs_path: Path,\n    image_dir: Path,\n    match_path: Path,  # out\n    feature_path_q: Path,  # out\n    feature_paths_refs: Optional[List[Path]] = [],\n    max_kps: Optional[int] = 8192,\n    overwrite: bool = False,\n) -> Path:",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "def main(\n    conf: Dict,\n    pairs: Path,\n    image_dir: Path,\n    export_dir: Optional[Path] = None,\n    matches: Optional[Path] = None,  # out\n    features: Optional[Path] = None,  # out\n    features_ref: Optional[Path] = None,\n    max_kps: Optional[int] = 8192,\n    overwrite: bool = False,",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "confs",
        "kind": 5,
        "importPath": "hloc.match_dense",
        "description": "hloc.match_dense",
        "peekOfCode": "confs = {\n    # Best quality but loads of points. Only use for small scenes\n    \"loftr\": {\n        \"output\": \"matches-loftr\",\n        \"model\": {\"name\": \"loftr\", \"weights\": \"outdoor\"},\n        \"preprocessing\": {\"grayscale\": True, \"resize_max\": 1024, \"dfactor\": 8},\n        \"max_error\": 1,  # max error for assigned keypoints (in px)\n        \"cell_size\": 1,  # size of quantization patch (max 1 kp/patch)\n    },\n    # Semi-scalable loftr which limits detected keypoints",
        "detail": "hloc.match_dense",
        "documentation": {}
    },
    {
        "label": "WorkQueue",
        "kind": 6,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "class WorkQueue:\n    def __init__(self, work_fn, num_threads=1):\n        self.queue = Queue(num_threads)\n        self.threads = [\n            Thread(target=self.thread_fn, args=(work_fn,)) for _ in range(num_threads)\n        ]\n        for thread in self.threads:\n            thread.start()\n    def join(self):\n        for thread in self.threads:",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "FeaturePairsDataset",
        "kind": 6,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "class FeaturePairsDataset(torch.utils.data.Dataset):\n    def __init__(self, pairs, feature_path_q, feature_path_r):\n        self.pairs = pairs\n        self.feature_path_q = feature_path_q\n        self.feature_path_r = feature_path_r\n    def __getitem__(self, idx):\n        name0, name1 = self.pairs[idx]\n        data = {}\n        with h5py.File(self.feature_path_q, \"r\") as fd:\n            grp = fd[name0]",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "writer_fn",
        "kind": 2,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "def writer_fn(inp, match_path):\n    pair, pred = inp\n    with h5py.File(str(match_path), \"a\", libver=\"latest\") as fd:\n        if pair in fd:\n            del fd[pair]\n        grp = fd.create_group(pair)\n        matches = pred[\"matches0\"][0].cpu().short().numpy()\n        grp.create_dataset(\"matches0\", data=matches)\n        if \"matching_scores0\" in pred:\n            scores = pred[\"matching_scores0\"][0].cpu().half().numpy()",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "def main(\n    conf: Dict,\n    pairs: Path,\n    features: Union[Path, str],\n    export_dir: Optional[Path] = None,\n    matches: Optional[Path] = None,\n    features_ref: Optional[Path] = None,\n    overwrite: bool = False,\n) -> Path:\n    if isinstance(features, Path) or Path(features).exists():",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "find_unique_new_pairs",
        "kind": 2,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "def find_unique_new_pairs(pairs_all: List[Tuple[str]], match_path: Path = None):\n    \"\"\"Avoid to recompute duplicates to save time.\"\"\"\n    pairs = set()\n    for i, j in pairs_all:\n        if (j, i) not in pairs:\n            pairs.add((i, j))\n    pairs = list(pairs)\n    if match_path is not None and match_path.exists():\n        with h5py.File(str(match_path), \"r\", libver=\"latest\") as fd:\n            pairs_filtered = []",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "match_from_paths",
        "kind": 2,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "def match_from_paths(\n    conf: Dict,\n    pairs_path: Path,\n    match_path: Path,\n    feature_path_q: Path,\n    feature_path_ref: Path,\n    overwrite: bool = False,\n) -> Path:\n    logger.info(\n        \"Matching local features with configuration:\" f\"\\n{pprint.pformat(conf)}\"",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "confs",
        "kind": 5,
        "importPath": "hloc.match_features",
        "description": "hloc.match_features",
        "peekOfCode": "confs = {\n    \"superpoint+lightglue\": {\n        \"output\": \"matches-superpoint-lightglue\",\n        \"model\": {\n            \"name\": \"lightglue\",\n            \"features\": \"superpoint\",\n        },\n    },\n    \"disk+lightglue\": {\n        \"output\": \"matches-disk-lightglue\",",
        "detail": "hloc.match_features",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.pairs_from_covisibility",
        "description": "hloc.pairs_from_covisibility",
        "peekOfCode": "def main(model, output, num_matched):\n    logger.info(\"Reading the COLMAP model...\")\n    cameras, images, points3D = read_model(model)\n    logger.info(\"Extracting image pairs from covisibility info...\")\n    pairs = []\n    for image_id, image in tqdm(images.items()):\n        matched = image.point3D_ids != -1\n        points3D_covis = image.point3D_ids[matched]\n        covis = defaultdict(int)\n        for point_id in points3D_covis:",
        "detail": "hloc.pairs_from_covisibility",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.pairs_from_exhaustive",
        "description": "hloc.pairs_from_exhaustive",
        "peekOfCode": "def main(\n    output: Path,\n    image_list: Optional[Union[Path, List[str]]] = None,\n    features: Optional[Path] = None,\n    ref_list: Optional[Union[Path, List[str]]] = None,\n    ref_features: Optional[Path] = None,\n):\n    if image_list is not None:\n        if isinstance(image_list, (str, Path)):\n            names_q = parse_image_lists(image_list)",
        "detail": "hloc.pairs_from_exhaustive",
        "documentation": {}
    },
    {
        "label": "get_pairwise_distances",
        "kind": 2,
        "importPath": "hloc.pairs_from_poses",
        "description": "hloc.pairs_from_poses",
        "peekOfCode": "def get_pairwise_distances(images):\n    ids = np.array(list(images.keys()))\n    Rs = []\n    ts = []\n    for id_ in ids:\n        image = images[id_]\n        R = image.qvec2rotmat()\n        t = image.tvec\n        Rs.append(R)\n        ts.append(t)",
        "detail": "hloc.pairs_from_poses",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.pairs_from_poses",
        "description": "hloc.pairs_from_poses",
        "peekOfCode": "def main(model, output, num_matched, rotation_threshold=DEFAULT_ROT_THRESH):\n    logger.info(\"Reading the COLMAP model...\")\n    images = read_images_binary(model / \"images.bin\")\n    logger.info(f\"Obtaining pairwise distances between {len(images)} images...\")\n    ids, dist, dR = get_pairwise_distances(images)\n    scores = -dist\n    invalid = dR >= rotation_threshold\n    np.fill_diagonal(invalid, True)\n    pairs = pairs_from_score_matrix(scores, invalid, num_matched)\n    pairs = [(images[ids[i]].name, images[ids[j]].name) for i, j in pairs]",
        "detail": "hloc.pairs_from_poses",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ROT_THRESH",
        "kind": 5,
        "importPath": "hloc.pairs_from_poses",
        "description": "hloc.pairs_from_poses",
        "peekOfCode": "DEFAULT_ROT_THRESH = 30  # in degrees\ndef get_pairwise_distances(images):\n    ids = np.array(list(images.keys()))\n    Rs = []\n    ts = []\n    for id_ in ids:\n        image = images[id_]\n        R = image.qvec2rotmat()\n        t = image.tvec\n        Rs.append(R)",
        "detail": "hloc.pairs_from_poses",
        "documentation": {}
    },
    {
        "label": "parse_names",
        "kind": 2,
        "importPath": "hloc.pairs_from_retrieval",
        "description": "hloc.pairs_from_retrieval",
        "peekOfCode": "def parse_names(prefix, names, names_all):\n    if prefix is not None:\n        if not isinstance(prefix, str):\n            prefix = tuple(prefix)\n        names = [n for n in names_all if n.startswith(prefix)]\n        if len(names) == 0:\n            raise ValueError(f\"Could not find any image with the prefix `{prefix}`.\")\n    elif names is not None:\n        if isinstance(names, (str, Path)):\n            names = parse_image_lists(names)",
        "detail": "hloc.pairs_from_retrieval",
        "documentation": {}
    },
    {
        "label": "get_descriptors",
        "kind": 2,
        "importPath": "hloc.pairs_from_retrieval",
        "description": "hloc.pairs_from_retrieval",
        "peekOfCode": "def get_descriptors(names, path, name2idx=None, key=\"global_descriptor\"):\n    if name2idx is None:\n        with h5py.File(str(path), \"r\", libver=\"latest\") as fd:\n            desc = [fd[n][key].__array__() for n in names]\n    else:\n        desc = []\n        for n in names:\n            with h5py.File(str(path[name2idx[n]]), \"r\", libver=\"latest\") as fd:\n                desc.append(fd[n][key].__array__())\n    return torch.from_numpy(np.stack(desc, 0)).float()",
        "detail": "hloc.pairs_from_retrieval",
        "documentation": {}
    },
    {
        "label": "pairs_from_score_matrix",
        "kind": 2,
        "importPath": "hloc.pairs_from_retrieval",
        "description": "hloc.pairs_from_retrieval",
        "peekOfCode": "def pairs_from_score_matrix(\n    scores: torch.Tensor,\n    invalid: np.array,\n    num_select: int,\n    min_score: Optional[float] = None,\n):\n    assert scores.shape == invalid.shape\n    if isinstance(scores, np.ndarray):\n        scores = torch.from_numpy(scores)\n    invalid = torch.from_numpy(invalid).to(scores.device)",
        "detail": "hloc.pairs_from_retrieval",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.pairs_from_retrieval",
        "description": "hloc.pairs_from_retrieval",
        "peekOfCode": "def main(\n    descriptors,\n    output,\n    num_matched,\n    query_prefix=None,\n    query_list=None,\n    db_prefix=None,\n    db_list=None,\n    db_model=None,\n    db_descriptors=None,",
        "detail": "hloc.pairs_from_retrieval",
        "documentation": {}
    },
    {
        "label": "create_empty_db",
        "kind": 2,
        "importPath": "hloc.reconstruction",
        "description": "hloc.reconstruction",
        "peekOfCode": "def create_empty_db(database_path: Path):\n    if database_path.exists():\n        logger.warning(\"The database already exists, deleting it.\")\n        database_path.unlink()\n    logger.info(\"Creating an empty database...\")\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    db.commit()\n    db.close()\ndef import_images(",
        "detail": "hloc.reconstruction",
        "documentation": {}
    },
    {
        "label": "import_images",
        "kind": 2,
        "importPath": "hloc.reconstruction",
        "description": "hloc.reconstruction",
        "peekOfCode": "def import_images(\n    image_dir: Path,\n    database_path: Path,\n    camera_mode: pycolmap.CameraMode,\n    image_list: Optional[List[str]] = None,\n    options: Optional[Dict[str, Any]] = None,\n):\n    logger.info(\"Importing images into the database...\")\n    if options is None:\n        options = {}",
        "detail": "hloc.reconstruction",
        "documentation": {}
    },
    {
        "label": "get_image_ids",
        "kind": 2,
        "importPath": "hloc.reconstruction",
        "description": "hloc.reconstruction",
        "peekOfCode": "def get_image_ids(database_path: Path) -> Dict[str, int]:\n    db = COLMAPDatabase.connect(database_path)\n    images = {}\n    for name, image_id in db.execute(\"SELECT name, image_id FROM images;\"):\n        images[name] = image_id\n    db.close()\n    return images\ndef run_reconstruction(\n    sfm_dir: Path,\n    database_path: Path,",
        "detail": "hloc.reconstruction",
        "documentation": {}
    },
    {
        "label": "run_reconstruction",
        "kind": 2,
        "importPath": "hloc.reconstruction",
        "description": "hloc.reconstruction",
        "peekOfCode": "def run_reconstruction(\n    sfm_dir: Path,\n    database_path: Path,\n    image_dir: Path,\n    verbose: bool = False,\n    options: Optional[Dict[str, Any]] = None,\n) -> pycolmap.Reconstruction:\n    models_path = sfm_dir / \"models\"\n    models_path.mkdir(exist_ok=True, parents=True)\n    logger.info(\"Running 3D reconstruction...\")",
        "detail": "hloc.reconstruction",
        "documentation": {}
    },
    {
        "label": "main_with_existed_database",
        "kind": 2,
        "importPath": "hloc.reconstruction",
        "description": "hloc.reconstruction",
        "peekOfCode": "def main_with_existed_database(\n    sfm_dir: Path,\n    image_dir: Path,\n    pairs: Path,\n    features: Path,\n    matches: Path,\n    camera_mode: pycolmap.CameraMode = pycolmap.CameraMode.AUTO,\n    verbose: bool = False,\n    skip_geometric_verification: bool = False,\n    min_match_score: Optional[float] = None,",
        "detail": "hloc.reconstruction",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.reconstruction",
        "description": "hloc.reconstruction",
        "peekOfCode": "def main(\n    sfm_dir: Path,\n    image_dir: Path,\n    pairs: Path,\n    features: Path,\n    matches: Path,\n    camera_mode: pycolmap.CameraMode = pycolmap.CameraMode.AUTO,\n    verbose: bool = False,\n    skip_geometric_verification: bool = False,\n    min_match_score: Optional[float] = None,",
        "detail": "hloc.reconstruction",
        "documentation": {}
    },
    {
        "label": "OutputCapture",
        "kind": 6,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "class OutputCapture:\n    def __init__(self, verbose: bool):\n        self.verbose = verbose\n    def __enter__(self):\n        if not self.verbose:\n            self.capture = contextlib.redirect_stdout(io.StringIO())\n            self.out = self.capture.__enter__()\n    def __exit__(self, exc_type, *args):\n        if not self.verbose:\n            self.capture.__exit__(exc_type, *args)",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "create_db_from_model",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def create_db_from_model(\n    reconstruction: pycolmap.Reconstruction, database_path: Path\n) -> Dict[str, int]:\n    if database_path.exists():\n        logger.warning(\"The database already exists, deleting it.\")\n        database_path.unlink()\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    for i, camera in reconstruction.cameras.items():\n        db.add_camera(",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "import_features",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def import_features(\n    image_ids: Dict[str, int], database_path: Path, features_path: Path\n):\n    logger.info(\"Importing features into the database...\")\n    db = COLMAPDatabase.connect(database_path)\n    for image_name, image_id in tqdm(image_ids.items()):\n        keypoints = get_keypoints(features_path, image_name)\n        keypoints += 0.5  # COLMAP origin\n        db.add_keypoints(image_id, keypoints)\n    db.commit()",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "import_matches",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def import_matches(\n    image_ids: Dict[str, int],\n    database_path: Path,\n    pairs_path: Path,\n    matches_path: Path,\n    min_match_score: Optional[float] = None,\n    skip_geometric_verification: bool = False,\n):\n    logger.info(\"Importing matches into the database...\")\n    with open(str(pairs_path), \"r\") as f:",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "estimation_and_geometric_verification",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def estimation_and_geometric_verification(\n    database_path: Path, pairs_path: Path, verbose: bool = False\n):\n    logger.info(\"Performing geometric verification of the matches...\")\n    with OutputCapture(verbose):\n        with pycolmap.ostream():\n            pycolmap.verify_matches(\n                database_path,\n                pairs_path,\n                options=dict(ransac=dict(max_num_trials=20000, min_inlier_ratio=0.1)),",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "geometric_verification",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def geometric_verification(\n    image_ids: Dict[str, int],\n    reference: pycolmap.Reconstruction,\n    database_path: Path,\n    features_path: Path,\n    pairs_path: Path,\n    matches_path: Path,\n    max_error: float = 4.0,\n):\n    logger.info(\"Performing geometric verification of the matches...\")",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "run_triangulation",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def run_triangulation(\n    model_path: Path,\n    database_path: Path,\n    image_dir: Path,\n    reference_model: pycolmap.Reconstruction,\n    verbose: bool = False,\n    options: Optional[Dict[str, Any]] = None,\n) -> pycolmap.Reconstruction:\n    model_path.mkdir(parents=True, exist_ok=True)\n    logger.info(\"Running 3D triangulation...\")",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def main(\n    sfm_dir: Path,\n    reference_model: Path,\n    image_dir: Path,\n    pairs: Path,\n    features: Path,\n    matches: Path,\n    skip_geometric_verification: bool = False,\n    estimate_two_view_geometries: bool = False,\n    min_match_score: Optional[float] = None,",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "parse_option_args",
        "kind": 2,
        "importPath": "hloc.triangulation",
        "description": "hloc.triangulation",
        "peekOfCode": "def parse_option_args(args: List[str], default_options) -> Dict[str, Any]:\n    options = {}\n    for arg in args:\n        idx = arg.find(\"=\")\n        if idx == -1:\n            raise ValueError(\"Options format: key1=value1 key2=value2 etc.\")\n        key, value = arg[:idx], arg[idx + 1 :]\n        if not hasattr(default_options, key):\n            raise ValueError(\n                f'Unknown option \"{key}\", allowed options and default values'",
        "detail": "hloc.triangulation",
        "documentation": {}
    },
    {
        "label": "visualize_sfm_2d",
        "kind": 2,
        "importPath": "hloc.visualization",
        "description": "hloc.visualization",
        "peekOfCode": "def visualize_sfm_2d(\n    reconstruction, image_dir, color_by=\"visibility\", selected=[], n=1, seed=0, dpi=75\n):\n    assert image_dir.exists()\n    if not isinstance(reconstruction, pycolmap.Reconstruction):\n        reconstruction = pycolmap.Reconstruction(reconstruction)\n    if not selected:\n        image_ids = reconstruction.reg_image_ids()\n        selected = random.Random(seed).sample(image_ids, min(n, len(image_ids)))\n    for i in selected:",
        "detail": "hloc.visualization",
        "documentation": {}
    },
    {
        "label": "visualize_loc",
        "kind": 2,
        "importPath": "hloc.visualization",
        "description": "hloc.visualization",
        "peekOfCode": "def visualize_loc(\n    results,\n    image_dir,\n    reconstruction=None,\n    db_image_dir=None,\n    selected=[],\n    n=1,\n    seed=0,\n    prefix=None,\n    **kwargs,",
        "detail": "hloc.visualization",
        "documentation": {}
    },
    {
        "label": "visualize_loc_from_log",
        "kind": 2,
        "importPath": "hloc.visualization",
        "description": "hloc.visualization",
        "peekOfCode": "def visualize_loc_from_log(\n    image_dir,\n    query_name,\n    loc,\n    reconstruction=None,\n    db_image_dir=None,\n    top_k_db=2,\n    dpi=75,\n):\n    q_image = read_image(image_dir / query_name)",
        "detail": "hloc.visualization",
        "documentation": {}
    },
    {
        "label": "filter_query_images",
        "kind": 2,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "def filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"\n    skip_list = ['rgb7', 'rgb8', 'rgb9']\n    filtered = []\n    for name in ref_register_list:\n        prefix = name.split('/')[0]\n        if prefix not in skip_list:\n            filtered.append(name)",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "parse_retrieval",
        "kind": 2,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "def parse_retrieval(path):\n    \"\"\"\n    Reads the pairs from the NetVLAD retrieval output file (pairs-loc.txt).\n    Returns a list of reference image names that are retrieved for each query.\n    \"\"\"\n    retrieval = []\n    with open(path, \"r\") as f:\n        for line in f.read().splitlines():\n            if not line.strip():\n                continue",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "query_position",
        "kind": 2,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "def query_position(\n    query_list, model, references_registered, model_index, \n    feature_conf, retrieval_conf, matcher_conf, images, outputs\n):\n    \"\"\"\n    Localizes a list of queries against a given model by:\n      - extracting features,\n      - global descriptors,\n      - retrieving reference pairs,\n      - matching local features,",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "def main():\n    \"\"\"\n    Main entry point for reference reconstruction and query localization.\n    \"\"\"\n    # Adjust paths according to your setup\n    images = Path(\"/home/siyanhu/Gits/Tramway/Hierarchical-Localization/datasets/sacre_coeur/mapping\")\n    queries_dir  = Path(\"/home/siyanhu/Gits/Tramway/Hierarchical-Localization/datasets/sacre_coeur/mapping\")\n    outputs_ref = Path(\"/home/siyanhu/Gits/Tramway/Hierarchical-Localization/datasets/sacre_coeur/hloc\")\n    outputs_query = Path(\"/home/siyanhu/Gits/Tramway/Hierarchical-Localization/datasets/sacre_coeur/query\")\n    outputs_query.mkdir(parents=True, exist_ok=True)",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Global configuration for extraction and matching\nretrieval_conf = extract_features.confs[\"netvlad\"]\nretrieval_conf[\"device\"] = \"cuda\"  # Force GPU for NetVLAD\nfeature_conf = extract_features.confs[\"superpoint_aachen\"]\nfeature_conf[\"device\"] = \"cuda\"    # Force GPU for SuperPoint\nmatcher_conf = match_features.confs[\"superpoint+lightglue\"]\nmatcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "retrieval_conf",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "retrieval_conf = extract_features.confs[\"netvlad\"]\nretrieval_conf[\"device\"] = \"cuda\"  # Force GPU for NetVLAD\nfeature_conf = extract_features.confs[\"superpoint_aachen\"]\nfeature_conf[\"device\"] = \"cuda\"    # Force GPU for SuperPoint\nmatcher_conf = match_features.confs[\"superpoint+lightglue\"]\nmatcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "retrieval_conf[\"device\"]",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "retrieval_conf[\"device\"] = \"cuda\"  # Force GPU for NetVLAD\nfeature_conf = extract_features.confs[\"superpoint_aachen\"]\nfeature_conf[\"device\"] = \"cuda\"    # Force GPU for SuperPoint\nmatcher_conf = match_features.confs[\"superpoint+lightglue\"]\nmatcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"\n    skip_list = ['rgb7', 'rgb8', 'rgb9']",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "feature_conf",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "feature_conf = extract_features.confs[\"superpoint_aachen\"]\nfeature_conf[\"device\"] = \"cuda\"    # Force GPU for SuperPoint\nmatcher_conf = match_features.confs[\"superpoint+lightglue\"]\nmatcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"\n    skip_list = ['rgb7', 'rgb8', 'rgb9']\n    filtered = []",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "feature_conf[\"device\"]",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "feature_conf[\"device\"] = \"cuda\"    # Force GPU for SuperPoint\nmatcher_conf = match_features.confs[\"superpoint+lightglue\"]\nmatcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"\n    skip_list = ['rgb7', 'rgb8', 'rgb9']\n    filtered = []\n    for name in ref_register_list:",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "matcher_conf",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "matcher_conf = match_features.confs[\"superpoint+lightglue\"]\nmatcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"\n    skip_list = ['rgb7', 'rgb8', 'rgb9']\n    filtered = []\n    for name in ref_register_list:\n        prefix = name.split('/')[0]",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "matcher_conf[\"device\"]",
        "kind": 5,
        "importPath": "nohup_scripts.query_recon",
        "description": "nohup_scripts.query_recon",
        "peekOfCode": "matcher_conf[\"device\"] = \"cuda\"    # Force GPU for LightGlue\ndef filter_query_images(ref_register_list):\n    \"\"\"\n    Filters out images from 'rgb7', 'rgb8', 'rgb9' for references to avoid query overlap.\n    \"\"\"\n    skip_list = ['rgb7', 'rgb8', 'rgb9']\n    filtered = []\n    for name in ref_register_list:\n        prefix = name.split('/')[0]\n        if prefix not in skip_list:",
        "detail": "nohup_scripts.query_recon",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "nohup_scripts.run_recon",
        "description": "nohup_scripts.run_recon",
        "peekOfCode": "def main():\n    # Set up device: CPU or GPU\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n    # --------------------------------------------------------------\n    # Step 1: Define paths and copy images (adjust to your needs)\n    # --------------------------------------------------------------\n    # Example commands from the original notebook (not automatically run here):\n    #\n    # os.system(\"cp -r /media/.../images /media/.../lidarcam/rgb7\")",
        "detail": "nohup_scripts.run_recon",
        "documentation": {}
    },
    {
        "label": "Matching",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.matching",
        "description": "third_party.SuperGluePretrainedNetwork.models.matching",
        "peekOfCode": "class Matching(torch.nn.Module):\n    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n    def __init__(self, config={}):\n        super().__init__()\n        self.superpoint = SuperPoint(config.get('superpoint', {}))\n        self.superglue = SuperGlue(config.get('superglue', {}))\n    def forward(self, data):\n        \"\"\" Run SuperPoint (optionally) and SuperGlue\n        SuperPoint is skipped if ['keypoints0', 'keypoints1'] exist in input\n        Args:",
        "detail": "third_party.SuperGluePretrainedNetwork.models.matching",
        "documentation": {}
    },
    {
        "label": "KeypointEncoder",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "class KeypointEncoder(nn.Module):\n    \"\"\" Joint encoding of visual appearance and location using MLPs\"\"\"\n    def __init__(self, feature_dim: int, layers: List[int]) -> None:\n        super().__init__()\n        self.encoder = MLP([3] + layers + [feature_dim])\n        nn.init.constant_(self.encoder[-1].bias, 0.0)\n    def forward(self, kpts, scores):\n        inputs = [kpts.transpose(1, 2), scores.unsqueeze(1)]\n        return self.encoder(torch.cat(inputs, dim=1))\ndef attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "MultiHeadedAttention",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "class MultiHeadedAttention(nn.Module):\n    \"\"\" Multi-head attention to increase model expressivitiy \"\"\"\n    def __init__(self, num_heads: int, d_model: int):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.dim = d_model // num_heads\n        self.num_heads = num_heads\n        self.merge = nn.Conv1d(d_model, d_model, kernel_size=1)\n        self.proj = nn.ModuleList([deepcopy(self.merge) for _ in range(3)])\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "AttentionalPropagation",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "class AttentionalPropagation(nn.Module):\n    def __init__(self, feature_dim: int, num_heads: int):\n        super().__init__()\n        self.attn = MultiHeadedAttention(num_heads, feature_dim)\n        self.mlp = MLP([feature_dim*2, feature_dim*2, feature_dim])\n        nn.init.constant_(self.mlp[-1].bias, 0.0)\n    def forward(self, x: torch.Tensor, source: torch.Tensor) -> torch.Tensor:\n        message = self.attn(x, source, source)\n        return self.mlp(torch.cat([x, message], dim=1))\nclass AttentionalGNN(nn.Module):",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "AttentionalGNN",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "class AttentionalGNN(nn.Module):\n    def __init__(self, feature_dim: int, layer_names: List[str]) -> None:\n        super().__init__()\n        self.layers = nn.ModuleList([\n            AttentionalPropagation(feature_dim, 4)\n            for _ in range(len(layer_names))])\n        self.names = layer_names\n    def forward(self, desc0: torch.Tensor, desc1: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:\n        for layer, name in zip(self.layers, self.names):\n            if name == 'cross':",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "SuperGlue",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "class SuperGlue(nn.Module):\n    \"\"\"SuperGlue feature matching middle-end\n    Given two sets of keypoints and locations, we determine the\n    correspondences by:\n      1. Keypoint Encoding (normalization + visual feature and location fusion)\n      2. Graph Neural Network with multiple self and cross-attention layers\n      3. Final projection layer\n      4. Optimal Transport Layer (a differentiable Hungarian matching algorithm)\n      5. Thresholding matrix based on mutual exclusivity and a match_threshold\n    The correspondence ids use -1 to indicate non-matching points.",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "def MLP(channels: List[int], do_bn: bool = True) -> nn.Module:\n    \"\"\" Multi-layer perceptron \"\"\"\n    n = len(channels)\n    layers = []\n    for i in range(1, n):\n        layers.append(\n            nn.Conv1d(channels[i - 1], channels[i], kernel_size=1, bias=True))\n        if i < (n-1):\n            if do_bn:\n                layers.append(nn.BatchNorm1d(channels[i]))",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "normalize_keypoints",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "def normalize_keypoints(kpts, image_shape):\n    \"\"\" Normalize keypoints locations based on image image_shape\"\"\"\n    _, _, height, width = image_shape\n    one = kpts.new_tensor(1)\n    size = torch.stack([one*width, one*height])[None]\n    center = size / 2\n    scaling = size.max(1, keepdim=True).values * 0.7\n    return (kpts - center[:, None, :]) / scaling[:, None, :]\nclass KeypointEncoder(nn.Module):\n    \"\"\" Joint encoding of visual appearance and location using MLPs\"\"\"",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "attention",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "def attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:\n    dim = query.shape[1]\n    scores = torch.einsum('bdhn,bdhm->bhnm', query, key) / dim**.5\n    prob = torch.nn.functional.softmax(scores, dim=-1)\n    return torch.einsum('bhnm,bdhm->bdhn', prob, value), prob\nclass MultiHeadedAttention(nn.Module):\n    \"\"\" Multi-head attention to increase model expressivitiy \"\"\"\n    def __init__(self, num_heads: int, d_model: int):\n        super().__init__()\n        assert d_model % num_heads == 0",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "log_sinkhorn_iterations",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "def log_sinkhorn_iterations(Z: torch.Tensor, log_mu: torch.Tensor, log_nu: torch.Tensor, iters: int) -> torch.Tensor:\n    \"\"\" Perform Sinkhorn Normalization in Log-space for stability\"\"\"\n    u, v = torch.zeros_like(log_mu), torch.zeros_like(log_nu)\n    for _ in range(iters):\n        u = log_mu - torch.logsumexp(Z + v.unsqueeze(1), dim=2)\n        v = log_nu - torch.logsumexp(Z + u.unsqueeze(2), dim=1)\n    return Z + u.unsqueeze(2) + v.unsqueeze(1)\ndef log_optimal_transport(scores: torch.Tensor, alpha: torch.Tensor, iters: int) -> torch.Tensor:\n    \"\"\" Perform Differentiable Optimal Transport in Log-space for stability\"\"\"\n    b, m, n = scores.shape",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "log_optimal_transport",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "def log_optimal_transport(scores: torch.Tensor, alpha: torch.Tensor, iters: int) -> torch.Tensor:\n    \"\"\" Perform Differentiable Optimal Transport in Log-space for stability\"\"\"\n    b, m, n = scores.shape\n    one = scores.new_tensor(1)\n    ms, ns = (m*one).to(scores), (n*one).to(scores)\n    bins0 = alpha.expand(b, m, 1)\n    bins1 = alpha.expand(b, 1, n)\n    alpha = alpha.expand(b, 1, 1)\n    couplings = torch.cat([torch.cat([scores, bins0], -1),\n                           torch.cat([bins1, alpha], -1)], 1)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "arange_like",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "description": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "peekOfCode": "def arange_like(x, dim: int):\n    return x.new_ones(x.shape[dim]).cumsum(0) - 1  # traceable in 1.1\nclass SuperGlue(nn.Module):\n    \"\"\"SuperGlue feature matching middle-end\n    Given two sets of keypoints and locations, we determine the\n    correspondences by:\n      1. Keypoint Encoding (normalization + visual feature and location fusion)\n      2. Graph Neural Network with multiple self and cross-attention layers\n      3. Final projection layer\n      4. Optimal Transport Layer (a differentiable Hungarian matching algorithm)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superglue",
        "documentation": {}
    },
    {
        "label": "SuperPoint",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "description": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "peekOfCode": "class SuperPoint(nn.Module):\n    \"\"\"SuperPoint Convolutional Detector and Descriptor\n    SuperPoint: Self-Supervised Interest Point Detection and\n    Description. Daniel DeTone, Tomasz Malisiewicz, and Andrew\n    Rabinovich. In CVPRW, 2019. https://arxiv.org/abs/1712.07629\n    \"\"\"\n    default_config = {\n        'descriptor_dim': 256,\n        'nms_radius': 4,\n        'keypoint_threshold': 0.005,",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "documentation": {}
    },
    {
        "label": "simple_nms",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "description": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "peekOfCode": "def simple_nms(scores, nms_radius: int):\n    \"\"\" Fast Non-maximum suppression to remove nearby points \"\"\"\n    assert(nms_radius >= 0)\n    def max_pool(x):\n        return torch.nn.functional.max_pool2d(\n            x, kernel_size=nms_radius*2+1, stride=1, padding=nms_radius)\n    zeros = torch.zeros_like(scores)\n    max_mask = scores == max_pool(scores)\n    for _ in range(2):\n        supp_mask = max_pool(max_mask.float()) > 0",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "documentation": {}
    },
    {
        "label": "remove_borders",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "description": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "peekOfCode": "def remove_borders(keypoints, scores, border: int, height: int, width: int):\n    \"\"\" Removes keypoints too close to the border \"\"\"\n    mask_h = (keypoints[:, 0] >= border) & (keypoints[:, 0] < (height - border))\n    mask_w = (keypoints[:, 1] >= border) & (keypoints[:, 1] < (width - border))\n    mask = mask_h & mask_w\n    return keypoints[mask], scores[mask]\ndef top_k_keypoints(keypoints, scores, k: int):\n    if k >= len(keypoints):\n        return keypoints, scores\n    scores, indices = torch.topk(scores, k, dim=0)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "documentation": {}
    },
    {
        "label": "top_k_keypoints",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "description": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "peekOfCode": "def top_k_keypoints(keypoints, scores, k: int):\n    if k >= len(keypoints):\n        return keypoints, scores\n    scores, indices = torch.topk(scores, k, dim=0)\n    return keypoints[indices], scores\ndef sample_descriptors(keypoints, descriptors, s: int = 8):\n    \"\"\" Interpolate descriptors at keypoint locations \"\"\"\n    b, c, h, w = descriptors.shape\n    keypoints = keypoints - s / 2 + 0.5\n    keypoints /= torch.tensor([(w*s - s/2 - 0.5), (h*s - s/2 - 0.5)],",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "documentation": {}
    },
    {
        "label": "sample_descriptors",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "description": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "peekOfCode": "def sample_descriptors(keypoints, descriptors, s: int = 8):\n    \"\"\" Interpolate descriptors at keypoint locations \"\"\"\n    b, c, h, w = descriptors.shape\n    keypoints = keypoints - s / 2 + 0.5\n    keypoints /= torch.tensor([(w*s - s/2 - 0.5), (h*s - s/2 - 0.5)],\n                              ).to(keypoints)[None]\n    keypoints = keypoints*2 - 1  # normalize to (-1, 1)\n    args = {'align_corners': True} if torch.__version__ >= '1.3' else {}\n    descriptors = torch.nn.functional.grid_sample(\n        descriptors, keypoints.view(b, 1, -1, 2), mode='bilinear', **args)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.superpoint",
        "documentation": {}
    },
    {
        "label": "AverageTimer",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "class AverageTimer:\n    \"\"\" Class to help manage printing simple timing of code execution. \"\"\"\n    def __init__(self, smoothing=0.3, newline=False):\n        self.smoothing = smoothing\n        self.newline = newline\n        self.times = OrderedDict()\n        self.will_print = OrderedDict()\n        self.reset()\n    def reset(self):\n        now = time.time()",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "VideoStreamer",
        "kind": 6,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "class VideoStreamer:\n    \"\"\" Class to help process image streams. Four types of possible inputs:\"\n        1.) USB Webcam.\n        2.) An IP camera\n        3.) A directory of images (files in directory matching 'image_glob').\n        4.) A video file, such as an .mp4 or .avi file.\n    \"\"\"\n    def __init__(self, basedir, resize, skip, image_glob, max_length=1000000):\n        self._ip_grabbed = False\n        self._ip_running = False",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "process_resize",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def process_resize(w, h, resize):\n    assert(len(resize) > 0 and len(resize) <= 2)\n    if len(resize) == 1 and resize[0] > -1:\n        scale = resize[0] / max(h, w)\n        w_new, h_new = int(round(w*scale)), int(round(h*scale))\n    elif len(resize) == 1 and resize[0] == -1:\n        w_new, h_new = w, h\n    else:  # len(resize) == 2:\n        w_new, h_new = resize[0], resize[1]\n    # Issue warning if resolution is too small or too large.",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "frame2tensor",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def frame2tensor(frame, device):\n    return torch.from_numpy(frame/255.).float()[None, None].to(device)\ndef read_image(path, device, resize, rotation, resize_float):\n    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        return None, None, None\n    w, h = image.shape[1], image.shape[0]\n    w_new, h_new = process_resize(w, h, resize)\n    scales = (float(w) / float(w_new), float(h) / float(h_new))\n    if resize_float:",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "read_image",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def read_image(path, device, resize, rotation, resize_float):\n    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        return None, None, None\n    w, h = image.shape[1], image.shape[0]\n    w_new, h_new = process_resize(w, h, resize)\n    scales = (float(w) / float(w_new), float(h) / float(h_new))\n    if resize_float:\n        image = cv2.resize(image.astype('float32'), (w_new, h_new))\n    else:",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "estimate_pose",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def estimate_pose(kpts0, kpts1, K0, K1, thresh, conf=0.99999):\n    if len(kpts0) < 5:\n        return None\n    f_mean = np.mean([K0[0, 0], K1[1, 1], K0[0, 0], K1[1, 1]])\n    norm_thresh = thresh / f_mean\n    kpts0 = (kpts0 - K0[[0, 1], [2, 2]][None]) / K0[[0, 1], [0, 1]][None]\n    kpts1 = (kpts1 - K1[[0, 1], [2, 2]][None]) / K1[[0, 1], [0, 1]][None]\n    E, mask = cv2.findEssentialMat(\n        kpts0, kpts1, np.eye(3), threshold=norm_thresh, prob=conf,\n        method=cv2.RANSAC)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "rotate_intrinsics",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def rotate_intrinsics(K, image_shape, rot):\n    \"\"\"image_shape is the shape of the image after rotation\"\"\"\n    assert rot <= 3\n    h, w = image_shape[:2][::-1 if (rot % 2) else 1]\n    fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n    rot = rot % 4\n    if rot == 1:\n        return np.array([[fy, 0., cy],\n                         [0., fx, w-1-cx],\n                         [0., 0., 1.]], dtype=K.dtype)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "rotate_pose_inplane",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def rotate_pose_inplane(i_T_w, rot):\n    rotation_matrices = [\n        np.array([[np.cos(r), -np.sin(r), 0., 0.],\n                  [np.sin(r), np.cos(r), 0., 0.],\n                  [0., 0., 1., 0.],\n                  [0., 0., 0., 1.]], dtype=np.float32)\n        for r in [np.deg2rad(d) for d in (0, 270, 180, 90)]\n    ]\n    return np.dot(rotation_matrices[rot], i_T_w)\ndef scale_intrinsics(K, scales):",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "scale_intrinsics",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def scale_intrinsics(K, scales):\n    scales = np.diag([1./scales[0], 1./scales[1], 1.])\n    return np.dot(scales, K)\ndef to_homogeneous(points):\n    return np.concatenate([points, np.ones_like(points[:, :1])], axis=-1)\ndef compute_epipolar_error(kpts0, kpts1, T_0to1, K0, K1):\n    kpts0 = (kpts0 - K0[[0, 1], [2, 2]][None]) / K0[[0, 1], [0, 1]][None]\n    kpts1 = (kpts1 - K1[[0, 1], [2, 2]][None]) / K1[[0, 1], [0, 1]][None]\n    kpts0 = to_homogeneous(kpts0)\n    kpts1 = to_homogeneous(kpts1)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "to_homogeneous",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def to_homogeneous(points):\n    return np.concatenate([points, np.ones_like(points[:, :1])], axis=-1)\ndef compute_epipolar_error(kpts0, kpts1, T_0to1, K0, K1):\n    kpts0 = (kpts0 - K0[[0, 1], [2, 2]][None]) / K0[[0, 1], [0, 1]][None]\n    kpts1 = (kpts1 - K1[[0, 1], [2, 2]][None]) / K1[[0, 1], [0, 1]][None]\n    kpts0 = to_homogeneous(kpts0)\n    kpts1 = to_homogeneous(kpts1)\n    t0, t1, t2 = T_0to1[:3, 3]\n    t_skew = np.array([\n        [0, -t2, t1],",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "compute_epipolar_error",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def compute_epipolar_error(kpts0, kpts1, T_0to1, K0, K1):\n    kpts0 = (kpts0 - K0[[0, 1], [2, 2]][None]) / K0[[0, 1], [0, 1]][None]\n    kpts1 = (kpts1 - K1[[0, 1], [2, 2]][None]) / K1[[0, 1], [0, 1]][None]\n    kpts0 = to_homogeneous(kpts0)\n    kpts1 = to_homogeneous(kpts1)\n    t0, t1, t2 = T_0to1[:3, 3]\n    t_skew = np.array([\n        [0, -t2, t1],\n        [t2, 0, -t0],\n        [-t1, t0, 0]",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "angle_error_mat",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def angle_error_mat(R1, R2):\n    cos = (np.trace(np.dot(R1.T, R2)) - 1) / 2\n    cos = np.clip(cos, -1., 1.)  # numercial errors can make it out of bounds\n    return np.rad2deg(np.abs(np.arccos(cos)))\ndef angle_error_vec(v1, v2):\n    n = np.linalg.norm(v1) * np.linalg.norm(v2)\n    return np.rad2deg(np.arccos(np.clip(np.dot(v1, v2) / n, -1.0, 1.0)))\ndef compute_pose_error(T_0to1, R, t):\n    R_gt = T_0to1[:3, :3]\n    t_gt = T_0to1[:3, 3]",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "angle_error_vec",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def angle_error_vec(v1, v2):\n    n = np.linalg.norm(v1) * np.linalg.norm(v2)\n    return np.rad2deg(np.arccos(np.clip(np.dot(v1, v2) / n, -1.0, 1.0)))\ndef compute_pose_error(T_0to1, R, t):\n    R_gt = T_0to1[:3, :3]\n    t_gt = T_0to1[:3, 3]\n    error_t = angle_error_vec(t, t_gt)\n    error_t = np.minimum(error_t, 180 - error_t)  # ambiguity of E estimation\n    error_R = angle_error_mat(R, R_gt)\n    return error_t, error_R",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "compute_pose_error",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def compute_pose_error(T_0to1, R, t):\n    R_gt = T_0to1[:3, :3]\n    t_gt = T_0to1[:3, 3]\n    error_t = angle_error_vec(t, t_gt)\n    error_t = np.minimum(error_t, 180 - error_t)  # ambiguity of E estimation\n    error_R = angle_error_mat(R, R_gt)\n    return error_t, error_R\ndef pose_auc(errors, thresholds):\n    sort_idx = np.argsort(errors)\n    errors = np.array(errors.copy())[sort_idx]",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "pose_auc",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def pose_auc(errors, thresholds):\n    sort_idx = np.argsort(errors)\n    errors = np.array(errors.copy())[sort_idx]\n    recall = (np.arange(len(errors)) + 1) / len(errors)\n    errors = np.r_[0., errors]\n    recall = np.r_[0., recall]\n    aucs = []\n    for t in thresholds:\n        last_index = np.searchsorted(errors, t)\n        r = np.r_[recall[:last_index], recall[last_index-1]]",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "plot_image_pair",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def plot_image_pair(imgs, dpi=100, size=6, pad=.5):\n    n = len(imgs)\n    assert n == 2, 'number of images must be two'\n    figsize = (size*n, size*3/4) if size is not None else None\n    _, ax = plt.subplots(1, n, figsize=figsize, dpi=dpi)\n    for i in range(n):\n        ax[i].imshow(imgs[i], cmap=plt.get_cmap('gray'), vmin=0, vmax=255)\n        ax[i].get_yaxis().set_ticks([])\n        ax[i].get_xaxis().set_ticks([])\n        for spine in ax[i].spines.values():  # remove frame",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "plot_keypoints",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def plot_keypoints(kpts0, kpts1, color='w', ps=2):\n    ax = plt.gcf().axes\n    ax[0].scatter(kpts0[:, 0], kpts0[:, 1], c=color, s=ps)\n    ax[1].scatter(kpts1[:, 0], kpts1[:, 1], c=color, s=ps)\ndef plot_matches(kpts0, kpts1, color, lw=1.5, ps=4):\n    fig = plt.gcf()\n    ax = fig.axes\n    fig.canvas.draw()\n    transFigure = fig.transFigure.inverted()\n    fkpts0 = transFigure.transform(ax[0].transData.transform(kpts0))",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "plot_matches",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def plot_matches(kpts0, kpts1, color, lw=1.5, ps=4):\n    fig = plt.gcf()\n    ax = fig.axes\n    fig.canvas.draw()\n    transFigure = fig.transFigure.inverted()\n    fkpts0 = transFigure.transform(ax[0].transData.transform(kpts0))\n    fkpts1 = transFigure.transform(ax[1].transData.transform(kpts1))\n    fig.lines = [matplotlib.lines.Line2D(\n        (fkpts0[i, 0], fkpts1[i, 0]), (fkpts0[i, 1], fkpts1[i, 1]), zorder=1,\n        transform=fig.transFigure, c=color[i], linewidth=lw)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "make_matching_plot",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def make_matching_plot(image0, image1, kpts0, kpts1, mkpts0, mkpts1,\n                       color, text, path, show_keypoints=False,\n                       fast_viz=False, opencv_display=False,\n                       opencv_title='matches', small_text=[]):\n    if fast_viz:\n        make_matching_plot_fast(image0, image1, kpts0, kpts1, mkpts0, mkpts1,\n                                color, text, path, show_keypoints, 10,\n                                opencv_display, opencv_title, small_text)\n        return\n    plot_image_pair([image0, image1])",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "make_matching_plot_fast",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def make_matching_plot_fast(image0, image1, kpts0, kpts1, mkpts0,\n                            mkpts1, color, text, path=None,\n                            show_keypoints=False, margin=10,\n                            opencv_display=False, opencv_title='',\n                            small_text=[]):\n    H0, W0 = image0.shape\n    H1, W1 = image1.shape\n    H, W = max(H0, H1), W0 + W1 + margin\n    out = 255*np.ones((H, W), np.uint8)\n    out[:H0, :W0] = image0",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "error_colormap",
        "kind": 2,
        "importPath": "third_party.SuperGluePretrainedNetwork.models.utils",
        "description": "third_party.SuperGluePretrainedNetwork.models.utils",
        "peekOfCode": "def error_colormap(x):\n    return np.clip(\n        np.stack([2-x*2, x*2, np.zeros_like(x), np.ones_like(x)], -1), 0, 1)",
        "detail": "third_party.SuperGluePretrainedNetwork.models.utils",
        "documentation": {}
    },
    {
        "label": "MegaDepthDataset",
        "kind": 6,
        "importPath": "third_party.d2net.lib.dataset",
        "description": "third_party.d2net.lib.dataset",
        "peekOfCode": "class MegaDepthDataset(Dataset):\n    def __init__(\n            self,\n            scene_list_path='megadepth_utils/train_scenes.txt',\n            scene_info_path='/local/dataset/megadepth/scene_info',\n            base_path='/local/dataset/megadepth',\n            train=True,\n            preprocessing=None,\n            min_overlap_ratio=.5,\n            max_overlap_ratio=1,",
        "detail": "third_party.d2net.lib.dataset",
        "documentation": {}
    },
    {
        "label": "EmptyTensorError",
        "kind": 6,
        "importPath": "third_party.d2net.lib.exceptions",
        "description": "third_party.d2net.lib.exceptions",
        "peekOfCode": "class EmptyTensorError(Exception):\n    pass\nclass NoGradientError(Exception):\n    pass",
        "detail": "third_party.d2net.lib.exceptions",
        "documentation": {}
    },
    {
        "label": "NoGradientError",
        "kind": 6,
        "importPath": "third_party.d2net.lib.exceptions",
        "description": "third_party.d2net.lib.exceptions",
        "peekOfCode": "class NoGradientError(Exception):\n    pass",
        "detail": "third_party.d2net.lib.exceptions",
        "documentation": {}
    },
    {
        "label": "loss_function",
        "kind": 2,
        "importPath": "third_party.d2net.lib.loss",
        "description": "third_party.d2net.lib.loss",
        "peekOfCode": "def loss_function(\n        model, batch, device, margin=1, safe_radius=4, scaling_steps=3, plot=False\n):\n    output = model({\n        'image1': batch['image1'].to(device),\n        'image2': batch['image2'].to(device)\n    })\n    loss = torch.tensor(np.array([0], dtype=np.float32), device=device)\n    has_grad = False\n    n_valid_samples = 0",
        "detail": "third_party.d2net.lib.loss",
        "documentation": {}
    },
    {
        "label": "interpolate_depth",
        "kind": 2,
        "importPath": "third_party.d2net.lib.loss",
        "description": "third_party.d2net.lib.loss",
        "peekOfCode": "def interpolate_depth(pos, depth):\n    device = pos.device\n    ids = torch.arange(0, pos.size(1), device=device)\n    h, w = depth.size()\n    i = pos[0, :]\n    j = pos[1, :]\n    # Valid corners\n    i_top_left = torch.floor(i).long()\n    j_top_left = torch.floor(j).long()\n    valid_top_left = torch.min(i_top_left >= 0, j_top_left >= 0)",
        "detail": "third_party.d2net.lib.loss",
        "documentation": {}
    },
    {
        "label": "uv_to_pos",
        "kind": 2,
        "importPath": "third_party.d2net.lib.loss",
        "description": "third_party.d2net.lib.loss",
        "peekOfCode": "def uv_to_pos(uv):\n    return torch.cat([uv[1, :].view(1, -1), uv[0, :].view(1, -1)], dim=0)\ndef warp(\n        pos1,\n        depth1, intrinsics1, pose1, bbox1,\n        depth2, intrinsics2, pose2, bbox2\n):\n    device = pos1.device\n    Z1, pos1, ids = interpolate_depth(pos1, depth1)\n    # COLMAP convention",
        "detail": "third_party.d2net.lib.loss",
        "documentation": {}
    },
    {
        "label": "warp",
        "kind": 2,
        "importPath": "third_party.d2net.lib.loss",
        "description": "third_party.d2net.lib.loss",
        "peekOfCode": "def warp(\n        pos1,\n        depth1, intrinsics1, pose1, bbox1,\n        depth2, intrinsics2, pose2, bbox2\n):\n    device = pos1.device\n    Z1, pos1, ids = interpolate_depth(pos1, depth1)\n    # COLMAP convention\n    u1 = pos1[1, :] + bbox1[1] + .5\n    v1 = pos1[0, :] + bbox1[0] + .5",
        "detail": "third_party.d2net.lib.loss",
        "documentation": {}
    },
    {
        "label": "DenseFeatureExtractionModule",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model",
        "description": "third_party.d2net.lib.model",
        "peekOfCode": "class DenseFeatureExtractionModule(nn.Module):\n    def __init__(self, finetune_feature_extraction=False, use_cuda=True):\n        super(DenseFeatureExtractionModule, self).__init__()\n        model = models.vgg16()\n        vgg16_layers = [\n            'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2',\n            'pool1',\n            'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2',\n            'pool2',\n            'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3',",
        "detail": "third_party.d2net.lib.model",
        "documentation": {}
    },
    {
        "label": "SoftDetectionModule",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model",
        "description": "third_party.d2net.lib.model",
        "peekOfCode": "class SoftDetectionModule(nn.Module):\n    def __init__(self, soft_local_max_size=3):\n        super(SoftDetectionModule, self).__init__()\n        self.soft_local_max_size = soft_local_max_size\n        self.pad = self.soft_local_max_size // 2\n    def forward(self, batch):\n        b = batch.size(0)\n        batch = F.relu(batch)\n        max_per_sample = torch.max(batch.view(b, -1), dim=1)[0]\n        exp = torch.exp(batch / max_per_sample.view(b, 1, 1, 1))",
        "detail": "third_party.d2net.lib.model",
        "documentation": {}
    },
    {
        "label": "D2Net",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model",
        "description": "third_party.d2net.lib.model",
        "peekOfCode": "class D2Net(nn.Module):\n    def __init__(self, model_file=None, use_cuda=True):\n        super(D2Net, self).__init__()\n        self.dense_feature_extraction = DenseFeatureExtractionModule(\n            finetune_feature_extraction=True,\n            use_cuda=use_cuda\n        )\n        self.detection = SoftDetectionModule()\n        if model_file is not None:\n            if use_cuda:",
        "detail": "third_party.d2net.lib.model",
        "documentation": {}
    },
    {
        "label": "DenseFeatureExtractionModule",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model_test",
        "description": "third_party.d2net.lib.model_test",
        "peekOfCode": "class DenseFeatureExtractionModule(nn.Module):\n    def __init__(self, use_relu=True, use_cuda=True):\n        super(DenseFeatureExtractionModule, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            nn.Conv2d(64, 128, 3, padding=1),",
        "detail": "third_party.d2net.lib.model_test",
        "documentation": {}
    },
    {
        "label": "D2Net",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model_test",
        "description": "third_party.d2net.lib.model_test",
        "peekOfCode": "class D2Net(nn.Module):\n    def __init__(self, model_file=None, use_relu=True, use_cuda=True):\n        super(D2Net, self).__init__()\n        self.dense_feature_extraction = DenseFeatureExtractionModule(\n            use_relu=use_relu, use_cuda=use_cuda\n        )\n        self.detection = HardDetectionModule()\n        self.localization = HandcraftedLocalizationModule()\n        if model_file is not None:\n            if use_cuda:",
        "detail": "third_party.d2net.lib.model_test",
        "documentation": {}
    },
    {
        "label": "HardDetectionModule",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model_test",
        "description": "third_party.d2net.lib.model_test",
        "peekOfCode": "class HardDetectionModule(nn.Module):\n    def __init__(self, edge_threshold=5):\n        super(HardDetectionModule, self).__init__()\n        self.edge_threshold = edge_threshold\n        self.dii_filter = torch.tensor(\n            [[0, 1., 0], [0, -2., 0], [0, 1., 0]]\n        ).view(1, 1, 3, 3)\n        self.dij_filter = 0.25 * torch.tensor(\n            [[1., 0, -1.], [0, 0., 0], [-1., 0, 1.]]\n        ).view(1, 1, 3, 3)",
        "detail": "third_party.d2net.lib.model_test",
        "documentation": {}
    },
    {
        "label": "HandcraftedLocalizationModule",
        "kind": 6,
        "importPath": "third_party.d2net.lib.model_test",
        "description": "third_party.d2net.lib.model_test",
        "peekOfCode": "class HandcraftedLocalizationModule(nn.Module):\n    def __init__(self):\n        super(HandcraftedLocalizationModule, self).__init__()\n        self.di_filter = torch.tensor(\n            [[0, -0.5, 0], [0, 0, 0], [0,  0.5, 0]]\n        ).view(1, 1, 3, 3)\n        self.dj_filter = torch.tensor(\n            [[0, 0, 0], [-0.5, 0, 0.5], [0, 0, 0]]\n        ).view(1, 1, 3, 3)\n        self.dii_filter = torch.tensor(",
        "detail": "third_party.d2net.lib.model_test",
        "documentation": {}
    },
    {
        "label": "process_multiscale",
        "kind": 2,
        "importPath": "third_party.d2net.lib.pyramid",
        "description": "third_party.d2net.lib.pyramid",
        "peekOfCode": "def process_multiscale(image, model, scales=[.5, 1, 2]):\n    b, _, h_init, w_init = image.size()\n    device = image.device\n    assert(b == 1)\n    all_keypoints = torch.zeros([3, 0])\n    all_descriptors = torch.zeros([\n        model.dense_feature_extraction.num_channels, 0\n    ])\n    all_scores = torch.zeros(0)\n    previous_dense_features = None",
        "detail": "third_party.d2net.lib.pyramid",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def preprocess_image(image, preprocessing=None):\n    image = image.astype(np.float32)\n    image = np.transpose(image, [2, 0, 1])\n    if preprocessing is None:\n        pass\n    elif preprocessing == 'caffe':\n        # RGB -> BGR\n        image = image[:: -1, :, :]\n        # Zero-center by mean pixel\n        mean = np.array([103.939, 116.779, 123.68])",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "imshow_image",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def imshow_image(image, preprocessing=None):\n    if preprocessing is None:\n        pass\n    elif preprocessing == 'caffe':\n        mean = np.array([103.939, 116.779, 123.68])\n        image = image + mean.reshape([3, 1, 1])\n        # RGB -> BGR\n        image = image[:: -1, :, :]\n    elif preprocessing == 'torch':\n        mean = np.array([0.485, 0.456, 0.406])",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "grid_positions",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def grid_positions(h, w, device, matrix=False):\n    lines = torch.arange(\n        0, h, device=device\n    ).view(-1, 1).float().repeat(1, w)\n    columns = torch.arange(\n        0, w, device=device\n    ).view(1, -1).float().repeat(h, 1)\n    if matrix:\n        return torch.stack([lines, columns], dim=0)\n    else:",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "upscale_positions",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def upscale_positions(pos, scaling_steps=0):\n    for _ in range(scaling_steps):\n        pos = pos * 2 + 0.5\n    return pos\ndef downscale_positions(pos, scaling_steps=0):\n    for _ in range(scaling_steps):\n        pos = (pos - 0.5) / 2\n    return pos\ndef interpolate_dense_features(pos, dense_features, return_corners=False):\n    device = pos.device",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "downscale_positions",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def downscale_positions(pos, scaling_steps=0):\n    for _ in range(scaling_steps):\n        pos = (pos - 0.5) / 2\n    return pos\ndef interpolate_dense_features(pos, dense_features, return_corners=False):\n    device = pos.device\n    ids = torch.arange(0, pos.size(1), device=device)\n    _, h, w = dense_features.size()\n    i = pos[0, :]\n    j = pos[1, :]",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "interpolate_dense_features",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def interpolate_dense_features(pos, dense_features, return_corners=False):\n    device = pos.device\n    ids = torch.arange(0, pos.size(1), device=device)\n    _, h, w = dense_features.size()\n    i = pos[0, :]\n    j = pos[1, :]\n    # Valid corners\n    i_top_left = torch.floor(i).long()\n    j_top_left = torch.floor(j).long()\n    valid_top_left = torch.min(i_top_left >= 0, j_top_left >= 0)",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "savefig",
        "kind": 2,
        "importPath": "third_party.d2net.lib.utils",
        "description": "third_party.d2net.lib.utils",
        "peekOfCode": "def savefig(filepath, fig=None, dpi=None):\n    # TomNorway - https://stackoverflow.com/a/53516034\n    if not fig:\n        fig = plt.gcf()\n    plt.subplots_adjust(0, 0, 1, 1, 0, 0)\n    for ax in fig.axes:\n        ax.axis('off')\n        ax.margins(0, 0)\n        ax.xaxis.set_major_locator(plt.NullLocator())\n        ax.yaxis.set_major_locator(plt.NullLocator())",
        "detail": "third_party.d2net.lib.utils",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "parser = argparse.ArgumentParser(description='MegaDepth preprocessing script')\nparser.add_argument(\n    '--base_path', type=str, required=True,\n    help='path to MegaDepth'\n)\nparser.add_argument(\n    '--scene_id', type=str, required=True,\n    help='scene ID'\n)\nparser.add_argument(",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "args = parser.parse_args()\nbase_path = args.base_path\n# Remove the trailing / if need be.\nif base_path[-1] in ['/', '\\\\']:\n    base_path = base_path[: - 1]\nscene_id = args.scene_id\nbase_depth_path = os.path.join(\n    base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\nbase_undistorted_sfm_path = os.path.join(",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "base_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "base_path = args.base_path\n# Remove the trailing / if need be.\nif base_path[-1] in ['/', '\\\\']:\n    base_path = base_path[: - 1]\nscene_id = args.scene_id\nbase_depth_path = os.path.join(\n    base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\nbase_undistorted_sfm_path = os.path.join(\n    base_path, 'Undistorted_SfM'",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "scene_id",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "scene_id = args.scene_id\nbase_depth_path = os.path.join(\n    base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\nbase_undistorted_sfm_path = os.path.join(\n    base_path, 'Undistorted_SfM'\n)\nundistorted_sparse_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'sparse-txt'\n)",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "base_depth_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "base_depth_path = os.path.join(\n    base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\nbase_undistorted_sfm_path = os.path.join(\n    base_path, 'Undistorted_SfM'\n)\nundistorted_sparse_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'sparse-txt'\n)\nif not os.path.exists(undistorted_sparse_path):",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "base_undistorted_sfm_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "base_undistorted_sfm_path = os.path.join(\n    base_path, 'Undistorted_SfM'\n)\nundistorted_sparse_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'sparse-txt'\n)\nif not os.path.exists(undistorted_sparse_path):\n    exit()\ndepths_path = os.path.join(\n    base_depth_path, scene_id, 'dense0', 'depths'",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "undistorted_sparse_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "undistorted_sparse_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'sparse-txt'\n)\nif not os.path.exists(undistorted_sparse_path):\n    exit()\ndepths_path = os.path.join(\n    base_depth_path, scene_id, 'dense0', 'depths'\n)\nif not os.path.exists(depths_path):\n    exit()",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "depths_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "depths_path = os.path.join(\n    base_depth_path, scene_id, 'dense0', 'depths'\n)\nif not os.path.exists(depths_path):\n    exit()\nimages_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'images'\n)\nif not os.path.exists(images_path):\n    exit()",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "images_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "images_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'images'\n)\nif not os.path.exists(images_path):\n    exit()\n# Process cameras.txt\nwith open(os.path.join(undistorted_sparse_path, 'cameras.txt'), 'r') as f:\n    raw = f.readlines()[3 :]  # skip the header\ncamera_intrinsics = {}\nfor camera in raw:",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "camera_intrinsics",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "camera_intrinsics = {}\nfor camera in raw:\n    camera = camera.split(' ')\n    camera_intrinsics[int(camera[0])] = [float(elem) for elem in camera[2 :]]\n# Process points3D.txt\nwith open(os.path.join(undistorted_sparse_path, 'points3D.txt'), 'r') as f:\n    raw = f.readlines()[3 :]  # skip the header\npoints3D = {}\nfor point3D in raw:\n    point3D = point3D.split(' ')",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "points3D",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "points3D = {}\nfor point3D in raw:\n    point3D = point3D.split(' ')\n    points3D[int(point3D[0])] = np.array([\n        float(point3D[1]), float(point3D[2]), float(point3D[3])\n    ])\n# Process images.txt\nwith open(os.path.join(undistorted_sparse_path, 'images.txt'), 'r') as f:\n    raw = f.readlines()[4 :]  # skip the header\nimage_id_to_idx = {}",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "image_id_to_idx",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "image_id_to_idx = {}\nimage_names = []\nraw_pose = []\ncamera = []\npoints3D_id_to_2D = []\nn_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n    image_id_to_idx[int(image[0])] = idx",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "image_names",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "image_names = []\nraw_pose = []\ncamera = []\npoints3D_id_to_2D = []\nn_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n    image_id_to_idx[int(image[0])] = idx\n    image_name = image[-1].strip('\\n')",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "raw_pose",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "raw_pose = []\ncamera = []\npoints3D_id_to_2D = []\nn_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n    image_id_to_idx[int(image[0])] = idx\n    image_name = image[-1].strip('\\n')\n    image_names.append(image_name)",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "camera",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "camera = []\npoints3D_id_to_2D = []\nn_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n    image_id_to_idx[int(image[0])] = idx\n    image_name = image[-1].strip('\\n')\n    image_names.append(image_name)\n    raw_pose.append([float(elem) for elem in image[1 : -2]])",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "points3D_id_to_2D",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "points3D_id_to_2D = []\nn_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n    image_id_to_idx[int(image[0])] = idx\n    image_name = image[-1].strip('\\n')\n    image_names.append(image_name)\n    raw_pose.append([float(elem) for elem in image[1 : -2]])\n    camera.append(int(image[-2]))",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "n_points3D",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "n_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n    image_id_to_idx[int(image[0])] = idx\n    image_name = image[-1].strip('\\n')\n    image_names.append(image_name)\n    raw_pose.append([float(elem) for elem in image[1 : -2]])\n    camera.append(int(image[-2]))\n    current_points3D_id_to_2D = {}",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "n_images",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "n_images = len(image_names)\n# Image and depthmaps paths\nimage_paths = []\ndepth_paths = []\nfor image_name in image_names:\n    image_path = os.path.join(images_path, image_name)\n    # Path to the depth file\n    depth_path = os.path.join(\n        depths_path, '%s.h5' % os.path.splitext(image_name)[0]\n    )",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "image_paths",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "image_paths = []\ndepth_paths = []\nfor image_name in image_names:\n    image_path = os.path.join(images_path, image_name)\n    # Path to the depth file\n    depth_path = os.path.join(\n        depths_path, '%s.h5' % os.path.splitext(image_name)[0]\n    )\n    if os.path.exists(depth_path):\n        # Check if depth map or background / foreground mask",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "depth_paths",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "depth_paths = []\nfor image_name in image_names:\n    image_path = os.path.join(images_path, image_name)\n    # Path to the depth file\n    depth_path = os.path.join(\n        depths_path, '%s.h5' % os.path.splitext(image_name)[0]\n    )\n    if os.path.exists(depth_path):\n        # Check if depth map or background / foreground mask\n        file_size = os.stat(depth_path).st_size",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "intrinsics",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "intrinsics = []\nposes = []\nprincipal_axis = []\npoints3D_id_to_ndepth = []\nfor idx, image_name in enumerate(image_names):\n    if image_paths[idx] is None:\n        intrinsics.append(None)\n        poses.append(None)\n        principal_axis.append([0, 0, 0])\n        points3D_id_to_ndepth.append({})",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "poses",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "poses = []\nprincipal_axis = []\npoints3D_id_to_ndepth = []\nfor idx, image_name in enumerate(image_names):\n    if image_paths[idx] is None:\n        intrinsics.append(None)\n        poses.append(None)\n        principal_axis.append([0, 0, 0])\n        points3D_id_to_ndepth.append({})\n        continue",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "principal_axis",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "principal_axis = []\npoints3D_id_to_ndepth = []\nfor idx, image_name in enumerate(image_names):\n    if image_paths[idx] is None:\n        intrinsics.append(None)\n        poses.append(None)\n        principal_axis.append([0, 0, 0])\n        points3D_id_to_ndepth.append({})\n        continue\n    image_intrinsics = camera_intrinsics[camera[idx]]",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "points3D_id_to_ndepth",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "points3D_id_to_ndepth = []\nfor idx, image_name in enumerate(image_names):\n    if image_paths[idx] is None:\n        intrinsics.append(None)\n        poses.append(None)\n        principal_axis.append([0, 0, 0])\n        points3D_id_to_ndepth.append({})\n        continue\n    image_intrinsics = camera_intrinsics[camera[idx]]\n    K = np.zeros([3, 3])",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "principal_axis",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "principal_axis = np.array(principal_axis)\nangles = np.rad2deg(np.arccos(\n    np.clip(\n        np.dot(principal_axis, np.transpose(principal_axis)),\n        -1, 1\n    )\n))\n# Compute overlap score\noverlap_matrix = np.full([n_images, n_images], -1.)\nscale_ratio_matrix = np.full([n_images, n_images], -1.)",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "angles",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "angles = np.rad2deg(np.arccos(\n    np.clip(\n        np.dot(principal_axis, np.transpose(principal_axis)),\n        -1, 1\n    )\n))\n# Compute overlap score\noverlap_matrix = np.full([n_images, n_images], -1.)\nscale_ratio_matrix = np.full([n_images, n_images], -1.)\nfor idx1 in range(n_images):",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "overlap_matrix",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "overlap_matrix = np.full([n_images, n_images], -1.)\nscale_ratio_matrix = np.full([n_images, n_images], -1.)\nfor idx1 in range(n_images):\n    if image_paths[idx1] is None or depth_paths[idx1] is None:\n        continue\n    for idx2 in range(idx1 + 1, n_images):\n        if image_paths[idx2] is None or depth_paths[idx2] is None:\n            continue\n        matches = (\n            points3D_id_to_2D[idx1].keys() &",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "scale_ratio_matrix",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.preprocess_scene",
        "description": "third_party.d2net.megadepth_utils.preprocess_scene",
        "peekOfCode": "scale_ratio_matrix = np.full([n_images, n_images], -1.)\nfor idx1 in range(n_images):\n    if image_paths[idx1] is None or depth_paths[idx1] is None:\n        continue\n    for idx2 in range(idx1 + 1, n_images):\n        if image_paths[idx2] is None or depth_paths[idx2] is None:\n            continue\n        matches = (\n            points3D_id_to_2D[idx1].keys() &\n            points3D_id_to_2D[idx2].keys()",
        "detail": "third_party.d2net.megadepth_utils.preprocess_scene",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "description": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "peekOfCode": "parser = argparse.ArgumentParser(description='MegaDepth Undistortion')\nparser.add_argument(\n    '--colmap_path', type=str, required=True,\n    help='path to colmap executable'\n)\nparser.add_argument(\n    '--base_path', type=str, required=True,\n    help='path to MegaDepth'\n)\nargs = parser.parse_args()",
        "detail": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "description": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "peekOfCode": "args = parser.parse_args()\nsfm_path = os.path.join(\n    args.base_path, 'MegaDepth_v1_SfM'\n)\nbase_depth_path = os.path.join(\n    args.base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\noutput_path = os.path.join(\n    args.base_path, 'Undistorted_SfM'\n)",
        "detail": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "documentation": {}
    },
    {
        "label": "sfm_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "description": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "peekOfCode": "sfm_path = os.path.join(\n    args.base_path, 'MegaDepth_v1_SfM'\n)\nbase_depth_path = os.path.join(\n    args.base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\noutput_path = os.path.join(\n    args.base_path, 'Undistorted_SfM'\n)\nos.mkdir(output_path)",
        "detail": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "documentation": {}
    },
    {
        "label": "base_depth_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "description": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "peekOfCode": "base_depth_path = os.path.join(\n    args.base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\noutput_path = os.path.join(\n    args.base_path, 'Undistorted_SfM'\n)\nos.mkdir(output_path)\nfor scene_name in os.listdir(base_depth_path):\n    current_output_path = os.path.join(output_path, scene_name)\n    os.mkdir(current_output_path)",
        "detail": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "description": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "peekOfCode": "output_path = os.path.join(\n    args.base_path, 'Undistorted_SfM'\n)\nos.mkdir(output_path)\nfor scene_name in os.listdir(base_depth_path):\n    current_output_path = os.path.join(output_path, scene_name)\n    os.mkdir(current_output_path)\n    image_path = os.path.join(\n        base_depth_path, scene_name, 'dense0', 'imgs'\n    )",
        "detail": "third_party.d2net.megadepth_utils.undistort_reconstructions",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "third_party.d2net.extract_features",
        "description": "third_party.d2net.extract_features",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n# Argument parsing\nparser = argparse.ArgumentParser(description='Feature extraction script')\nparser.add_argument(\n    '--image_list_file', type=str, required=True,\n    help='path to a file containing a list of images to process'\n)\nparser.add_argument(\n    '--preprocessing', type=str, default='caffe',",
        "detail": "third_party.d2net.extract_features",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "third_party.d2net.extract_features",
        "description": "third_party.d2net.extract_features",
        "peekOfCode": "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n# Argument parsing\nparser = argparse.ArgumentParser(description='Feature extraction script')\nparser.add_argument(\n    '--image_list_file', type=str, required=True,\n    help='path to a file containing a list of images to process'\n)\nparser.add_argument(\n    '--preprocessing', type=str, default='caffe',\n    help='image preprocessing (caffe or torch)'",
        "detail": "third_party.d2net.extract_features",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "third_party.d2net.extract_features",
        "description": "third_party.d2net.extract_features",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Feature extraction script')\nparser.add_argument(\n    '--image_list_file', type=str, required=True,\n    help='path to a file containing a list of images to process'\n)\nparser.add_argument(\n    '--preprocessing', type=str, default='caffe',\n    help='image preprocessing (caffe or torch)'\n)\nparser.add_argument(",
        "detail": "third_party.d2net.extract_features",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "third_party.d2net.extract_features",
        "description": "third_party.d2net.extract_features",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\n# Creating CNN model\nmodel = D2Net(\n    model_file=args.model_file,\n    use_relu=args.use_relu,\n    use_cuda=use_cuda\n)\n# Process the file\nwith open(args.image_list_file, 'r') as f:",
        "detail": "third_party.d2net.extract_features",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "third_party.d2net.extract_features",
        "description": "third_party.d2net.extract_features",
        "peekOfCode": "model = D2Net(\n    model_file=args.model_file,\n    use_relu=args.use_relu,\n    use_cuda=use_cuda\n)\n# Process the file\nwith open(args.image_list_file, 'r') as f:\n    lines = f.readlines()\nfor line in tqdm(lines, total=len(lines)):\n    path = line.strip()",
        "detail": "third_party.d2net.extract_features",
        "documentation": {}
    },
    {
        "label": "process_epoch",
        "kind": 2,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "def process_epoch(\n        epoch_idx,\n        model, loss_function, optimizer, dataloader, device,\n        log_file, args, train=True\n):\n    epoch_losses = []\n    torch.set_grad_enabled(train)\n    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for batch_idx, batch in progress_bar:\n        if train:",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n# Seed\ntorch.manual_seed(1)\nif use_cuda:\n    torch.cuda.manual_seed(1)\nnp.random.seed(1)\n# Argument parsing\nparser = argparse.ArgumentParser(description='Training script')\nparser.add_argument(",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n# Seed\ntorch.manual_seed(1)\nif use_cuda:\n    torch.cuda.manual_seed(1)\nnp.random.seed(1)\n# Argument parsing\nparser = argparse.ArgumentParser(description='Training script')\nparser.add_argument(\n    '--dataset_path', type=str, required=True,",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "parser = argparse.ArgumentParser(description='Training script')\nparser.add_argument(\n    '--dataset_path', type=str, required=True,\n    help='path to the dataset'\n)\nparser.add_argument(\n    '--scene_info_path', type=str, required=True,\n    help='path to the processed scenes'\n)\nparser.add_argument(",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "args = parser.parse_args()\nprint(args)\n# Create the folders for plotting if need be\nif args.plot:\n    plot_path = 'train_vis'\n    if os.path.isdir(plot_path):\n        print('[Warning] Plotting directory already exists.')\n    else:\n        os.mkdir(plot_path)\n# Creating CNN model",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "model = D2Net(\n    model_file=args.model_file,\n    use_cuda=use_cuda\n)\n# Optimizer\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr\n)\n# Dataset\nif args.use_validation:",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "optimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr\n)\n# Dataset\nif args.use_validation:\n    validation_dataset = MegaDepthDataset(\n        scene_list_path='megadepth_utils/valid_scenes.txt',\n        scene_info_path=args.scene_info_path,\n        base_path=args.dataset_path,\n        train=False,",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "training_dataset",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "training_dataset = MegaDepthDataset(\n    scene_list_path='megadepth_utils/train_scenes.txt',\n    scene_info_path=args.scene_info_path,\n    base_path=args.dataset_path,\n    preprocessing=args.preprocessing\n)\ntraining_dataloader = DataLoader(\n    training_dataset,\n    batch_size=args.batch_size,\n    num_workers=args.num_workers",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "training_dataloader",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "training_dataloader = DataLoader(\n    training_dataset,\n    batch_size=args.batch_size,\n    num_workers=args.num_workers\n)\n# Define epoch function\ndef process_epoch(\n        epoch_idx,\n        model, loss_function, optimizer, dataloader, device,\n        log_file, args, train=True",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "log_file",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "log_file = open(args.log_file, 'a+')\n# Initialize the history\ntrain_loss_history = []\nvalidation_loss_history = []\nif args.use_validation:\n    validation_dataset.build_dataset()\n    min_validation_loss = process_epoch(\n        0,\n        model, loss_function, optimizer, validation_dataloader, device,\n        log_file, args,",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "train_loss_history",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "train_loss_history = []\nvalidation_loss_history = []\nif args.use_validation:\n    validation_dataset.build_dataset()\n    min_validation_loss = process_epoch(\n        0,\n        model, loss_function, optimizer, validation_dataloader, device,\n        log_file, args,\n        train=False\n    )",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "validation_loss_history",
        "kind": 5,
        "importPath": "third_party.d2net.train",
        "description": "third_party.d2net.train",
        "peekOfCode": "validation_loss_history = []\nif args.use_validation:\n    validation_dataset.build_dataset()\n    min_validation_loss = process_epoch(\n        0,\n        model, loss_function, optimizer, validation_dataloader, device,\n        log_file, args,\n        train=False\n    )\n# Start the training",
        "detail": "third_party.d2net.train",
        "documentation": {}
    },
    {
        "label": "DatasetCreator",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.create",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.create",
        "peekOfCode": "class DatasetCreator:\n    ''' Create a dataset from a string.\n    dataset_cmd (str):\n        Command to execute.\n        ex: \"ImageList('path/to/list.txt')\"\n    Returns:\n        instanciated dataset.\n    '''\n    def __init__(self, globs):\n        for k, v in globs.items():",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.create",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "class Dataset(object):\n    ''' Base class for a dataset. To be overloaded.\n        Contains:\n            - images                --> get_image(i) --> image\n            - image labels          --> get_label(i)\n            - list of image queries --> get_query(i) --> image\n            - list of query ROIs    --> get_query_roi(i)\n        Creation:\n            Use dataset.create( \"...\" ) to instanciate one.\n            db = dataset.create( \"ImageList('path/to/list.txt')\" )",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "SubDataset",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "class SubDataset(Dataset):\n    ''' Contains a sub-part of another dataset.\n    '''\n    def __init__(self, dataset, indices):\n        self.root = dataset.root\n        self.img_dir = dataset.img_dir\n        self.dataset = dataset\n        self.indices = indices\n        self.nimg = len(self.indices)\n        self.nclass = self.dataset.nclass",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "CatDataset",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "class CatDataset(Dataset):\n    ''' Concatenation of several datasets.\n    '''\n    def __init__(self, *datasets):\n        assert len(datasets) >= 1\n        self.datasets = datasets\n        db = datasets[0]\n        self.root = os.path.normpath(os.path.join(db.root, db.img_dir)) + os.sep\n        self.labels = self.imgs = None # cannot access it the normal way\n        self.classes = db.classes",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "DeployedDataset",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "class DeployedDataset(Dataset):\n    '''Just a deployed dataset with a different root and image extension.\n    '''\n    def __init__(self, dataset, root, imsizes=None, trfs=None, ext=None):\n        self.dataset = dataset\n        if root[-1] != '/': root += '/'\n        self.root = root\n        self.ext = ext\n        self.imsizes = imsizes or json.load(open(root+'imsizes.json'))\n        self.trfs = trfs or (lambda x: x)",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "CropDataset",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "class CropDataset(Dataset):\n    \"\"\"list_of_imgs_and_crops = [(img_key, (l, t, r, b)), ...]\n    \"\"\"\n    def __init__(self, dataset, list_of_imgs_and_crops):\n        self.dataset = dataset\n        self.root = dataset.root\n        self.img_dir = dataset.img_dir\n        self.imgs, self.crops = zip(*list_of_imgs_and_crops)\n        self.nimg = len(self.imgs)\n    def get_image(self, img_idx):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "def split( dataset, train_prop, val_prop=0, method='balanced' ):\n    ''' Split a dataset into several subset:\n        train, val and test\n        method = hash:\n            Split are reliable, i.e. unaffected by adding/removing images.\n            But some clusters might be uneven (not respecting props well)\n        method = balanced:\n            splits are balanced (they respect props well), but not\n            stable to modifications of the dataset.\n        Returns:",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "deploy",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "def deploy( dataset, target_dir, transforms=None, redo=False, ext=None, **savekwargs):\n    if not target_dir: return dataset\n    from PIL import Image\n    from fcntl import flock, LOCK_EX\n    import tqdm\n    if transforms is not None:\n        # identify transform with a unique hash\n        import hashlib\n        def get_params(trf):\n            if type(trf).__name__ == 'Compose':",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "deploy_and_split",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "peekOfCode": "def deploy_and_split( trainset, deploy_trf=None, deploy_dir='/dev/shm',\n                      valset=None, split_val=0.0,\n                      img_ext='jpg', img_quality=95,\n                      **_useless ):\n    ''' Deploy and split a dataset into train / val.\n    if valset is not provided, then trainset is automatically split into train/val\n    based on the split_val proportion.\n    '''\n    # first, deploy the training set\n    traindb = deploy( trainset, deploy_dir, transforms=deploy_trf, ext=img_ext, quality=img_quality )",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "download_dataset",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.downloader",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.downloader",
        "peekOfCode": "def download_dataset(dataset):\n    if not os.path.isdir(DB_ROOT):\n        os.makedirs(DB_ROOT)\n    dataset = dataset.lower()\n    if dataset in ('oxford5k', 'roxford5k'):\n        src_dir = 'http://www.robots.ox.ac.uk/~vgg/data/oxbuildings'\n        dl_files = ['oxbuild_images.tgz']\n        dir_name = 'oxford5k'\n    elif dataset in ('paris6k', 'rparis6k'):\n        src_dir = 'http://www.robots.ox.ac.uk/~vgg/data/parisbuildings'",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.downloader",
        "documentation": {}
    },
    {
        "label": "DB_ROOT",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.downloader",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.downloader",
        "peekOfCode": "DB_ROOT = os.environ['DB_ROOT']\ndef download_dataset(dataset):\n    if not os.path.isdir(DB_ROOT):\n        os.makedirs(DB_ROOT)\n    dataset = dataset.lower()\n    if dataset in ('oxford5k', 'roxford5k'):\n        src_dir = 'http://www.robots.ox.ac.uk/~vgg/data/oxbuildings'\n        dl_files = ['oxbuild_images.tgz']\n        dir_name = 'oxford5k'\n    elif dataset in ('paris6k', 'rparis6k'):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.downloader",
        "documentation": {}
    },
    {
        "label": "ImageList",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImageList(Dataset):\n    ''' Just a list of images (no labels, no query).\n    Input:  text file, 1 image path per row\n    '''\n    def __init__(self, img_list_path, root='', imgs=None):\n        self.root = root\n        if imgs is not None:\n            self.imgs = imgs\n        else:\n            self.imgs = [e.strip() for e in open(img_list_path)]",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "LabelledDatase",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class LabelledDataset (Dataset):\n    \"\"\" A dataset with per-image labels\n        and some convenient functions.\n    \"\"\"\n    def find_classes(self, *arg, **cls_idx):\n        labels = arg[0] if arg else self.labels\n        self.classes, self.cls_idx = find_and_list_classes(labels, cls_idx=cls_idx)\n        self.nclass = len(self.classes)\n        self.c_relevant_idx = find_relevants(self.labels)\nclass ImageListLabels(LabelledDataset):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "ImageListLabels",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImageListLabels(LabelledDataset):\n    ''' Just a list of images with labels (no queries).\n    Input:  text file, 1 image path and label per row (space-separated)\n    '''\n    def __init__(self, img_list_path, root=None):\n        self.root = root\n        if osp.splitext(img_list_path)[1] == '.txt':\n            tmp = [e.strip() for e in open(img_list_path)]\n            self.imgs = [e.split(' ')[0] for e in tmp]\n            self.labels = [e.split(' ')[1] for e in tmp]",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "ImageListLabelsQ",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImageListLabelsQ(ImageListLabels):\n    ''' Two list of images with labels: one for the dataset and one for the queries.\n    Input:  text file, 1 image path and label per row (space-separated)\n    '''\n    def __init__(self, img_list_path, query_list_path, root=None):\n        self.root = root\n        tmp = [e.strip() for e in open(img_list_path)]\n        self.imgs = [e.split(' ')[0] for e in tmp]\n        self.labels = [e.split(' ')[1] for e in tmp]\n        tmp = [e.strip() for e in open(query_list_path)]",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "ImagesAndLabels",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImagesAndLabels(ImageListLabels):\n    ''' Just a list of images with labels.\n    Input:  two arrays containing the text file, 1 image path and label per row (space-separated)\n    '''\n    def __init__(self, imgs, labels, cls_idx, root=None):\n        self.root = root\n        self.imgs = imgs\n        self.labels = labels\n        self.cls_idx = cls_idx\n        self.nclass = len(self.cls_idx.keys())",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "ImageListRelevants",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImageListRelevants(Dataset):\n    \"\"\" A dataset composed by a list of images, a list of indices used as queries,\n        and for each query a list of relevant and junk indices (ie. Oxford-like GT format)\n        Input: path to the pickle file\n    \"\"\"\n    def __init__(self, gt_file, root=None, img_dir='jpg', ext='.jpg'):\n        self.root = root\n        self.img_dir = img_dir\n        with open(gt_file, 'rb') as f:\n            gt = pickle.load(f)",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "ImageListROIs",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImageListROIs(Dataset):\n    def __init__(self, root, img_dir, imgs, rois):\n        self.root = root\n        self.img_dir = img_dir\n        self.imgs = imgs\n        self.rois = rois\n        self.nimg = len(self.imgs)\n        self.nclass = 0\n        self.nquery = 0\n    def get_key(self, i):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "ImageClusters",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class ImageClusters(LabelledDataset):\n    ''' Just a list of images with labels (no query).\n    Input:  JSON, dict of {img_path:class, ...}\n    '''\n    def __init__(self, json_path, root=None, filter=not_none):\n        self.root = root\n        self.imgs = []\n        self.labels = []\n        if isinstance(json_path, dict):\n            data = json_path",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "NullCluster",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "class NullCluster(ImageClusters):\n    ''' Select only images with null label\n    '''\n    def __init__(self, json_path, root=None):\n        ImageClusters.__init__(self, json_path, root, lambda c: c is None)",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "not_none",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "peekOfCode": "def not_none(label):\n    return label is not None\nclass ImageClusters(LabelledDataset):\n    ''' Just a list of images with labels (no query).\n    Input:  JSON, dict of {img_path:class, ...}\n    '''\n    def __init__(self, json_path, root=None, filter=not_none):\n        self.root = root\n        self.imgs = []\n        self.labels = []",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic",
        "documentation": {}
    },
    {
        "label": "find_and_list_classes",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic_func",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic_func",
        "peekOfCode": "def find_and_list_classes(labels, cls_idx=None ):\n    ''' Given a list of image labels, deduce the list of classes.\n    Parameters:\n    -----------\n    labels : list\n        per-image labels (can be str, int, ...)\n    cls_idx : dict or None\n    Returns:\n    --------\n        classes = [class0_name, class1_name, ...]",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic_func",
        "documentation": {}
    },
    {
        "label": "find_relevants",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.generic_func",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.generic_func",
        "peekOfCode": "def find_relevants(labels):\n    \"\"\" For each class, find the set of images from the same class.\n    Returns:\n    --------\n    c_relevant_idx = {class_name: [list of image indices]}\n    \"\"\"\n    assert not isinstance(labels, set), 'labels must be ordered'\n    # Get relevants images for each class\n    c_relevant_idx = defaultdict(list)\n    for i in range(len(labels)):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.generic_func",
        "documentation": {}
    },
    {
        "label": "Landmarks_clean",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "peekOfCode": "class Landmarks_clean(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/annotation_clean_train.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))\nclass Landmarks_clean_val(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/annotation_clean_val.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))\nclass Landmarks_lite(ImageListLabels):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "documentation": {}
    },
    {
        "label": "Landmarks_clean_val",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "peekOfCode": "class Landmarks_clean_val(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/annotation_clean_val.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))\nclass Landmarks_lite(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/extra_landmark_images.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "documentation": {}
    },
    {
        "label": "Landmarks_lite",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "peekOfCode": "class Landmarks_lite(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/extra_landmark_images.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "documentation": {}
    },
    {
        "label": "DB_ROOT",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "peekOfCode": "DB_ROOT = os.environ['DB_ROOT']\nclass Landmarks_clean(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/annotation_clean_train.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))\nclass Landmarks_clean_val(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks/annotations/annotation_clean_val.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks/'))\nclass Landmarks_lite(ImageListLabels):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks",
        "documentation": {}
    },
    {
        "label": "Landmarks18_train",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_train(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_all.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_lite(ImageListLabels):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_all.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_lite(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_lite.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_mid(ImageListLabels):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_lite",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_lite(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_lite.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_mid(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_mid.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_5K(ImageListLabels):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_mid",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_mid(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_mid.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_5K(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_5K.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_val(ImageListLabels):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_5K",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_5K(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_5K.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_val(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/val.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_valdstr(ImageListLabels):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_val",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_val(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/val.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_valdstr(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/val_distractors.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_index(ImageList):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_valdstr",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_valdstr(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/val_distractors.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_index(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/index.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_new_index(ImageList):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_index",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_index(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/index.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_new_index(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/index_new.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_test(ImageList):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_new_index",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_new_index(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/index_new.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_test(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/test.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_pca(ImageList):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_test",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_test(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/test.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_pca(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_pca.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_missing_index(ImageList):\n    def __init__(self):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_pca",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_pca(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_pca.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_missing_index(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/missing_index.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Landmarks18_missing_index",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "class Landmarks18_missing_index(ImageList):\n    def __init__(self):\n        ImageList.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/missing_index.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "DB_ROOT",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "peekOfCode": "DB_ROOT = os.environ['DB_ROOT']\nclass Landmarks18_train(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18(ImageListLabels):\n    def __init__(self):\n        ImageListLabels.__init__(self, os.path.join(DB_ROOT, 'landmarks18/lists/train_all.txt'),\n                                 os.path.join(DB_ROOT, 'landmarks18/'))\nclass Landmarks18_lite(ImageListLabels):",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.landmarks18",
        "documentation": {}
    },
    {
        "label": "Oxford5K",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "peekOfCode": "class Oxford5K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'oxford5k/gnd_oxford5k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'oxford5k'))\nclass ROxford5K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'oxford5k/gnd_roxford5k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'oxford5k'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "documentation": {}
    },
    {
        "label": "ROxford5K",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "peekOfCode": "class ROxford5K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'oxford5k/gnd_roxford5k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'oxford5k'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "documentation": {}
    },
    {
        "label": "DB_ROOT",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "peekOfCode": "DB_ROOT = os.environ['DB_ROOT']\nclass Oxford5K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'oxford5k/gnd_oxford5k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'oxford5k'))\nclass ROxford5K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'oxford5k/gnd_roxford5k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'oxford5k'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.oxford",
        "documentation": {}
    },
    {
        "label": "Paris6K",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "peekOfCode": "class Paris6K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'paris6k/gnd_paris6k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'paris6k'))\nclass RParis6K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'paris6k/gnd_rparis6k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'paris6k'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "documentation": {}
    },
    {
        "label": "RParis6K",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "peekOfCode": "class RParis6K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'paris6k/gnd_rparis6k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'paris6k'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "documentation": {}
    },
    {
        "label": "DB_ROOT",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "description": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "peekOfCode": "DB_ROOT = os.environ['DB_ROOT']\nclass Paris6K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'paris6k/gnd_paris6k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'paris6k'))\nclass RParis6K(ImageListRelevants):\n    def __init__(self):\n        ImageListRelevants.__init__(self, os.path.join(DB_ROOT, 'paris6k/gnd_rparis6k.pkl'),\n                                 root=os.path.join(DB_ROOT, 'paris6k'))",
        "detail": "third_party.deep-image-retrieval.dirtorch.datasets.paris",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "class Bottleneck(nn.Module):\n    ''' Standard bottleneck block\n    input  = inplanes * H * W\n    middle =   planes * H/stride * W/stride\n    output = 4*planes * H/stride * W/stride\n    '''\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "class ResNet(nn.Module):\n    \"\"\" A standard ResNet.\n    \"\"\"\n    def __init__(self, block, layers, fc_out, model_name, self_similarity_radius=None, self_similarity_version=2):\n        nn.Module.__init__(self)\n        self.model_name = model_name\n        # default values for a network pre-trained on imagenet\n        self.rgb_means = [0.485, 0.456, 0.406]\n        self.rgb_stds  = [0.229, 0.224, 0.225]\n        self.input_size = (3, 224, 224)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "reset_weights",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "def reset_weights(net):\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2. / n))\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\nclass ResNet(nn.Module):\n    \"\"\" A standard ResNet.",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "resnet18",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "def resnet18(out_dim=2048):\n    \"\"\"Constructs a ResNet-18 model.\n    \"\"\"\n    net = ResNet(BasicBlock, [2, 2, 2, 2], out_dim, 'resnet18')\n    return net\ndef resnet50(out_dim=2048):\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    net = ResNet(Bottleneck, [3, 4, 6, 3], out_dim, 'resnet50')\n    return net",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "resnet50",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "def resnet50(out_dim=2048):\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    net = ResNet(Bottleneck, [3, 4, 6, 3], out_dim, 'resnet50')\n    return net\ndef resnet101(out_dim=2048):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    net = ResNet(Bottleneck, [3, 4, 23, 3], out_dim, 'resnet101')\n    return net",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "resnet101",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "def resnet101(out_dim=2048):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    net = ResNet(Bottleneck, [3, 4, 23, 3], out_dim, 'resnet101')\n    return net\ndef resnet152(out_dim=2048):\n    \"\"\"Constructs a ResNet-152 model.\n    \"\"\"\n    net = ResNet(Bottleneck, [3, 8, 36, 3], out_dim, 'resnet152')\n    return net",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "resnet152",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "peekOfCode": "def resnet152(out_dim=2048):\n    \"\"\"Constructs a ResNet-152 model.\n    \"\"\"\n    net = ResNet(Bottleneck, [3, 8, 36, 3], out_dim, 'resnet152')\n    return net",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnet",
        "documentation": {}
    },
    {
        "label": "LambdaBase",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "peekOfCode": "class LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\nclass Lambda(LambdaBase):",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "documentation": {}
    },
    {
        "label": "Lambda",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "peekOfCode": "class Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\nresnext101_32x4d_features = nn.Sequential( # Sequential,",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "documentation": {}
    },
    {
        "label": "LambdaMap",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "peekOfCode": "class LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\nresnext101_32x4d_features = nn.Sequential( # Sequential,\n    nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "documentation": {}
    },
    {
        "label": "LambdaReduce",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "peekOfCode": "class LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\nresnext101_32x4d_features = nn.Sequential( # Sequential,\n    nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "documentation": {}
    },
    {
        "label": "resnext101_32x4d_features",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "peekOfCode": "resnext101_32x4d_features = nn.Sequential( # Sequential,\n    nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap(lambda x: x, # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "documentation": {}
    },
    {
        "label": "resnext101_64x4d_features",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "peekOfCode": "resnext101_64x4d_features = nn.Sequential(#Sequential,\n    nn.Conv2d(3, 64, (7, 7), (2, 2), (3, 3), 1, 1, bias = False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3), (2, 2), (1, 1)),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap(lambda x: x, #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.backbones.resnext101_features",
        "documentation": {}
    },
    {
        "label": "GeneralizedMeanPooling",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.layers.pooling",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.layers.pooling",
        "peekOfCode": "class GeneralizedMeanPooling(Module):\n    r\"\"\"Applies a 2D power-average adaptive pooling over an input signal composed of several input planes.\n    The function computed is: :math:`f(X) = pow(sum(pow(X, p)), 1/p)`\n        - At p = infinity, one gets Max Pooling\n        - At p = 1, one gets Average Pooling\n    The output is of size H x W, for any input size.\n    The number of output features is equal to the number of input planes.\n    Args:\n        output_size: the target output size of the image of the form H x W.\n                     Can be a tuple (H, W) or a single H for a square image H x H",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.layers.pooling",
        "documentation": {}
    },
    {
        "label": "GeneralizedMeanPoolingP",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.layers.pooling",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.layers.pooling",
        "peekOfCode": "class GeneralizedMeanPoolingP(GeneralizedMeanPooling):\n    \"\"\" Same, but norm is trainable\n    \"\"\"\n    def __init__(self, norm=3, output_size=1, eps=1e-6):\n        super(GeneralizedMeanPoolingP, self).__init__(norm, output_size, eps)\n        self.p = Parameter(torch.ones(1) * norm)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.layers.pooling",
        "documentation": {}
    },
    {
        "label": "ResNet_RMAC",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "peekOfCode": "class ResNet_RMAC(ResNet):\n    \"\"\" ResNet for RMAC (without ROI pooling)\n    \"\"\"\n    def __init__(self, block, layers, model_name, out_dim=2048, norm_features=False,\n                       pooling='gem', gemp=3, center_bias=0,\n                       dropout_p=None, without_fc=False, **kwargs):\n        ResNet.__init__(self, block, layers, 0, model_name, **kwargs)\n        self.norm_features = norm_features\n        self.without_fc = without_fc\n        self.pooling = pooling",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "documentation": {}
    },
    {
        "label": "l2_normalize",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "peekOfCode": "def l2_normalize(x, axis=-1):\n    x = F.normalize(x, p=2, dim=axis)\n    return x\nclass ResNet_RMAC(ResNet):\n    \"\"\" ResNet for RMAC (without ROI pooling)\n    \"\"\"\n    def __init__(self, block, layers, model_name, out_dim=2048, norm_features=False,\n                       pooling='gem', gemp=3, center_bias=0,\n                       dropout_p=None, without_fc=False, **kwargs):\n        ResNet.__init__(self, block, layers, 0, model_name, **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "documentation": {}
    },
    {
        "label": "resnet18_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "peekOfCode": "def resnet18_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(BasicBlock, [2, 2, 2, 2], 'resnet18', **kwargs)\ndef resnet50_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 6, 3], 'resnet50', **kwargs)\ndef resnet101_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet152_rmac(backbone=ResNet_RMAC, **kwargs):",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "documentation": {}
    },
    {
        "label": "resnet50_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "peekOfCode": "def resnet50_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 6, 3], 'resnet50', **kwargs)\ndef resnet101_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet152_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "documentation": {}
    },
    {
        "label": "resnet101_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "peekOfCode": "def resnet101_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet152_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "documentation": {}
    },
    {
        "label": "resnet152_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "peekOfCode": "def resnet152_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet",
        "documentation": {}
    },
    {
        "label": "ResNet_RMAC_FPN",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "class ResNet_RMAC_FPN(ResNet):\n    \"\"\" ResNet for RMAC (without ROI pooling)\n    \"\"\"\n    def __init__(self, block, layers, model_name, out_dim=None, norm_features=False,\n                       pooling='gem', gemp=3, center_bias=0, mode=1,\n                       dropout_p=None, without_fc=False, **kwargs):\n        ResNet.__init__(self, block, layers, 0, model_name, **kwargs)\n        self.norm_features = norm_features\n        self.without_fc = without_fc\n        self.pooling = pooling",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "l2_normalize",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "def l2_normalize(x, axis=-1):\n    x = F.normalize(x, p=2, dim=axis)\n    return x\nclass ResNet_RMAC_FPN(ResNet):\n    \"\"\" ResNet for RMAC (without ROI pooling)\n    \"\"\"\n    def __init__(self, block, layers, model_name, out_dim=None, norm_features=False,\n                       pooling='gem', gemp=3, center_bias=0, mode=1,\n                       dropout_p=None, without_fc=False, **kwargs):\n        ResNet.__init__(self, block, layers, 0, model_name, **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "resnet18_fpn_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "def resnet18_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(BasicBlock, [2, 2, 2, 2], 'resnet18', **kwargs)\ndef resnet50_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 6, 3], 'resnet50', **kwargs)\ndef resnet101_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet101_fpn0_rmac(backbone=ResNet_RMAC_FPN, **kwargs):",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "resnet50_fpn_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "def resnet50_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 6, 3], 'resnet50', **kwargs)\ndef resnet101_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet101_fpn0_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', mode=0, **kwargs)\ndef resnet152_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "resnet101_fpn_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "def resnet101_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet101_fpn0_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', mode=0, **kwargs)\ndef resnet152_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "resnet101_fpn0_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "def resnet101_fpn0_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', mode=0, **kwargs)\ndef resnet152_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "resnet152_fpn_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "peekOfCode": "def resnet152_fpn_rmac(backbone=ResNet_RMAC_FPN, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnet_fpn",
        "documentation": {}
    },
    {
        "label": "ResNext_RMAC",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "peekOfCode": "class ResNext_RMAC(nn.Module):\n    \"\"\" ResNet for RMAC (without ROI pooling)\n    \"\"\"\n    def __init__(self, backbone, out_dim=2048, norm_features=False,\n                       pooling='gem', gemp=3, center_bias=0,\n                       dropout_p=None, without_fc=False, **kwargs):\n        super(ResNeXt_RMAC, self).__init__()\n        self.backbone = backbone\n        self.norm_features = norm_features\n        self.without_fc = without_fc",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "documentation": {}
    },
    {
        "label": "l2_normalize",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "peekOfCode": "def l2_normalize(x, axis=-1):\n    x = F.normalize(x, p=2, dim=axis)\n    return x\nclass ResNext_RMAC(nn.Module):\n    \"\"\" ResNet for RMAC (without ROI pooling)\n    \"\"\"\n    def __init__(self, backbone, out_dim=2048, norm_features=False,\n                       pooling='gem', gemp=3, center_bias=0,\n                       dropout_p=None, without_fc=False, **kwargs):\n        super(ResNeXt_RMAC, self).__init__()",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "documentation": {}
    },
    {
        "label": "resnet18_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "peekOfCode": "def resnet18_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(BasicBlock, [2, 2, 2, 2], 'resnet18', **kwargs)\ndef resnet50_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 6, 3], 'resnet50', **kwargs)\ndef resnet101_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet152_rmac(backbone=ResNet_RMAC, **kwargs):",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "documentation": {}
    },
    {
        "label": "resnet50_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "peekOfCode": "def resnet50_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 6, 3], 'resnet50', **kwargs)\ndef resnet101_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet152_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "documentation": {}
    },
    {
        "label": "resnet101_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "peekOfCode": "def resnet101_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 4, 23, 3], 'resnet101', **kwargs)\ndef resnet152_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "documentation": {}
    },
    {
        "label": "resnet152_rmac",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "description": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "peekOfCode": "def resnet152_rmac(backbone=ResNet_RMAC, **kwargs):\n    kwargs.pop('scales', None)\n    return backbone(Bottleneck, [3, 8, 36, 3], 'resnet152', **kwargs)",
        "detail": "third_party.deep-image-retrieval.dirtorch.nets.rmac_resnext",
        "documentation": {}
    },
    {
        "label": "typename",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def typename(x):\n    return type(x).__module__\ndef tonumpy(x):\n    if typename(x) == torch.__name__:\n        return x.cpu().numpy()\n    else:\n        return x\ndef matmul(A, B):\n    if typename(A) == np.__name__:\n        B = tonumpy(B)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "tonumpy",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def tonumpy(x):\n    if typename(x) == torch.__name__:\n        return x.cpu().numpy()\n    else:\n        return x\ndef matmul(A, B):\n    if typename(A) == np.__name__:\n        B = tonumpy(B)\n        scores = np.dot(A, B.T)\n    elif typename(B) == torch.__name__:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "matmul",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def matmul(A, B):\n    if typename(A) == np.__name__:\n        B = tonumpy(B)\n        scores = np.dot(A, B.T)\n    elif typename(B) == torch.__name__:\n        scores = torch.matmul(A, B.t()).cpu().numpy()\n    else:\n        raise TypeError(\"matrices must be either numpy or torch type\")\n    return scores\ndef pool(x, pooling='mean', gemp=3):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "pool",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def pool(x, pooling='mean', gemp=3):\n    if len(x) == 1:\n        return x[0]\n    x = torch.stack(x, dim=0)\n    if pooling == 'mean':\n        return torch.mean(x, dim=0)\n    elif pooling == 'gem':\n        def sympow(x, p, eps=1e-6):\n            s = torch.sign(x)\n            return (x*s).clamp(min=eps).pow(p) * s",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "torch_set_gpu",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def torch_set_gpu(gpus, seed=None, randomize=True):\n    if type(gpus) is int:\n            gpus = [gpus]\n    assert gpus, 'error: empty gpu list, use --gpu N N ...'\n    cuda = all(gpu >= 0 for gpu in gpus)\n    if cuda:\n        if any(gpu >= 1000 for gpu in gpus):\n            visible_gpus = [int(gpu) for gpu in os.environ['CUDA_VISIBLE_DEVICES'].split(',')]\n            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(visible_gpus[gpu-1000]) for gpu in gpus])\n        else:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "torch_set_seed",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def torch_set_seed(seed, cuda, randomize=True):\n    if seed:\n        # this makes it 3x SLOWER but deterministic\n        torch.backends.cudnn.enabled = False\n    if randomize and not seed:\n        import time\n        try:\n            seed = int(np.uint32(hash(time.time())))\n        except OverflowError:\n            seed = int.from_bytes(os.urandom(4), byteorder='little', signed=False)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def save_checkpoint(state, is_best, filename):\n    try:\n        dirs = os.path.split(filename)[0]\n        if not os.path.isdir(dirs):\n            os.makedirs(dirs)\n        torch.save(state, filename)\n        if is_best:\n            filenamebest = filename+'.best'\n            shutil.copyfile(filename, filenamebest)\n            filename = filenamebest",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def load_checkpoint(filename, iscuda=False):\n    if not filename:\n        return None\n    assert os.path.isfile(filename), \"=> no checkpoint found at '%s'\" % filename\n    checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n    print(\"=> loading checkpoint '%s'\" % filename, end='')\n    for key in ['epoch', 'iter', 'current_iter']:\n        if key in checkpoint:\n            print(\" (%s %d)\" % (key, checkpoint[key]), end='')\n    print()",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "switch_model_to_cuda",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def switch_model_to_cuda(model, iscuda=True, checkpoint=None):\n    if iscuda:\n        if checkpoint:\n            checkpoint['state_dict'] = {'module.' + k: v for k, v in checkpoint['state_dict'].items()}\n        try:\n            model = torch.nn.DataParallel(model)\n            # copy attributes automatically\n            for var in dir(model.module):\n                if var.startswith('_'):\n                    continue",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "model_size",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def model_size(model):\n    ''' Computes the number of parameters of the model\n    '''\n    size = 0\n    for weights in model.state_dict().values():\n        size += np.prod(weights.shape)\n    return size\ndef freeze_batch_norm(model, freeze=True, only_running=False):\n    model.freeze_bn = bool(freeze)\n    if not freeze:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "freeze_batch_norm",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def freeze_batch_norm(model, freeze=True, only_running=False):\n    model.freeze_bn = bool(freeze)\n    if not freeze:\n        return\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            # Eval mode freezes the running mean and std\n            m.eval()\n            for param in m.named_parameters():\n                if only_running:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "variables",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def variables(inputs, iscuda, not_on_gpu=[]):\n    ''' convert several Tensors to cuda.Variables\n    Tensor whose index are in not_on_gpu stays on cpu.\n    '''\n    inputs_var = []\n    for i, x in enumerate(inputs):\n        if i not in not_on_gpu and not isinstance(x, (tuple, list)):\n            if iscuda:\n                x = x.cuda(non_blocking=True)\n            x = torch.autograd.Variable(x)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def transform(pca, X, whitenp=0.5, whitenv=None, whitenm=1.0, use_sklearn=True):\n    if use_sklearn:\n        # https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/base.py#L99\n        if pca.mean_ is not None:\n            X = X - pca.mean_\n        X_transformed = np.dot(X, pca.components_[:whitenv].T)\n        if pca.whiten:\n            X_transformed /= whitenm * np.power(pca.explained_variance_[:whitenv], whitenp)\n    else:\n        X = X - pca['means']",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "whiten_features",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "peekOfCode": "def whiten_features(X, pca, l2norm=True, whitenp=0.5, whitenv=None, whitenm=1.0, use_sklearn=True):\n    res = transform(pca, X, whitenp=whitenp, whitenv=whitenv, whitenm=whitenm, use_sklearn=use_sklearn)\n    if l2norm:\n        res = res / np.expand_dims(np.linalg.norm(res, axis=1), axis=1)\n    return res",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.common",
        "documentation": {}
    },
    {
        "label": "_BasePoo",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "class _BasePool (object):\n  def __init__(self, nt=0):\n    self.n = max(1,min(mp.cpu_count(), nt if nt>0 else nt+mp.cpu_count()))\n  def starmap(self, func, args):\n    self.map(lambda a: func(*a), args)\nclass ProcessPool (_BasePool):\n  def __init__(self, nt=0):\n    CorePool.__init__(self, nt)\n    self.map = map if self.n==1 else mp.Pool(self.n).map\nclass ThreadPool (_BasePool):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "ProcessPoo",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "class ProcessPool (_BasePool):\n  def __init__(self, nt=0):\n    CorePool.__init__(self, nt)\n    self.map = map if self.n==1 else mp.Pool(self.n).map\nclass ThreadPool (_BasePool):\n  def __init__(self, nt=0):\n    CorePool.__init__(self, nt)\n    self.map = map if self.n==1 else mp.dummy.Pool(self.n).map\n################################################\n# List utils",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "ThreadPoo",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "class ThreadPool (_BasePool):\n  def __init__(self, nt=0):\n    CorePool.__init__(self, nt)\n    self.map = map if self.n==1 else mp.dummy.Pool(self.n).map\n################################################\n# List utils\ndef is_iterable(val, exclude={str}):\n    if type(exclude) not in (tuple, dict, set):\n      exclude = {exclude}\n    try:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "mkdir",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def mkdir(d):\n    try: os.makedirs(d)\n    except OSError: pass\ndef mkdir( fname, isfile='auto' ):\n    ''' Make a directory given a file path\n        If the path is already a directory, make sure it ends with '/' !\n    '''\n    if isfile == 'auto':\n        isfile = bool(os.path.splitext(fname)[1])\n    if isfile:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "mkdir",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def mkdir( fname, isfile='auto' ):\n    ''' Make a directory given a file path\n        If the path is already a directory, make sure it ends with '/' !\n    '''\n    if isfile == 'auto':\n        isfile = bool(os.path.splitext(fname)[1])\n    if isfile:\n        directory = os.path.split(fname)[0]\n    else:\n        directory = fname",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "touch",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def touch(filename):\n    ''' Touch is file. Create the file and directory if necessary.\n    '''\n    assert isinstance(filename, str), 'filename \"%s\" must be a string' % (str(filename))\n    dirs = os.path.split(filename)[0]\n    mkdir(dirs)\n    open(filename,'r+' if os.path.isfile(filename) else 'w') # touch\ndef assert_outpath( path, ext='', mkdir=False ):\n    \"\"\" Verify that the output file has correct format.\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "assert_outpath",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def assert_outpath( path, ext='', mkdir=False ):\n    \"\"\" Verify that the output file has correct format.\n    \"\"\"\n    folder, fname = os.path.split(path)\n    if ext:  assert os.path.splitext(fname)[1] == ext, 'Bad file extension, should be '+ext\n    if mkdir: _mkdir(folder, isfile=False)\n    assert os.path.isdir(folder), 'Destination folder not found '+folder\n    assert not os.path.isfile(path), 'File already exists '+path\n################################################\n# Multiprocessing stuff",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "is_iterable",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def is_iterable(val, exclude={str}):\n    if type(exclude) not in (tuple, dict, set):\n      exclude = {exclude}\n    try:\n        if type(val) in exclude:    # iterable but no\n            raise TypeError()\n        plouf = iter(val)\n        return True\n    except TypeError:\n        return False",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "listify",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def listify( val, exclude={str} ):\n    # make it iterable\n    return val if is_iterable(val,exclude=exclude) else (val,)\ndef unlistify( lis ):\n    # if it contains just one element, returns it\n    if len(lis) == 1:\n        for e in lis: return e\n    return lis\n################################################\n# file stuff",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "unlistify",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def unlistify( lis ):\n    # if it contains just one element, returns it\n    if len(lis) == 1:\n        for e in lis: return e\n    return lis\n################################################\n# file stuff\ndef sig_folder_ext(f):\n    return (os.path.split(f)[0], os.path.splitext(f)[1])\ndef sig_folder(f):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "sig_folder_ext",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def sig_folder_ext(f):\n    return (os.path.split(f)[0], os.path.splitext(f)[1])\ndef sig_folder(f):\n    return os.path.split(f)[0]\ndef sig_ext(f):\n    return os.path.splitext(f)[1]\ndef sig_3folder_ext(f):\n    f = f.replace('//','/')\n    f = f.replace('//','/')\n    return tuple(f.split('/')[:3]) + (os.path.splitext(f)[1],)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "sig_folder",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def sig_folder(f):\n    return os.path.split(f)[0]\ndef sig_ext(f):\n    return os.path.splitext(f)[1]\ndef sig_3folder_ext(f):\n    f = f.replace('//','/')\n    f = f.replace('//','/')\n    return tuple(f.split('/')[:3]) + (os.path.splitext(f)[1],)\ndef sig_all(f):\n    return ()",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "sig_ext",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def sig_ext(f):\n    return os.path.splitext(f)[1]\ndef sig_3folder_ext(f):\n    f = f.replace('//','/')\n    f = f.replace('//','/')\n    return tuple(f.split('/')[:3]) + (os.path.splitext(f)[1],)\ndef sig_all(f):\n    return ()\ndef saferm(f, sig=sig_folder_ext ):\n    if not os.path.isfile(f):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "sig_3folder_ext",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def sig_3folder_ext(f):\n    f = f.replace('//','/')\n    f = f.replace('//','/')\n    return tuple(f.split('/')[:3]) + (os.path.splitext(f)[1],)\ndef sig_all(f):\n    return ()\ndef saferm(f, sig=sig_folder_ext ):\n    if not os.path.isfile(f):\n        return True\n    if not hasattr(saferm,'signature'):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "sig_all",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def sig_all(f):\n    return ()\ndef saferm(f, sig=sig_folder_ext ):\n    if not os.path.isfile(f):\n        return True\n    if not hasattr(saferm,'signature'):\n        saferm.signature = set() # init\n    if sig(f) not in saferm.signature:\n        # reset if the signature is different\n        saferm.ask = True",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "saferm",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def saferm(f, sig=sig_folder_ext ):\n    if not os.path.isfile(f):\n        return True\n    if not hasattr(saferm,'signature'):\n        saferm.signature = set() # init\n    if sig(f) not in saferm.signature:\n        # reset if the signature is different\n        saferm.ask = True\n        saferm.signature.add( sig(f) )\n    if saferm.ask:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "tic",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def tic(tag='tic'):\n    from time import time as now\n    _tics['__last__'] = tag\n    _tics[tag] = now()\ndef toc(tag='', cum=False):\n    from time import time as now\n    t = now()\n    tag = tag or _tics['__last__']\n    t -= _tics[tag]\n    if cum:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "toc",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "def toc(tag='', cum=False):\n    from time import time as now\n    t = now()\n    tag = tag or _tics['__last__']\n    t -= _tics[tag]\n    if cum:\n        nb, oldt = _tics_cum.get(tag,(0,0))\n        nb += 1\n        t += oldt\n        _tics_cum[tag] = nb,t",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "_mkdir",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "_mkdir = mkdir\ndef touch(filename):\n    ''' Touch is file. Create the file and directory if necessary.\n    '''\n    assert isinstance(filename, str), 'filename \"%s\" must be a string' % (str(filename))\n    dirs = os.path.split(filename)[0]\n    mkdir(dirs)\n    open(filename,'r+' if os.path.isfile(filename) else 'w') # touch\ndef assert_outpath( path, ext='', mkdir=False ):\n    \"\"\" Verify that the output file has correct format.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "_tics",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "_tics = dict()\nfrom collections import defaultdict\n_tics_cum = defaultdict(float)\ndef tic(tag='tic'):\n    from time import time as now\n    _tics['__last__'] = tag\n    _tics[tag] = now()\ndef toc(tag='', cum=False):\n    from time import time as now\n    t = now()",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "_tics_cum",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "peekOfCode": "_tics_cum = defaultdict(float)\ndef tic(tag='tic'):\n    from time import time as now\n    _tics['__last__'] = tag\n    _tics[tag] = now()\ndef toc(tag='', cum=False):\n    from time import time as now\n    t = now()\n    tag = tag or _tics['__last__']\n    t -= _tics[tag]",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.convenient",
        "documentation": {}
    },
    {
        "label": "accuracy_topk",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "peekOfCode": "def accuracy_topk(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\n    output: torch.FloatTensoror np.array(float)\n            shape = B * L [* H * W]\n            L: number of possible labels\n    target: torch.IntTensor or np.array(int)\n            shape = B     [* H * W]\n            ground-truth labels\n    \"\"\"\n    if isinstance(output, np.ndarray):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "compute_AP",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "peekOfCode": "def compute_AP(label, score):\n    from sklearn.metrics import average_precision_score\n    return average_precision_score(label, score)\ndef compute_average_precision(positive_ranks):\n    \"\"\"\n    Extracted from: https://github.com/tensorflow/models/blob/master/research/delf/delf/python/detect_to_retrieve/dataset.py\n    Computes average precision according to dataset convention.\n    It assumes that `positive_ranks` contains the ranks for all expected positive\n    index images to be retrieved. If `positive_ranks` is empty, returns\n    `average_precision` = 0.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "compute_average_precision",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "peekOfCode": "def compute_average_precision(positive_ranks):\n    \"\"\"\n    Extracted from: https://github.com/tensorflow/models/blob/master/research/delf/delf/python/detect_to_retrieve/dataset.py\n    Computes average precision according to dataset convention.\n    It assumes that `positive_ranks` contains the ranks for all expected positive\n    index images to be retrieved. If `positive_ranks` is empty, returns\n    `average_precision` = 0.\n    Note that average precision computation here does NOT use the finite sum\n    method (see\n    https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "compute_average_precision_quantized",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "peekOfCode": "def compute_average_precision_quantized(labels, idx, step=0.01):\n    recall_checkpoints = np.arange(0, 1, step)\n    def mymax(x, default):\n        return np.max(x) if len(x) else default\n    Nrel = np.sum(labels)\n    if Nrel == 0:\n        return 0\n    recall = np.cumsum(labels[idx])/float(Nrel)\n    irange = np.arange(1, len(idx)+1)\n    prec = np.cumsum(labels[idx]).astype(np.float32) / irange",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "pixelwise_iou",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "peekOfCode": "def pixelwise_iou(output, target):\n    \"\"\" For each image, for each label, compute the IoU between\n    \"\"\"\n    assert False",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.funcs",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.funcs",
        "peekOfCode": "def sigmoid(x, a=1, b=0):\n    return 1 / (1 + np.exp(a * (b - x)))\ndef sigmoid_range(x, at5, at95):\n    \"\"\" create sigmoid function like that: \n            sigmoid(at5)  = 0.05\n            sigmoid(at95) = 0.95\n        and returns sigmoid(x) \n    \"\"\"\n    a = 6 / (at95 - at5)\n    b = at5 + 3 / a",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.funcs",
        "documentation": {}
    },
    {
        "label": "sigmoid_range",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.funcs",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.funcs",
        "peekOfCode": "def sigmoid_range(x, at5, at95):\n    \"\"\" create sigmoid function like that: \n            sigmoid(at5)  = 0.05\n            sigmoid(at95) = 0.95\n        and returns sigmoid(x) \n    \"\"\"\n    a = 6 / (at95 - at5)\n    b = at5 + 3 / a\n    return sigmoid(x, a, b)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.funcs",
        "documentation": {}
    },
    {
        "label": "PytorchLoade",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "class PytorchLoader (data.Dataset):\n    \"\"\"A pytorch dataset-loader\n     Args:\n        dataset (object):  dataset inherited from dataset.Dataset()\n        transform (deprecated, callable): pytorch transforms. Use img_and_target_transform instead.\n        target_transform (deprecated, callable): applied on target. Use img_and_target_transform instead.\n        img_and_target_transform (callable):\n                applied on dict(img=, label=, bbox=, ...)\n                and should return a similar dictionary.\n     Attributes:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "BalancedSample",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "class BalancedSampler (data.sampler.Sampler):\n    \"\"\" Data sampler that will provide an equal number of each class\n    to the network.\n    size:   float in [0,2]\n        specify the size increase/decrease w.r.t to the original dataset.\n        1 means that the over-classes (with more than median n_per_class images)\n        will have less items, but conversely, under-classes will have more items.\n    balanced:  float in [0,1]\n        specify whether the balance constraint should be respected firmly or not.\n        if ==1: balance is exactly respected; if ==0, same as dataset (no change).",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "get_loader",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "def get_loader( dataset, trf_chain, iscuda,\n                preprocess = {}, # variables for preprocessing (input_size, mean, std, ...)\n                output = ('img','label'),\n                batch_size = None,\n                threads = 1,\n                shuffle = True,\n                balanced = 0, use_all = False,\n                totensor = True,\n                **_useless_kw):\n    ''' Get a data loader, given the dataset and some parameters.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "load_one_img",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "def load_one_img( loader ):\n    ''' Helper to iterate on get_loader()\n    loader: output of get_loader()\n    '''\n    iterator = iter(loader)\n    batch = []\n    while iterator:\n        if not batch: # refill\n            things = next(iterator)\n            batch = list(zip(*[t.numpy() if torch.is_tensor(t) else t for t in things]))",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "tensor2img",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "def tensor2img(tensor, model):\n    \"\"\" convert a numpy tensor to a PIL Image\n        (undo the ToTensor() and Normalize() transforms)\n    \"\"\"\n    mean = model.preprocess['mean']\n    std = model.preprocess['std']\n    if not isinstance(tensor, np.ndarray):\n        if not isinstance(tensor, torch.Tensor):\n            tensor = tensor.data\n        tensor = tensor.squeeze().cpu().numpy()",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "test_loader_speed",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "def test_loader_speed(loader_):\n    ''' Test the speed of a data loader\n    '''\n    from tqdm import tqdm\n    loader = load_one_img(loader_)\n    for _ in tqdm(loader):\n        pass\n    pdb.set_trace()\ndef try_to_get(func, *args, **kwargs):\n    try:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "try_to_get",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "peekOfCode": "def try_to_get(func, *args, **kwargs):\n    try:\n        return func(*args, **kwargs)\n    except NotImplementedError:\n        return None",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.pytorch_loader",
        "documentation": {}
    },
    {
        "label": "Identit",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class Identity (object):\n    \"\"\" Identity transform. It does nothing!\n    \"\"\"\n    def __call__(self, inp):\n        return inp\nclass Pad(object):\n    \"\"\" Pads the shortest side of the image to a given size\n    If size is shorter than the shortest image, then the image will be untouched\n    \"\"\"\n    def __init__(self, size, color=(127,127,127)):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "Pad",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class Pad(object):\n    \"\"\" Pads the shortest side of the image to a given size\n    If size is shorter than the shortest image, then the image will be untouched\n    \"\"\"\n    def __init__(self, size, color=(127,127,127)):\n        self.size = size\n        assert len(color) == 3\n        if not all(isinstance(c,int) for c in color):\n            color = tuple([int(255*c) for c in color])\n        self.color = color",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "PadSquar",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class PadSquare (object):\n    \"\"\" Pads the image to a square size\n    The dimension of the output image will be equal to size x size\n    If size is None, then the image will be padded to the largest dimension\n    If size is smaller than the original image size, the image will be cropped\n    \"\"\"\n    def __init__(self, size=None, color=(127,127,127)):\n        self.size = size\n        assert len(color) == 3\n        if not all(isinstance(c,int) for c in color):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomBorde",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomBorder (object):\n    \"\"\" Expands the image with a random size border\n    \"\"\"\n    def __init__(self, min_size, max_size, color=(127,127,127)):\n        assert isinstance(min_size, int) and min_size >= 0\n        assert isinstance(max_size, int) and min_size <= max_size\n        self.min_size = min_size\n        self.max_size = max_size\n        assert len(color) == 3\n        if not all(isinstance(c,int) for c in color):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "Scal",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class Scale (object):\n    \"\"\" Rescale the input PIL.Image to a given size.\n        Same as torchvision.Scale\n    The smallest dimension of the resulting image will be = size.\n    if largest == True: same behaviour for the largest dimension.\n    if not can_upscale: don't upscale\n    if not can_downscale: don't downscale\n    \"\"\"\n    def __init__(self, size, interpolation=Image.BILINEAR, largest=False, can_upscale=True, can_downscale=True):\n        assert isinstance(size, (float,int)) or (len(size) == 2)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomScal",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomScale (Scale):\n    \"\"\"Rescale the input PIL.Image to a random size.\n    Args:\n        min_size (int): min size of the smaller edge of the picture.\n        max_size (int): max size of the smaller edge of the picture.\n        ar (float or tuple):\n            max change of aspect ratio (width/height).\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomCro",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomCrop (object):\n    \"\"\"Crop the given PIL Image at a random location.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is 0, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCro",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class CenterCrop (RandomCrop):\n    \"\"\"Crops the given PIL Image at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    \"\"\"\n    @staticmethod\n    def get_params(img, output_size):\n        w, h = img.size",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "CropToBbox",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class CropToBbox(object):\n    \"\"\" Crop the image according to the bounding box.\n    margin (float):\n        ensure a margin around the bbox equal to (margin * min(bbWidth,bbHeight))\n    min_size (int):\n        result cannot be smaller than this size\n    \"\"\"\n    def __init__(self, margin=0.5, min_size=0):\n        self.margin = margin\n        self.min_size = min_size",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomRotation",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomRotation(object):\n    \"\"\"Rescale the input PIL.Image to a random size.\n    Args:\n        degrees (float):\n            rotation angle.\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n    def __init__(self, degrees, interpolation=Image.BILINEAR):\n        self.degrees = degrees",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomFli",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomFlip (object):\n    \"\"\"Randomly flip the image.\n    \"\"\"\n    def __call__(self, inp):\n        img = F.grab_img(inp)\n        w, h = img.size\n        flip = np.random.rand() < 0.5\n        if flip:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n        return F.update_img_and_labels(inp, img, aff=(-1,0,w-1,0,1,0))",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomTilting",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomTilting(object):\n    \"\"\"Apply a random tilting (left, right, up, down) to the input PIL.Image\n    Args:\n        maginitude (float):\n            maximum magnitude of the random skew (value between 0 and 1)\n        directions (string):\n            tilting directions allowed (all, left, right, up, down)\n            examples: \"all\", \"left,right\", \"up-down-right\"\n    \"\"\"\n    def __init__(self, magnitude, directions='all'):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "StillTransfor",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class StillTransform (object):\n    \"\"\" Takes and return an image, without changing its shape or geometry.\n    \"\"\"\n    def _transform(self, img):\n        raise NotImplementedError()\n    def __call__(self, inp):\n        img = F.grab_img(inp)\n        # transform the image (size should not change)\n        img = self._transform(img)\n        return F.update_img_and_labels(inp, img, aff=(1,0,0,0,1,0))",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "ColorJitte",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class ColorJitter (StillTransform):\n    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n    Args:\n    brightness (float): How much to jitter brightness. brightness_factor\n    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n    contrast (float): How much to jitter contrast. contrast_factor\n    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n    saturation (float): How much to jitter saturation. saturation_factor\n    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n    hue(float): How much to jitter hue. hue_factor is chosen uniformly from",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "RandomErasin",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class RandomErasing (StillTransform):\n    \"\"\"\n    Class that performs Random Erasing, an augmentation technique described\n    in `https://arxiv.org/abs/1708.04896 <https://arxiv.org/abs/1708.04896>`_\n    by Zhong et al. To quote the authors, random erasing:\n    \"*... randomly selects a rectangle region in an image, and erases its\n    pixels with random values.*\"\n    The size of the random rectangle is controlled using the\n    :attr:`area` parameter. This area is random in its\n    width and height.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "ToTenso",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class ToTensor (StillTransform, tvf.ToTensor):\n    def _transform(self, img):\n        return tvf.ToTensor.__call__(self, img)\nclass Normalize (StillTransform, tvf.Normalize):\n    def _transform(self, img):\n        return tvf.Normalize.__call__(self, img)\nclass BBoxToPixelLabel (object):\n    \"\"\" Convert a bbox into per-pixel label\n    \"\"\"\n    def __init__(self, nclass, downsize, mode):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "Normaliz",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class Normalize (StillTransform, tvf.Normalize):\n    def _transform(self, img):\n        return tvf.Normalize.__call__(self, img)\nclass BBoxToPixelLabel (object):\n    \"\"\" Convert a bbox into per-pixel label\n    \"\"\"\n    def __init__(self, nclass, downsize, mode):\n        self.nclass = nclass\n        self.downsize = downsize\n        self.mode = mode",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "BBoxToPixelLabe",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "class BBoxToPixelLabel (object):\n    \"\"\" Convert a bbox into per-pixel label\n    \"\"\"\n    def __init__(self, nclass, downsize, mode):\n        self.nclass = nclass\n        self.downsize = downsize\n        self.mode = mode\n        self.nbin = 5\n        self.log_scale = 1.5\n        self.ref_scale = 8.0",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "create",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "peekOfCode": "def create(cmd_line, to_tensor=False, **vars):\n    ''' Create a sequence of transformations.\n    cmd_line: (str)\n        Comma-separated list of transformations.\n        Ex: \"Rotate(10), Scale(256)\"\n    to_tensor: (bool)\n        Whether to add the \"ToTensor(), Normalize(mean, std)\"\n        automatically to the end of the transformation string\n    vars: (dict)\n        dictionary of global variables.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms",
        "documentation": {}
    },
    {
        "label": "DummyImg",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "class DummyImg:\n    ''' This class is a dummy image only defined by its size.\n    '''\n    def __init__(self, size):\n        self.size = size\n    def resize(self, size, *args, **kwargs):\n        return DummyImg(size)\n    def expand(self, border):\n        w, h = self.size\n        if isinstance(border, int):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "is_pil_image",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def is_pil_image(img):\n    return isinstance(img, Image.Image)\nclass DummyImg:\n    ''' This class is a dummy image only defined by its size.\n    '''\n    def __init__(self, size):\n        self.size = size\n    def resize(self, size, *args, **kwargs):\n        return DummyImg(size)\n    def expand(self, border):",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "grab_img",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def grab_img( img_and_label ):\n    ''' Called to extract the image from an img_and_label input\n    (a dictionary). Also compatible with old-style PIL images.\n    '''\n    if isinstance(img_and_label, dict):\n        # if input is a dictionary, then\n        # it must contains the img or its size.\n        try:\n            return img_and_label['img']\n        except KeyError:",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "update_img_and_labels",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def update_img_and_labels(img_and_label, img, aff=None, persp=None):\n    ''' Called to update the img_and_label\n    '''\n    if isinstance(img_and_label, dict):\n        img_and_label['img'] = img\n        if 'bbox' in img_and_label:\n            l,t,r,b = img_and_label['bbox']\n            corners = [(l,t),(l,b),(r,b),(r,t)]\n            if aff:\n                pts = [aff_mul(aff, pt) for pt in corners]",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "rand_log_uniform",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def rand_log_uniform(a, b):\n    return np.exp(np.random.uniform(np.log(a),np.log(b)))\ndef int_tuple(*args):\n    return tuple(map(int,args))\ndef aff_translate(tx, ty):\n    return (1,0,tx,\n            0,1,ty)\ndef aff_rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "int_tuple",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def int_tuple(*args):\n    return tuple(map(int,args))\ndef aff_translate(tx, ty):\n    return (1,0,tx,\n            0,1,ty)\ndef aff_rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0)\ndef aff_mul(aff, aff2):\n    ''' affine multiplication.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "aff_translate",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def aff_translate(tx, ty):\n    return (1,0,tx,\n            0,1,ty)\ndef aff_rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0)\ndef aff_mul(aff, aff2):\n    ''' affine multiplication.\n    aff: 6-tuple (affine transform)\n    aff2: 6-tuple (affine transform) or 2-tuple (point)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "aff_rotate",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def aff_rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0)\ndef aff_mul(aff, aff2):\n    ''' affine multiplication.\n    aff: 6-tuple (affine transform)\n    aff2: 6-tuple (affine transform) or 2-tuple (point)\n    '''\n    assert isinstance(aff, tuple)\n    assert isinstance(aff2, tuple)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "aff_mul",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def aff_mul(aff, aff2):\n    ''' affine multiplication.\n    aff: 6-tuple (affine transform)\n    aff2: 6-tuple (affine transform) or 2-tuple (point)\n    '''\n    assert isinstance(aff, tuple)\n    assert isinstance(aff2, tuple)\n    aff = np.array(aff+(0,0,1)).reshape(3,3)\n    if len(aff2) == 6:\n        aff2 = np.array(aff2+(0,0,1)).reshape(3,3)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "persp_mul",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def persp_mul(mat, mat2):\n    ''' homography (perspective) multiplication.\n    mat: 8-tuple (homography transform)\n    mat2: 8-tuple (homography transform) or 2-tuple (point)\n    '''\n    assert isinstance(mat, tuple)\n    assert isinstance(mat2, tuple)\n    mat = np.array(mat+(1,)).reshape(3,3)\n    if len(mat2) == 8:\n        mat2 = np.array(mat2+(1,)).reshape(3,3)",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_brightness",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    brightness_factor (float):  How much to adjust the brightness. Can be\n    any non negative number. 0 gives a black image, 1 gives the\n    original image while 2 increases the brightness by a factor of 2.\n    Returns:\n    PIL Image: Brightness adjusted image.\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_contrast",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    contrast_factor (float): How much to adjust the contrast. Can be any\n    non negative number. 0 gives a solid gray image, 1 gives the\n    original image while 2 increases the contrast by a factor of 2.\n    Returns:\n    PIL Image: Contrast adjusted image.\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_saturation",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    saturation_factor (float):  How much to adjust the saturation. 0 will\n    give a black and white image, 1 will give the original image while\n    2 will enhance the saturation by a factor of 2.\n    Returns:\n    PIL Image: Saturation adjusted image.\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_hue",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "description": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "peekOfCode": "def adjust_hue(img, hue_factor):\n    \"\"\"Adjust hue of an image.\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.",
        "detail": "third_party.deep-image-retrieval.dirtorch.utils.transforms_tools",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.extract_features",
        "description": "third_party.deep-image-retrieval.dirtorch.extract_features",
        "peekOfCode": "def extract_features(db, net, trfs, pooling='mean', gemp=3, detailed=False, whiten=None,\n                     threads=8, batch_size=16, output=None, dbg=()):\n    \"\"\" Extract features from trained model (network) on a given dataset.\n    \"\"\"\n    print(\"\\n>> Extracting features...\")\n    try:\n        query_db = db.get_query_db()\n    except NotImplementedError:\n        query_db = None\n    # extract DB feats",
        "detail": "third_party.deep-image-retrieval.dirtorch.extract_features",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.extract_features",
        "description": "third_party.deep-image-retrieval.dirtorch.extract_features",
        "peekOfCode": "def load_model(path, iscuda):\n    checkpoint = common.load_checkpoint(path, iscuda)\n    net = nets.create_model(pretrained=\"\", **checkpoint['model_options'])\n    net = common.switch_model_to_cuda(net, iscuda, checkpoint)\n    net.load_state_dict(checkpoint['state_dict'])\n    net.preprocess = checkpoint.get('preprocess', net.preprocess)\n    if 'pca' in checkpoint:\n        net.pca = checkpoint.get('pca')\n    return net\nif __name__ == '__main__':",
        "detail": "third_party.deep-image-retrieval.dirtorch.extract_features",
        "documentation": {}
    },
    {
        "label": "extract_kapture_global_features",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.extract_kapture",
        "description": "third_party.deep-image-retrieval.dirtorch.extract_kapture",
        "peekOfCode": "def extract_kapture_global_features(kapture_root_path: str, net, trfs, pooling='mean', gemp=3, whiten=None,\n                                    threads=8, batch_size=16):\n    \"\"\" Extract features from trained model (network) on a given dataset.\n    \"\"\"\n    print(f'loading {kapture_root_path}')\n    kdata = kapture_from_dir(kapture_root_path, None, skip_list=[kapture.Keypoints,\n                                                                    kapture.Descriptors,\n                                                                    kapture.Matches,\n                                                                    kapture.Points3d,\n                                                                    kapture.Observations])",
        "detail": "third_party.deep-image-retrieval.dirtorch.extract_kapture",
        "documentation": {}
    },
    {
        "label": "os.environ['DB_ROOT']",
        "kind": 5,
        "importPath": "third_party.deep-image-retrieval.dirtorch.extract_kapture",
        "description": "third_party.deep-image-retrieval.dirtorch.extract_kapture",
        "peekOfCode": "os.environ['DB_ROOT'] = ''\nfrom dirtorch.utils import common\nfrom dirtorch.utils.common import tonumpy, pool\nfrom dirtorch.datasets.generic import ImageList\nfrom dirtorch.test_dir import extract_image_features\nfrom dirtorch.extract_features import load_model\nimport kapture\nfrom kapture.io.csv import kapture_from_dir\nfrom kapture.io.csv import get_csv_fullpath, global_features_to_file\nfrom kapture.io.records import get_image_fullpath",
        "detail": "third_party.deep-image-retrieval.dirtorch.extract_kapture",
        "documentation": {}
    },
    {
        "label": "APLos",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "class APLoss (nn.Module):\n    \"\"\" Differentiable AP loss, through quantization. From the paper:\n        Learning with Average Precision: Training Image Retrieval with a Listwise Loss\n        Jerome Revaud, Jon Almazan, Rafael Sampaio de Rezende, Cesar de Souza\n        https://arxiv.org/abs/1906.07589\n        Input: (N, M)   values in [min, max]\n        label: (N, M)   values in {0, 1}\n        Returns: 1 - mAP (mean AP for each n in {1..N})\n                 Note: typically, this is what you wanna minimize\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "TAPLos",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "class TAPLoss (APLoss):\n    \"\"\" Differentiable tie-aware AP loss, through quantization. From the paper:\n        Learning with Average Precision: Training Image Retrieval with a Listwise Loss\n        Jerome Revaud, Jon Almazan, Rafael Sampaio de Rezende, Cesar de Souza\n        https://arxiv.org/abs/1906.07589\n        Input: (N, M)   values in [min, max]\n        label: (N, M)   values in {0, 1}\n        Returns: 1 - mAP (mean AP for each n in {1..N})\n                 Note: typically, this is what you wanna minimize\n    \"\"\"",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "TripletMarginLoss",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "class TripletMarginLoss(nn.TripletMarginLoss):\n    \"\"\" PyTorch's margin triplet loss\n    TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False, size_average=True, reduce=True)\n    \"\"\"\n    def eval_func(self, dp, dn):\n        return max(0, dp - dn + self.margin)\nclass TripletLogExpLoss(nn.Module):\n    r\"\"\"Creates a criterion that measures the triplet loss given an input\n    tensors x1, x2, x3.\n    This is used for measuring a relative similarity between samples. A triplet",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "TripletLogExpLoss",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "class TripletLogExpLoss(nn.Module):\n    r\"\"\"Creates a criterion that measures the triplet loss given an input\n    tensors x1, x2, x3.\n    This is used for measuring a relative similarity between samples. A triplet\n    is composed by `a`, `p` and `n`: anchor, positive examples and negative\n    example respectively. The shape of all input variables should be\n    :math:`(N, D)`.\n    The distance is described in detail in the paper `Improving Pairwise Ranking for Multi-Label\n    Image Classification`_ by Y. Li et al.\n    .. math::",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "APLoss_dis",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "class APLoss_dist (APLoss):\n    def forward(self, x, label, **kw):\n        d = sim_to_dist(x)\n        return APLoss.forward(self, d, label, **kw)\nclass TAPLoss_dist (TAPLoss):\n    def forward(self, x, label, **kw):\n        d = sim_to_dist(x)\n        return TAPLoss.forward(self, d, label, **kw)",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "TAPLoss_dis",
        "kind": 6,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "class TAPLoss_dist (TAPLoss):\n    def forward(self, x, label, **kw):\n        d = sim_to_dist(x)\n        return TAPLoss.forward(self, d, label, **kw)",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "sim_to_dist",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.loss",
        "description": "third_party.deep-image-retrieval.dirtorch.loss",
        "peekOfCode": "def sim_to_dist(scores):\n    return 1 - torch.sqrt(2.001 - 2*scores)\nclass APLoss_dist (APLoss):\n    def forward(self, x, label, **kw):\n        d = sim_to_dist(x)\n        return APLoss.forward(self, d, label, **kw)\nclass TAPLoss_dist (TAPLoss):\n    def forward(self, x, label, **kw):\n        d = sim_to_dist(x)\n        return TAPLoss.forward(self, d, label, **kw)",
        "detail": "third_party.deep-image-retrieval.dirtorch.loss",
        "documentation": {}
    },
    {
        "label": "expand_descriptors",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "description": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "peekOfCode": "def expand_descriptors(descs, db=None, alpha=0, k=0):\n    assert k >= 0 and alpha >= 0, 'k and alpha must be non-negative'\n    if k == 0:\n        return descs\n    descs = tonumpy(descs)\n    n = descs.shape[0]\n    db_descs = tonumpy(db if db is not None else descs)\n    sim = matmul(descs, db_descs)\n    if db is None:\n        sim[np.diag_indices(n)] = 0",
        "detail": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "documentation": {}
    },
    {
        "label": "extract_image_features",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "description": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "peekOfCode": "def extract_image_features(dataset, transforms, net, ret_imgs=False, same_size=False, flip=None,\n                           desc=\"Extract feats...\", iscuda=True, threads=8, batch_size=8):\n    \"\"\" Extract image features for a given dataset.\n        Output is 2-dimensional (B, D)\n    \"\"\"\n    if not same_size:\n        batch_size = 1\n        old_benchmark = torch.backends.cudnn.benchmark\n        torch.backends.cudnn.benchmark = False\n    loader = get_loader(dataset, trf_chain=transforms, preprocess=net.preprocess, iscuda=iscuda,",
        "detail": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "documentation": {}
    },
    {
        "label": "eval_model",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "description": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "peekOfCode": "def eval_model(db, net, trfs, pooling='mean', gemp=3, detailed=False, whiten=None,\n               aqe=None, adba=None, threads=8, batch_size=16, save_feats=None,\n               load_feats=None, dbg=()):\n    \"\"\" Evaluate a trained model (network) on a given dataset.\n    The dataset is supposed to contain the evaluation code.\n    \"\"\"\n    print(\"\\n>> Evaluation...\")\n    query_db = db.get_query_db()\n    # extract DB feats\n    bdescs = []",
        "detail": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "description": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "peekOfCode": "def load_model(path, iscuda):\n    checkpoint = common.load_checkpoint(path, iscuda)\n    net = nets.create_model(pretrained=\"\", **checkpoint['model_options'])\n    net = common.switch_model_to_cuda(net, iscuda, checkpoint)\n    net.load_state_dict(checkpoint['state_dict'])\n    net.preprocess = checkpoint.get('preprocess', net.preprocess)\n    if 'pca' in checkpoint:\n        net.pca = checkpoint.get('pca')\n    return net\nif __name__ == '__main__':",
        "detail": "third_party.deep-image-retrieval.dirtorch.test_dir",
        "documentation": {}
    },
    {
        "label": "AachenImage",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.aachen",
        "description": "third_party.r2d2.datasets.aachen",
        "peekOfCode": "class AachenImages (Dataset):\n    \"\"\" Loads all images from the Aachen Day-Night dataset \n    \"\"\"\n    def __init__(self, select='db day night', root='data/aachen'):\n        Dataset.__init__(self)\n        self.root = root\n        self.img_dir = 'images_upright'\n        self.select = set(select.split())\n        assert self.select, 'Nothing was selected'\n        self.imgs = []",
        "detail": "third_party.r2d2.datasets.aachen",
        "documentation": {}
    },
    {
        "label": "AachenImages_D",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.aachen",
        "description": "third_party.r2d2.datasets.aachen",
        "peekOfCode": "class AachenImages_DB (AachenImages):\n    \"\"\" Only database (db) images.\n    \"\"\"\n    def __init__(self, **kw):\n        AachenImages.__init__(self, select='db', **kw)\n        self.db_image_idxs = {self.get_tag(i) : i for i,f in enumerate(self.imgs)}\n    def get_tag(self, idx): \n        # returns image tag == img number (name)\n        return os.path.split( self.imgs[idx][:-4] )[1]\nclass AachenPairs_StyleTransferDayNight (AachenImages_DB, StillPairDataset):",
        "detail": "third_party.r2d2.datasets.aachen",
        "documentation": {}
    },
    {
        "label": "AachenPairs_StyleTransferDayNigh",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.aachen",
        "description": "third_party.r2d2.datasets.aachen",
        "peekOfCode": "class AachenPairs_StyleTransferDayNight (AachenImages_DB, StillPairDataset):\n    \"\"\" synthetic day-night pairs of images \n        (night images obtained using autoamtic style transfer from web night images)\n    \"\"\"\n    def __init__(self, root='data/aachen/style_transfer', **kw):\n        StillPairDataset.__init__(self)\n        AachenImages_DB.__init__(self, **kw)\n        old_root = os.path.join(self.root, self.img_dir)\n        self.root = os.path.commonprefix((old_root, root))\n        self.img_dir = ''",
        "detail": "third_party.r2d2.datasets.aachen",
        "documentation": {}
    },
    {
        "label": "AachenPairs_OpticalFlo",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.aachen",
        "description": "third_party.r2d2.datasets.aachen",
        "peekOfCode": "class AachenPairs_OpticalFlow (AachenImages_DB, PairDataset):\n    \"\"\" Image pairs from Aachen db with optical flow.\n    \"\"\"\n    def __init__(self, root='data/aachen/optical_flow', **kw):\n        PairDataset.__init__(self)\n        AachenImages_DB.__init__(self, **kw)\n        self.root_flow = root\n        # find out the subsest of valid pairs from the list of flow files\n        flows = {f for f in os.listdir(os.path.join(root, 'flow')) if f.endswith('.png')}\n        masks = {f for f in os.listdir(os.path.join(root, 'mask')) if f.endswith('.png')}",
        "detail": "third_party.r2d2.datasets.aachen",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.dataset",
        "description": "third_party.r2d2.datasets.dataset",
        "peekOfCode": "class Dataset(object):\n    ''' Base class for a dataset. To be overloaded.\n    '''\n    root = ''\n    img_dir = ''\n    nimg = 0\n    def __len__(self):\n        return self.nimg\n    def get_key(self, img_idx):\n        raise NotImplementedError()",
        "detail": "third_party.r2d2.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "CatDatase",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.dataset",
        "description": "third_party.r2d2.datasets.dataset",
        "peekOfCode": "class CatDataset (Dataset):\n    ''' Concatenation of several datasets.\n    '''\n    def __init__(self, *datasets):\n        assert len(datasets) >= 1\n        self.datasets = datasets\n        offsets = [0]\n        for db in datasets:\n            offsets.append(db.nimg)\n        self.offsets = np.cumsum(offsets)",
        "detail": "third_party.r2d2.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "ImgFolde",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.imgfolder",
        "description": "third_party.r2d2.datasets.imgfolder",
        "peekOfCode": "class ImgFolder (Dataset):\n    \"\"\" load all images in a folder (no recursion).\n    \"\"\"\n    def __init__(self, root, imgs=None, exts=('.jpg','.png','.ppm')):\n        Dataset.__init__(self)\n        self.root = root\n        self.imgs = imgs or [f for f in os.listdir(root) if f.endswith(exts)]\n        self.nimg = len(self.imgs)\n    def get_key(self, idx):\n        return self.imgs[idx]",
        "detail": "third_party.r2d2.datasets.imgfolder",
        "documentation": {}
    },
    {
        "label": "PairDatase",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.pair_dataset",
        "description": "third_party.r2d2.datasets.pair_dataset",
        "peekOfCode": "class PairDataset (Dataset):\n    \"\"\" A dataset that serves image pairs with ground-truth pixel correspondences.\n    \"\"\"\n    def __init__(self):\n        Dataset.__init__(self)\n        self.npairs = 0\n    def get_filename(self, img_idx, root=None):\n        if is_pair(img_idx): # if img_idx is a pair of indices, we return a pair of filenames\n            return tuple(Dataset.get_filename(self, i, root) for i in img_idx)\n        return Dataset.get_filename(self, img_idx, root)",
        "detail": "third_party.r2d2.datasets.pair_dataset",
        "documentation": {}
    },
    {
        "label": "StillPairDatase",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.pair_dataset",
        "description": "third_party.r2d2.datasets.pair_dataset",
        "peekOfCode": "class StillPairDataset (PairDataset):\n    \"\"\" A dataset of 'still' image pairs.\n        By overloading a normal image dataset, it appends the get_pair(i) function\n        that serves trivial image pairs (img1, img2) where img1 == img2 == get_image(i).\n    \"\"\"\n    def get_pair(self, pair_idx, output=()):\n        if isinstance(output, str): output = output.split()\n        img1, img2 = map(self.get_image, self.image_pairs[pair_idx])\n        W,H = img1.size\n        sx = img2.size[0] / float(W)",
        "detail": "third_party.r2d2.datasets.pair_dataset",
        "documentation": {}
    },
    {
        "label": "SyntheticPairDatase",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.pair_dataset",
        "description": "third_party.r2d2.datasets.pair_dataset",
        "peekOfCode": "class SyntheticPairDataset (PairDataset):\n    \"\"\" A synthetic generator of image pairs.\n        Given a normal image dataset, it constructs pairs using random homographies & noise.\n    \"\"\"\n    def __init__(self, dataset, scale='', distort=''):\n        self.attach_dataset(dataset)\n        self.distort = instanciate_transformation(distort)\n        self.scale = instanciate_transformation(scale)\n    def attach_dataset(self, dataset):\n        assert isinstance(dataset, Dataset) and not isinstance(dataset, PairDataset)",
        "detail": "third_party.r2d2.datasets.pair_dataset",
        "documentation": {}
    },
    {
        "label": "TransformedPair",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.pair_dataset",
        "description": "third_party.r2d2.datasets.pair_dataset",
        "peekOfCode": "class TransformedPairs (PairDataset):\n    \"\"\" Automatic data augmentation for pre-existing image pairs.\n        Given an image pair dataset, it generates synthetically jittered pairs\n        using random transformations (e.g. homographies & noise).\n    \"\"\"\n    def __init__(self, dataset, trf=''):\n        self.attach_dataset(dataset)\n        self.trf = instanciate_transformation(trf)\n    def attach_dataset(self, dataset):\n        assert isinstance(dataset, PairDataset)",
        "detail": "third_party.r2d2.datasets.pair_dataset",
        "documentation": {}
    },
    {
        "label": "CatPairDatase",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.pair_dataset",
        "description": "third_party.r2d2.datasets.pair_dataset",
        "peekOfCode": "class CatPairDataset (CatDataset):\n    ''' Concatenation of several pair datasets.\n    '''\n    def __init__(self, *datasets):\n        CatDataset.__init__(self, *datasets)\n        pair_offsets = [0]\n        for db in datasets:\n            pair_offsets.append(db.npairs)\n        self.pair_offsets = np.cumsum(pair_offsets)\n        self.npairs = self.pair_offsets[-1]",
        "detail": "third_party.r2d2.datasets.pair_dataset",
        "documentation": {}
    },
    {
        "label": "is_pair",
        "kind": 2,
        "importPath": "third_party.r2d2.datasets.pair_dataset",
        "description": "third_party.r2d2.datasets.pair_dataset",
        "peekOfCode": "def is_pair(x):\n    if isinstance(x, (tuple,list)) and len(x) == 2:\n        return True\n    if isinstance(x, np.ndarray) and x.ndim == 1 and x.shape[0] == 2:\n        return True\n    return False",
        "detail": "third_party.r2d2.datasets.pair_dataset",
        "documentation": {}
    },
    {
        "label": "RandomWebImage",
        "kind": 6,
        "importPath": "third_party.r2d2.datasets.web_images",
        "description": "third_party.r2d2.datasets.web_images",
        "peekOfCode": "class RandomWebImages (Dataset):\n    \"\"\" 1 million distractors from Oxford and Paris Revisited\n        see http://ptak.felk.cvut.cz/revisitop/revisitop1m/\n    \"\"\"\n    def __init__(self, start=0, end=1024, root=\"data/revisitop1m\"):\n        Dataset.__init__(self)\n        self.root = root\n        bar = None\n        self.imgs  = []\n        for i in range(start, end):",
        "detail": "third_party.r2d2.datasets.web_images",
        "documentation": {}
    },
    {
        "label": "verify_img",
        "kind": 2,
        "importPath": "third_party.r2d2.datasets.web_images",
        "description": "third_party.r2d2.datasets.web_images",
        "peekOfCode": "def verify_img(folder, f):\n    path = os.path.join(folder, f)\n    if not f.endswith('.jpg'): return False\n    try: \n        from PIL import Image\n        Image.open(path).convert('RGB') # try to open it\n        return True\n    except: \n        return False",
        "detail": "third_party.r2d2.datasets.web_images",
        "documentation": {}
    },
    {
        "label": "APLos",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.ap_loss",
        "description": "third_party.r2d2.nets.ap_loss",
        "peekOfCode": "class APLoss (nn.Module):\n    \"\"\" differentiable AP loss, through quantization.\n        Input: (N, M)   values in [min, max]\n        label: (N, M)   values in {0, 1}\n        Returns: list of query AP (for each n in {1..N})\n                 Note: typically, you want to minimize 1 - mean(AP)\n    \"\"\"\n    def __init__(self, nq=25, min=0, max=1, euc=False):\n        nn.Module.__init__(self)\n        assert isinstance(nq, int) and 2 <= nq <= 100",
        "detail": "third_party.r2d2.nets.ap_loss",
        "documentation": {}
    },
    {
        "label": "MultiLos",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.losses",
        "description": "third_party.r2d2.nets.losses",
        "peekOfCode": "class MultiLoss (nn.Module):\n    \"\"\" Combines several loss functions for convenience.\n    *args: [loss weight (float), loss creator, ... ]\n    Example:\n        loss = MultiLoss( 1, MyFirstLoss(), 0.5, MySecondLoss() )\n    \"\"\"\n    def __init__(self, *args, dbg=()):\n        nn.Module.__init__(self)\n        assert len(args) % 2 == 0, 'args must be a list of (float, loss)'\n        self.weights = []",
        "detail": "third_party.r2d2.nets.losses",
        "documentation": {}
    },
    {
        "label": "BaseNe",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.patchnet",
        "description": "third_party.r2d2.nets.patchnet",
        "peekOfCode": "class BaseNet (nn.Module):\n    \"\"\" Takes a list of images as input, and returns for each image:\n        - a pixelwise descriptor\n        - a pixelwise confidence\n    \"\"\"\n    def softmax(self, ux):\n        if ux.shape[1] == 1:\n            x = F.softplus(ux)\n            return x / (1 + x)  # for sure in [0,1], much less plateaus than softmax\n        elif ux.shape[1] == 2:",
        "detail": "third_party.r2d2.nets.patchnet",
        "documentation": {}
    },
    {
        "label": "PatchNe",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.patchnet",
        "description": "third_party.r2d2.nets.patchnet",
        "peekOfCode": "class PatchNet (BaseNet):\n    \"\"\" Helper class to construct a fully-convolutional network that\n        extract a l2-normalized patch descriptor.\n    \"\"\"\n    def __init__(self, inchan=3, dilated=True, dilation=1, bn=True, bn_affine=False):\n        BaseNet.__init__(self)\n        self.inchan = inchan\n        self.curchan = inchan\n        self.dilated = dilated\n        self.dilation = dilation",
        "detail": "third_party.r2d2.nets.patchnet",
        "documentation": {}
    },
    {
        "label": "L2_Ne",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.patchnet",
        "description": "third_party.r2d2.nets.patchnet",
        "peekOfCode": "class L2_Net (PatchNet):\n    \"\"\" Compute a 128D descriptor for all overlapping 32x32 patches.\n        From the L2Net paper (CVPR'17).\n    \"\"\"\n    def __init__(self, dim=128, **kw ):\n        PatchNet.__init__(self, **kw)\n        add_conv = lambda n,**kw: self._add_conv((n*dim)//128,**kw)\n        add_conv(32)\n        add_conv(32)\n        add_conv(64, stride=2)",
        "detail": "third_party.r2d2.nets.patchnet",
        "documentation": {}
    },
    {
        "label": "Quad_L2Ne",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.patchnet",
        "description": "third_party.r2d2.nets.patchnet",
        "peekOfCode": "class Quad_L2Net (PatchNet):\n    \"\"\" Same than L2_Net, but replace the final 8x8 conv by 3 successive 2x2 convs.\n    \"\"\"\n    def __init__(self, dim=128, mchan=4, relu22=False, **kw ):\n        PatchNet.__init__(self, **kw)\n        self._add_conv(  8*mchan)\n        self._add_conv(  8*mchan)\n        self._add_conv( 16*mchan, stride=2)\n        self._add_conv( 16*mchan)\n        self._add_conv( 32*mchan, stride=2)",
        "detail": "third_party.r2d2.nets.patchnet",
        "documentation": {}
    },
    {
        "label": "Quad_L2Net_ConfCF",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.patchnet",
        "description": "third_party.r2d2.nets.patchnet",
        "peekOfCode": "class Quad_L2Net_ConfCFS (Quad_L2Net):\n    \"\"\" Same than Quad_L2Net, with 2 confidence maps for repeatability and reliability.\n    \"\"\"\n    def __init__(self, **kw ):\n        Quad_L2Net.__init__(self, **kw)\n        # reliability classifier\n        self.clf = nn.Conv2d(self.out_dim, 2, kernel_size=1)\n        # repeatability classifier: for some reasons it's a softplus, not a softmax!\n        # Why? I guess it's a mistake that was left unnoticed in the code for a long time...\n        self.sal = nn.Conv2d(self.out_dim, 1, kernel_size=1) ",
        "detail": "third_party.r2d2.nets.patchnet",
        "documentation": {}
    },
    {
        "label": "PixelAPLos",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.reliability_loss",
        "description": "third_party.r2d2.nets.reliability_loss",
        "peekOfCode": "class PixelAPLoss (nn.Module):\n    \"\"\" Computes the pixel-wise AP loss:\n        Given two images and ground-truth optical flow, computes the AP per pixel.\n        feat1:  (B, C, H, W)   pixel-wise features extracted from img1\n        feat2:  (B, C, H, W)   pixel-wise features extracted from img2\n        aflow:  (B, 2, H, W)   absolute flow: aflow[...,y1,x1] = x2,y2\n    \"\"\"\n    def __init__(self, sampler, nq=20):\n        nn.Module.__init__(self)\n        self.aploss = APLoss(nq, min=0, max=1, euc=False)",
        "detail": "third_party.r2d2.nets.reliability_loss",
        "documentation": {}
    },
    {
        "label": "ReliabilityLos",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.reliability_loss",
        "description": "third_party.r2d2.nets.reliability_loss",
        "peekOfCode": "class ReliabilityLoss (PixelAPLoss):\n    \"\"\" same than PixelAPLoss, but also train a pixel-wise confidence\n        that this pixel is going to have a good AP.\n    \"\"\"\n    def __init__(self, sampler, base=0.5, **kw):\n        PixelAPLoss.__init__(self, sampler, **kw)\n        assert 0 <= base < 1\n        self.base = base\n        self.name = 'reliability'\n    def loss_from_ap(self, ap, rel):",
        "detail": "third_party.r2d2.nets.reliability_loss",
        "documentation": {}
    },
    {
        "label": "CosimLos",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.repeatability_loss",
        "description": "third_party.r2d2.nets.repeatability_loss",
        "peekOfCode": "class CosimLoss (nn.Module):\n    \"\"\" Try to make the repeatability repeatable from one image to the other.\n    \"\"\"\n    def __init__(self, N=16):\n        nn.Module.__init__(self)\n        self.name = f'cosim{N}'\n        self.patches = nn.Unfold(N, padding=0, stride=N//2)\n    def extract_patches(self, sal):\n        patches = self.patches(sal).transpose(1,2) # flatten\n        patches = F.normalize(patches, p=2, dim=2) # norm",
        "detail": "third_party.r2d2.nets.repeatability_loss",
        "documentation": {}
    },
    {
        "label": "PeakyLos",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.repeatability_loss",
        "description": "third_party.r2d2.nets.repeatability_loss",
        "peekOfCode": "class PeakyLoss (nn.Module):\n    \"\"\" Try to make the repeatability locally peaky.\n    Mechanism: we maximize, for each pixel, the difference between the local mean\n               and the local max.\n    \"\"\"\n    def __init__(self, N=16):\n        nn.Module.__init__(self)\n        self.name = f'peaky{N}'\n        assert N % 2 == 0, 'N must be pair'\n        self.preproc = nn.AvgPool2d(3, stride=1, padding=1)",
        "detail": "third_party.r2d2.nets.repeatability_loss",
        "documentation": {}
    },
    {
        "label": "FullSampler",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.sampler",
        "description": "third_party.r2d2.nets.sampler",
        "peekOfCode": "class FullSampler(nn.Module):\n    \"\"\" all pixels are selected\n        - feats: keypoint descriptors\n        - confs: reliability values\n    \"\"\"\n    def __init__(self):\n        nn.Module.__init__(self)\n        self.mode = 'bilinear'\n        self.padding = 'zeros'\n    @staticmethod",
        "detail": "third_party.r2d2.nets.sampler",
        "documentation": {}
    },
    {
        "label": "SubSample",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.sampler",
        "description": "third_party.r2d2.nets.sampler",
        "peekOfCode": "class SubSampler (FullSampler):\n    \"\"\" pixels are selected in an uniformly spaced grid\n    \"\"\"\n    def __init__(self, border, subq, subd, perimage=False):\n        FullSampler.__init__(self)\n        assert subq % subd == 0, 'subq must be multiple of subd'\n        self.sub_q = subq\n        self.sub_d = subd\n        self.border = border\n        self.perimage = perimage",
        "detail": "third_party.r2d2.nets.sampler",
        "documentation": {}
    },
    {
        "label": "NghSample",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.sampler",
        "description": "third_party.r2d2.nets.sampler",
        "peekOfCode": "class NghSampler (FullSampler):\n    \"\"\" all pixels in a small neighborhood\n    \"\"\"\n    def __init__(self, ngh, subq=1, subd=1, ignore=1, border=None):\n        FullSampler.__init__(self)\n        assert 0 <= ignore < ngh\n        self.ngh = ngh\n        self.ignore = ignore\n        assert subd <= ngh\n        self.sub_q = subq",
        "detail": "third_party.r2d2.nets.sampler",
        "documentation": {}
    },
    {
        "label": "FarNearSample",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.sampler",
        "description": "third_party.r2d2.nets.sampler",
        "peekOfCode": "class FarNearSampler (FullSampler):\n    \"\"\" Sample pixels from *both* a small neighborhood *and* far-away pixels.\n    How it works?\n        1) Queries are sampled from img1,\n            - at least `border` pixels from borders and \n            - on a grid with step = `subq`\n        2) Close database pixels \n            - from the corresponding image (img2),\n            - within a `ngh` distance radius \n            - on a grid with step = `subd_ngh`",
        "detail": "third_party.r2d2.nets.sampler",
        "documentation": {}
    },
    {
        "label": "NghSampler",
        "kind": 6,
        "importPath": "third_party.r2d2.nets.sampler",
        "description": "third_party.r2d2.nets.sampler",
        "peekOfCode": "class NghSampler2 (nn.Module):\n    \"\"\" Similar to NghSampler, but doesnt warp the 2nd image.\n    Distance to GT =>  0 ... pos_d ... neg_d ... ngh\n    Pixel label    =>  + + + + + + 0 0 - - - - - - -\n    Subsample on query side: if > 0, regular grid\n                                < 0, random points \n    In both cases, the number of query points is = W*H/subq**2\n    \"\"\"\n    def __init__(self, ngh, subq=1, subd=1, pos_d=0, neg_d=2, border=None,\n                       maxpool_pos=True, subd_neg=0):",
        "detail": "third_party.r2d2.nets.sampler",
        "documentation": {}
    },
    {
        "label": "mkdir_for",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.common",
        "description": "third_party.r2d2.tools.common",
        "peekOfCode": "def mkdir_for(file_path):\n    os.makedirs(os.path.split(file_path)[0], exist_ok=True)\ndef model_size(model):\n    ''' Computes the number of parameters of the model \n    '''\n    size = 0\n    for weights in model.state_dict().values():\n        size += np.prod(weights.shape)\n    return size\ndef torch_set_gpu(gpus):",
        "detail": "third_party.r2d2.tools.common",
        "documentation": {}
    },
    {
        "label": "model_size",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.common",
        "description": "third_party.r2d2.tools.common",
        "peekOfCode": "def model_size(model):\n    ''' Computes the number of parameters of the model \n    '''\n    size = 0\n    for weights in model.state_dict().values():\n        size += np.prod(weights.shape)\n    return size\ndef torch_set_gpu(gpus):\n    if type(gpus) is int:\n        gpus = [gpus]",
        "detail": "third_party.r2d2.tools.common",
        "documentation": {}
    },
    {
        "label": "torch_set_gpu",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.common",
        "description": "third_party.r2d2.tools.common",
        "peekOfCode": "def torch_set_gpu(gpus):\n    if type(gpus) is int:\n        gpus = [gpus]\n    cuda = all(gpu>=0 for gpu in gpus)\n    if cuda:\n        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu) for gpu in gpus])\n        assert cuda and torch.cuda.is_available(), \"%s has GPUs %s unavailable\" % (\n            os.environ['HOSTNAME'],os.environ['CUDA_VISIBLE_DEVICES'])\n        torch.backends.cudnn.benchmark = True # speed-up cudnn\n        torch.backends.cudnn.fastest = True # even more speed-up?",
        "detail": "third_party.r2d2.tools.common",
        "documentation": {}
    },
    {
        "label": "PairLoader",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.dataloader",
        "description": "third_party.r2d2.tools.dataloader",
        "peekOfCode": "class PairLoader:\n    \"\"\" On-the-fly jittering of pairs of image with dense pixel ground-truth correspondences.\n    crop:   random crop applied to both images\n    scale:  random scaling applied to img2\n    distort: random ditorsion applied to img2\n    self[idx] returns a dictionary with keys: img1, img2, aflow, mask\n     - img1: cropped original\n     - img2: distorted cropped original\n     - aflow: 'absolute' optical flow = (x,y) position of each pixel from img1 in img2\n     - mask: (binary image) valid pixels of img1",
        "detail": "third_party.r2d2.tools.dataloader",
        "documentation": {}
    },
    {
        "label": "threaded_loader",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.dataloader",
        "description": "third_party.r2d2.tools.dataloader",
        "peekOfCode": "def threaded_loader( loader, iscuda, threads, batch_size=1, shuffle=True):\n    \"\"\" Get a data loader, given the dataset and some parameters.\n    Parameters\n    ----------\n    loader : object[i] returns the i-th training example.\n    iscuda : bool\n    batch_size : int\n    threads : int\n    shuffle : int\n    Returns",
        "detail": "third_party.r2d2.tools.dataloader",
        "documentation": {}
    },
    {
        "label": "collate",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.dataloader",
        "description": "third_party.r2d2.tools.dataloader",
        "peekOfCode": "def collate(batch, _use_shared_memory=True):\n    \"\"\"Puts each data field into a tensor with outer dimension batch size.\n    Copied from https://github.com/pytorch in torch/utils/data/_utils/collate.py\n    \"\"\"\n    import re\n    error_msg = \"batch must contain tensors, numbers, dicts or lists; found {}\"\n    elem_type = type(batch[0])\n    if isinstance(batch[0], torch.Tensor):\n        out = None\n        if _use_shared_memory:",
        "detail": "third_party.r2d2.tools.dataloader",
        "documentation": {}
    },
    {
        "label": "tensor2img",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.dataloader",
        "description": "third_party.r2d2.tools.dataloader",
        "peekOfCode": "def tensor2img(tensor, model=None):\n    \"\"\" convert back a torch/numpy tensor to a PIL Image\n        by undoing the ToTensor() and Normalize() transforms.\n    \"\"\"\n    mean = norm_RGB.transforms[1].mean\n    std =  norm_RGB.transforms[1].std\n    if isinstance(tensor, torch.Tensor):\n        tensor = tensor.detach().cpu().numpy()\n    res = np.uint8(np.clip(255*((tensor.transpose(1,2,0) * std) + mean), 0, 255))\n    from PIL import Image",
        "detail": "third_party.r2d2.tools.dataloader",
        "documentation": {}
    },
    {
        "label": "RGB_mean",
        "kind": 5,
        "importPath": "third_party.r2d2.tools.dataloader",
        "description": "third_party.r2d2.tools.dataloader",
        "peekOfCode": "RGB_mean = [0.485, 0.456, 0.406]\nRGB_std  = [0.229, 0.224, 0.225]\nnorm_RGB = tvf.Compose([tvf.ToTensor(), tvf.Normalize(mean=RGB_mean, std=RGB_std)])\nclass PairLoader:\n    \"\"\" On-the-fly jittering of pairs of image with dense pixel ground-truth correspondences.\n    crop:   random crop applied to both images\n    scale:  random scaling applied to img2\n    distort: random ditorsion applied to img2\n    self[idx] returns a dictionary with keys: img1, img2, aflow, mask\n     - img1: cropped original",
        "detail": "third_party.r2d2.tools.dataloader",
        "documentation": {}
    },
    {
        "label": "norm_RGB",
        "kind": 5,
        "importPath": "third_party.r2d2.tools.dataloader",
        "description": "third_party.r2d2.tools.dataloader",
        "peekOfCode": "norm_RGB = tvf.Compose([tvf.ToTensor(), tvf.Normalize(mean=RGB_mean, std=RGB_std)])\nclass PairLoader:\n    \"\"\" On-the-fly jittering of pairs of image with dense pixel ground-truth correspondences.\n    crop:   random crop applied to both images\n    scale:  random scaling applied to img2\n    distort: random ditorsion applied to img2\n    self[idx] returns a dictionary with keys: img1, img2, aflow, mask\n     - img1: cropped original\n     - img2: distorted cropped original\n     - aflow: 'absolute' optical flow = (x,y) position of each pixel from img1 in img2",
        "detail": "third_party.r2d2.tools.dataloader",
        "documentation": {}
    },
    {
        "label": "Traine",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.trainer",
        "description": "third_party.r2d2.tools.trainer",
        "peekOfCode": "class Trainer (nn.Module):\n    \"\"\" Helper class to train a deep network.\n        Overload this class `forward_backward` for your actual needs.\n    Usage: \n        train = Trainer(net, loader, loss, optimizer)\n        for epoch in range(n_epochs):\n            train()\n    \"\"\"\n    def __init__(self, net, loader, loss, optimizer):\n        nn.Module.__init__(self)",
        "detail": "third_party.r2d2.tools.trainer",
        "documentation": {}
    },
    {
        "label": "Scal",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class Scale (object):\n    \"\"\" Rescale the input PIL.Image to a given size.\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    The smallest dimension of the resulting image will be = size.\n    if largest == True: same behaviour for the largest dimension.\n    if not can_upscale: don't upscale\n    if not can_downscale: don't downscale\n    \"\"\"\n    def __init__(self, size, interpolation=Image.BILINEAR, largest=False, \n                 can_upscale=True, can_downscale=True):",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "RandomScal",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class RandomScale (Scale):\n    \"\"\"Rescale the input PIL.Image to a random size.\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    Args:\n        min_size (int): min size of the smaller edge of the picture.\n        max_size (int): max size of the smaller edge of the picture.\n        ar (float or tuple):\n            max change of aspect ratio (width/height).\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "RandomCro",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class RandomCrop (object):\n    \"\"\"Crop the given PIL Image at a random location.\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is 0, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCro",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class CenterCrop (RandomCrop):\n    \"\"\"Crops the given PIL Image at the center.\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    \"\"\"\n    @staticmethod\n    def get_params(img, output_size):",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "RandomRotation",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class RandomRotation(object):\n    \"\"\"Rescale the input PIL.Image to a random size.\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    Args:\n        degrees (float):\n            rotation angle.\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n    def __init__(self, degrees, interpolation=Image.BILINEAR):",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "RandomTilting",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class RandomTilting(object):\n    \"\"\"Apply a random tilting (left, right, up, down) to the input PIL.Image\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    Args:\n        maginitude (float):\n            maximum magnitude of the random skew (value between 0 and 1)\n        directions (string):\n            tilting directions allowed (all, left, right, up, down)\n            examples: \"all\", \"left,right\", \"up-down-right\"\n    \"\"\"",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "Tilt",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class Tilt(object):\n    \"\"\"Apply a known tilting to an image\n    \"\"\"\n    def __init__(self, *homography):\n        assert len(homography) == 8\n        self.homography = homography\n    def __call__(self, inp):\n        img = F.grab_img(inp)\n        homography = self.homography\n        #print(homography)",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "StillTransfor",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class StillTransform (object):\n    \"\"\" Takes and return an image, without changing its shape or geometry.\n    \"\"\"\n    def _transform(self, img):\n        raise NotImplementedError()\n    def __call__(self, inp):\n        img = F.grab_img(inp)\n        # transform the image (size should not change)\n        try:\n            img = self._transform(img)",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "PixelNois",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class PixelNoise (StillTransform):\n    \"\"\" Takes an image, and add random white noise.\n    \"\"\"\n    def __init__(self, ampl=20):\n        StillTransform.__init__(self)\n        assert 0 <= ampl < 255\n        self.ampl = ampl\n    def __repr__(self):\n        return \"PixelNoise(%g)\" % self.ampl\n    def _transform(self, img):",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "ColorJitte",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "class ColorJitter (StillTransform):\n    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n    Copied from https://github.com/pytorch in torchvision/transforms/transforms.py\n    Args:\n    brightness (float): How much to jitter brightness. brightness_factor\n    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n    contrast (float): How much to jitter contrast. contrast_factor\n    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n    saturation (float): How much to jitter saturation. saturation_factor\n    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "instanciate_transformation",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "def instanciate_transformation(cmd_line):\n    ''' Create a sequence of transformations.\n    cmd_line: (str)\n        Comma-separated list of transformations.\n        Ex: \"Rotate(10), Scale(256)\"\n    '''\n    if not isinstance(cmd_line, str):\n        return cmd_line # already instanciated\n    cmd_line = \"tvf.Compose([%s])\" % cmd_line\n    try:",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "RandomTilt",
        "kind": 5,
        "importPath": "third_party.r2d2.tools.transforms",
        "description": "third_party.r2d2.tools.transforms",
        "peekOfCode": "RandomTilt = RandomTilting # redefinition\nclass Tilt(object):\n    \"\"\"Apply a known tilting to an image\n    \"\"\"\n    def __init__(self, *homography):\n        assert len(homography) == 8\n        self.homography = homography\n    def __call__(self, inp):\n        img = F.grab_img(inp)\n        homography = self.homography",
        "detail": "third_party.r2d2.tools.transforms",
        "documentation": {}
    },
    {
        "label": "DummyImg",
        "kind": 6,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "class DummyImg:\n    ''' This class is a dummy image only defined by its size.\n    '''\n    def __init__(self, size):\n        self.size = size\n    def resize(self, size, *args, **kwargs):\n        return DummyImg(size)\n    def expand(self, border):\n        w, h = self.size\n        if isinstance(border, int):",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "grab_img",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def grab_img( img_and_label ):\n    ''' Called to extract the image from an img_and_label input\n    (a dictionary). Also compatible with old-style PIL images.\n    '''\n    if isinstance(img_and_label, dict):\n        # if input is a dictionary, then\n        # it must contains the img or its size.\n        try:\n            return img_and_label['img']\n        except KeyError:",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "update_img_and_labels",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def update_img_and_labels(img_and_label, img, persp=None):\n    ''' Called to update the img_and_label\n    '''\n    if isinstance(img_and_label, dict):\n        img_and_label['img'] = img\n        img_and_label['imsize'] = img.size\n        if persp:\n            if 'persp' not in img_and_label:\n                img_and_label['persp'] = (1,0,0,0,1,0,0,0)\n            img_and_label['persp'] = persp_mul(persp, img_and_label['persp'])",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "rand_log_uniform",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def rand_log_uniform(a, b):\n    return np.exp(np.random.uniform(np.log(a),np.log(b)))\ndef translate(tx, ty):\n    return (1,0,tx,\n            0,1,ty,\n            0,0)\ndef rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0,\n            0, 0)",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "translate",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def translate(tx, ty):\n    return (1,0,tx,\n            0,1,ty,\n            0,0)\ndef rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0,\n            0, 0)\ndef persp_mul(mat, mat2):\n    ''' homography (perspective) multiplication.",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "rotate",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def rotate(angle):\n    return (np.cos(angle),-np.sin(angle), 0,\n            np.sin(angle), np.cos(angle), 0,\n            0, 0)\ndef persp_mul(mat, mat2):\n    ''' homography (perspective) multiplication.\n    mat: 8-tuple (homography transform)\n    mat2: 8-tuple (homography transform) or 2-tuple (point)\n    '''\n    assert isinstance(mat, tuple)",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "persp_mul",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def persp_mul(mat, mat2):\n    ''' homography (perspective) multiplication.\n    mat: 8-tuple (homography transform)\n    mat2: 8-tuple (homography transform) or 2-tuple (point)\n    '''\n    assert isinstance(mat, tuple)\n    assert isinstance(mat2, tuple)\n    mat = np.float32(mat+(1,)).reshape(3,3)\n    mat2 = np.array(mat2+(1,)).reshape(3,3)\n    res = np.dot(mat, mat2)",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "persp_apply",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def persp_apply(mat, pts):\n    ''' homography (perspective) transformation.\n    mat: 8-tuple (homography transform)\n    pts: numpy array\n    '''\n    assert isinstance(mat, tuple)\n    assert isinstance(pts, np.ndarray)\n    assert pts.shape[-1] == 2\n    mat = np.float32(mat+(1,)).reshape(3,3)\n    if pts.ndim == 1:",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "is_pil_image",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def is_pil_image(img):\n    return isinstance(img, Image.Image)\ndef adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    brightness_factor (float):  How much to adjust the brightness. Can be\n    any non negative number. 0 gives a black image, 1 gives the\n    original image while 2 increases the brightness by a factor of 2.\n    Returns:",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_brightness",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def adjust_brightness(img, brightness_factor):\n    \"\"\"Adjust brightness of an Image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    brightness_factor (float):  How much to adjust the brightness. Can be\n    any non negative number. 0 gives a black image, 1 gives the\n    original image while 2 increases the brightness by a factor of 2.\n    Returns:\n    PIL Image: Brightness adjusted image.\n    Copied from https://github.com/pytorch in torchvision/transforms/functional.py",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_contrast",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def adjust_contrast(img, contrast_factor):\n    \"\"\"Adjust contrast of an Image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    contrast_factor (float): How much to adjust the contrast. Can be any\n    non negative number. 0 gives a solid gray image, 1 gives the\n    original image while 2 increases the contrast by a factor of 2.\n    Returns:\n    PIL Image: Contrast adjusted image.\n    Copied from https://github.com/pytorch in torchvision/transforms/functional.py",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_saturation",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def adjust_saturation(img, saturation_factor):\n    \"\"\"Adjust color saturation of an image.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.\n    saturation_factor (float):  How much to adjust the saturation. 0 will\n    give a black and white image, 1 will give the original image while\n    2 will enhance the saturation by a factor of 2.\n    Returns:\n    PIL Image: Saturation adjusted image.\n    Copied from https://github.com/pytorch in torchvision/transforms/functional.py",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "adjust_hue",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.transforms_tools",
        "description": "third_party.r2d2.tools.transforms_tools",
        "peekOfCode": "def adjust_hue(img, hue_factor):\n    \"\"\"Adjust hue of an image.\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n    Args:\n    img (PIL Image): PIL Image to be adjusted.",
        "detail": "third_party.r2d2.tools.transforms_tools",
        "documentation": {}
    },
    {
        "label": "make_colorwheel",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.viz",
        "description": "third_party.r2d2.tools.viz",
        "peekOfCode": "def make_colorwheel():\n    '''\n    Generates a color wheel for optical flow visualization as presented in:\n        Baker et al. \"A Database and Evaluation Methodology for Optical Flow\" (ICCV, 2007)\n        URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    Copied from https://github.com/tomrunia/OpticalFlow_Visualization/blob/master/flow_vis.py\n    Copyright (c) 2018 Tom Runia\n    '''",
        "detail": "third_party.r2d2.tools.viz",
        "documentation": {}
    },
    {
        "label": "flow_compute_color",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.viz",
        "description": "third_party.r2d2.tools.viz",
        "peekOfCode": "def flow_compute_color(u, v, convert_to_bgr=False):\n    '''\n    Applies the flow color wheel to (possibly clipped) flow components u and v.\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    :param u: np.ndarray, input horizontal flow\n    :param v: np.ndarray, input vertical flow\n    :param convert_to_bgr: bool, whether to change ordering and output BGR instead of RGB\n    :return:\n    Copied from https://github.com/tomrunia/OpticalFlow_Visualization/blob/master/flow_vis.py",
        "detail": "third_party.r2d2.tools.viz",
        "documentation": {}
    },
    {
        "label": "flow_to_color",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.viz",
        "description": "third_party.r2d2.tools.viz",
        "peekOfCode": "def flow_to_color(flow_uv, clip_flow=None, convert_to_bgr=False):\n    '''\n    Expects a two dimensional flow image of shape [H,W,2]\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    :param flow_uv: np.ndarray of shape [H,W,2]\n    :param clip_flow: float, maximum clipping value for flow\n    :return:\n    Copied from https://github.com/tomrunia/OpticalFlow_Visualization/blob/master/flow_vis.py\n    Copyright (c) 2018 Tom Runia",
        "detail": "third_party.r2d2.tools.viz",
        "documentation": {}
    },
    {
        "label": "show_flow",
        "kind": 2,
        "importPath": "third_party.r2d2.tools.viz",
        "description": "third_party.r2d2.tools.viz",
        "peekOfCode": "def show_flow( img0, img1, flow, mask=None ):\n    img0 = np.asarray(img0)\n    img1 = np.asarray(img1)\n    if mask is None: mask = 1\n    mask = np.asarray(mask)\n    if mask.ndim == 2: mask = mask[:,:,None]\n    assert flow.ndim == 3\n    assert flow.shape[:2] == img0.shape[:2] and flow.shape[2] == 2\n    def noticks():\n      pl.xticks([])",
        "detail": "third_party.r2d2.tools.viz",
        "documentation": {}
    },
    {
        "label": "NonMaxSuppressio",
        "kind": 6,
        "importPath": "third_party.r2d2.extract",
        "description": "third_party.r2d2.extract",
        "peekOfCode": "class NonMaxSuppression (torch.nn.Module):\n    def __init__(self, rel_thr=0.7, rep_thr=0.7):\n        nn.Module.__init__(self)\n        self.max_filter = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.rel_thr = rel_thr\n        self.rep_thr = rep_thr\n    def forward(self, reliability, repeatability, **kw):\n        assert len(reliability) == len(repeatability) == 1\n        reliability, repeatability = reliability[0], repeatability[0]\n        # local maxima",
        "detail": "third_party.r2d2.extract",
        "documentation": {}
    },
    {
        "label": "load_network",
        "kind": 2,
        "importPath": "third_party.r2d2.extract",
        "description": "third_party.r2d2.extract",
        "peekOfCode": "def load_network(model_fn): \n    checkpoint = torch.load(model_fn)\n    print(\"\\n>> Creating net = \" + checkpoint['net']) \n    net = eval(checkpoint['net'])\n    nb_of_weights = common.model_size(net)\n    print(f\" ( Model size: {nb_of_weights/1000:.0f}K parameters )\")\n    # initialization\n    weights = checkpoint['state_dict']\n    net.load_state_dict({k.replace('module.',''):v for k,v in weights.items()})\n    return net.eval()",
        "detail": "third_party.r2d2.extract",
        "documentation": {}
    },
    {
        "label": "extract_multiscale",
        "kind": 2,
        "importPath": "third_party.r2d2.extract",
        "description": "third_party.r2d2.extract",
        "peekOfCode": "def extract_multiscale( net, img, detector, scale_f=2**0.25, \n                        min_scale=0.0, max_scale=1, \n                        min_size=256, max_size=1024, \n                        verbose=False):\n    old_bm = torch.backends.cudnn.benchmark \n    torch.backends.cudnn.benchmark = False # speedup\n    # extract keypoints at multiple scales\n    B, three, H, W = img.shape\n    assert B == 1 and three == 3, \"should be a batch with a single RGB image\"\n    assert max_scale <= 1",
        "detail": "third_party.r2d2.extract",
        "documentation": {}
    },
    {
        "label": "extract_keypoints",
        "kind": 2,
        "importPath": "third_party.r2d2.extract",
        "description": "third_party.r2d2.extract",
        "peekOfCode": "def extract_keypoints(args):\n    iscuda = common.torch_set_gpu(args.gpu)\n    # load the network...\n    net = load_network(args.model)\n    if iscuda: net = net.cuda()\n    # create the non-maxima detector\n    detector = NonMaxSuppression(\n        rel_thr = args.reliability_thr, \n        rep_thr = args.repeatability_thr)\n    while args.images:",
        "detail": "third_party.r2d2.extract",
        "documentation": {}
    },
    {
        "label": "extract_kapture_keypoints",
        "kind": 2,
        "importPath": "third_party.r2d2.extract_kapture",
        "description": "third_party.r2d2.extract_kapture",
        "peekOfCode": "def extract_kapture_keypoints(args):\n    \"\"\"\n    Extract r2d2 keypoints and descritors to the kapture format directly \n    \"\"\"\n    print('extract_kapture_keypoints...')\n    kdata = kapture_from_dir(args.kapture_root, matches_pairs_file_path=None, \n    skip_list= [kapture.GlobalFeatures,\n                kapture.Matches,\n                kapture.Points3d,\n                kapture.Observations])",
        "detail": "third_party.r2d2.extract_kapture",
        "documentation": {}
    },
    {
        "label": "MyTrainer",
        "kind": 6,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "class MyTrainer(trainer.Trainer):\n    \"\"\" This class implements the network training.\n        Below is the function I need to overload to explain how to do the backprop.\n    \"\"\"\n    def forward_backward(self, inputs):\n        output = self.net(imgs=[inputs.pop('img1'),inputs.pop('img2')])\n        allvars = dict(inputs, **output)\n        loss, details = self.loss_func(**allvars)\n        if torch.is_grad_enabled(): loss.backward()\n        return loss, details",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "default_net",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "default_net = \"Quad_L2Net_ConfCFS()\"\ntoy_db_debug = \"\"\"SyntheticPairDataset(\n    ImgFolder('imgs'), \n            'RandomScale(256,1024,can_upscale=True)', \n            'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_web_images = \"\"\"SyntheticPairDataset(\n    web_images, \n        'RandomScale(256,1024,can_upscale=True)',\n        'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_images = \"\"\"SyntheticPairDataset(",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "toy_db_debug",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "toy_db_debug = \"\"\"SyntheticPairDataset(\n    ImgFolder('imgs'), \n            'RandomScale(256,1024,can_upscale=True)', \n            'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_web_images = \"\"\"SyntheticPairDataset(\n    web_images, \n        'RandomScale(256,1024,can_upscale=True)',\n        'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_images = \"\"\"SyntheticPairDataset(\n    aachen_db_images, ",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "db_web_images",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "db_web_images = \"\"\"SyntheticPairDataset(\n    web_images, \n        'RandomScale(256,1024,can_upscale=True)',\n        'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_images = \"\"\"SyntheticPairDataset(\n    aachen_db_images, \n        'RandomScale(256,1024,can_upscale=True)', \n        'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_style_transfer = \"\"\"TransformedPairs(\n    aachen_style_transfer_pairs,",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "db_aachen_images",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "db_aachen_images = \"\"\"SyntheticPairDataset(\n    aachen_db_images, \n        'RandomScale(256,1024,can_upscale=True)', \n        'RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_style_transfer = \"\"\"TransformedPairs(\n    aachen_style_transfer_pairs,\n            'RandomScale(256,1024,can_upscale=True), RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_flow = \"aachen_flow_pairs\"\ndata_sources = dict(\n    D = toy_db_debug,",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "db_aachen_style_transfer",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "db_aachen_style_transfer = \"\"\"TransformedPairs(\n    aachen_style_transfer_pairs,\n            'RandomScale(256,1024,can_upscale=True), RandomTilting(0.5), PixelNoise(25)')\"\"\"\ndb_aachen_flow = \"aachen_flow_pairs\"\ndata_sources = dict(\n    D = toy_db_debug,\n    W = db_web_images,\n    A = db_aachen_images,\n    F = db_aachen_flow,\n    S = db_aachen_style_transfer,",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "db_aachen_flow",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "db_aachen_flow = \"aachen_flow_pairs\"\ndata_sources = dict(\n    D = toy_db_debug,\n    W = db_web_images,\n    A = db_aachen_images,\n    F = db_aachen_flow,\n    S = db_aachen_style_transfer,\n    )\ndefault_dataloader = \"\"\"PairLoader(CatPairDataset(`data`),\n    scale   = 'RandomScale(256,1024,can_upscale=True)',",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "data_sources",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "data_sources = dict(\n    D = toy_db_debug,\n    W = db_web_images,\n    A = db_aachen_images,\n    F = db_aachen_flow,\n    S = db_aachen_style_transfer,\n    )\ndefault_dataloader = \"\"\"PairLoader(CatPairDataset(`data`),\n    scale   = 'RandomScale(256,1024,can_upscale=True)',\n    distort = 'ColorJitter(0.2,0.2,0.2,0.1)',",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "default_dataloader",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "default_dataloader = \"\"\"PairLoader(CatPairDataset(`data`),\n    scale   = 'RandomScale(256,1024,can_upscale=True)',\n    distort = 'ColorJitter(0.2,0.2,0.2,0.1)',\n    crop    = 'RandomCrop(192)')\"\"\"\ndefault_sampler = \"\"\"NghSampler2(ngh=7, subq=-8, subd=1, pos_d=3, neg_d=5, border=16,\n                            subd_neg=-8,maxpool_pos=True)\"\"\"\ndefault_loss = \"\"\"MultiLoss(\n        1, ReliabilityLoss(`sampler`, base=0.5, nq=20),\n        1, CosimLoss(N=`N`),\n        1, PeakyLoss(N=`N`))\"\"\"",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "default_sampler",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "default_sampler = \"\"\"NghSampler2(ngh=7, subq=-8, subd=1, pos_d=3, neg_d=5, border=16,\n                            subd_neg=-8,maxpool_pos=True)\"\"\"\ndefault_loss = \"\"\"MultiLoss(\n        1, ReliabilityLoss(`sampler`, base=0.5, nq=20),\n        1, CosimLoss(N=`N`),\n        1, PeakyLoss(N=`N`))\"\"\"\nclass MyTrainer(trainer.Trainer):\n    \"\"\" This class implements the network training.\n        Below is the function I need to overload to explain how to do the backprop.\n    \"\"\"",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "default_loss",
        "kind": 5,
        "importPath": "third_party.r2d2.train",
        "description": "third_party.r2d2.train",
        "peekOfCode": "default_loss = \"\"\"MultiLoss(\n        1, ReliabilityLoss(`sampler`, base=0.5, nq=20),\n        1, CosimLoss(N=`N`),\n        1, PeakyLoss(N=`N`))\"\"\"\nclass MyTrainer(trainer.Trainer):\n    \"\"\" This class implements the network training.\n        Below is the function I need to overload to explain how to do the backprop.\n    \"\"\"\n    def forward_backward(self, inputs):\n        output = self.net(imgs=[inputs.pop('img1'),inputs.pop('img2')])",
        "detail": "third_party.r2d2.train",
        "documentation": {}
    },
    {
        "label": "transparent",
        "kind": 2,
        "importPath": "third_party.r2d2.viz_heatmaps",
        "description": "third_party.r2d2.viz_heatmaps",
        "peekOfCode": "def transparent(img, alpha, cmap, **kw):\n    from matplotlib.colors import Normalize\n    colored_img = cmap(Normalize(clip=True,**kw)(img))\n    colored_img[:,:,-1] = alpha\n    return colored_img\nfrom tools import common\nfrom tools.dataloader import norm_RGB\nfrom nets.patchnet import *\nfrom extract import NonMaxSuppression\nif __name__ == '__main__':",
        "detail": "third_party.r2d2.viz_heatmaps",
        "documentation": {}
    },
    {
        "label": "smooth",
        "kind": 5,
        "importPath": "third_party.r2d2.viz_heatmaps",
        "description": "third_party.r2d2.viz_heatmaps",
        "peekOfCode": "smooth = lambda arr: uniform_filter(arr, 3)\ndef transparent(img, alpha, cmap, **kw):\n    from matplotlib.colors import Normalize\n    colored_img = cmap(Normalize(clip=True,**kw)(img))\n    colored_img[:,:,-1] = alpha\n    return colored_img\nfrom tools import common\nfrom tools.dataloader import norm_RGB\nfrom nets.patchnet import *\nfrom extract import NonMaxSuppression",
        "detail": "third_party.r2d2.viz_heatmaps",
        "documentation": {}
    },
    {
        "label": "description",
        "kind": 5,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "description = ['Tools and baselines for visual localization and mapping']\nroot = Path(__file__).parent\nwith open(str(root / 'README.md'), 'r', encoding='utf-8') as f:\n    readme = f.read()\nwith open(str(root / 'hloc/__init__.py'), 'r') as f:\n    version = eval(f.read().split('__version__ = ')[1].split()[0])\nwith open(str(root / 'requirements.txt'), 'r') as f:\n    dependencies = f.read().split('\\n')\nsetup(\n    name='hloc',",
        "detail": "setup",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 5,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "root = Path(__file__).parent\nwith open(str(root / 'README.md'), 'r', encoding='utf-8') as f:\n    readme = f.read()\nwith open(str(root / 'hloc/__init__.py'), 'r') as f:\n    version = eval(f.read().split('__version__ = ')[1].split()[0])\nwith open(str(root / 'requirements.txt'), 'r') as f:\n    dependencies = f.read().split('\\n')\nsetup(\n    name='hloc',\n    version=version,",
        "detail": "setup",
        "documentation": {}
    }
]